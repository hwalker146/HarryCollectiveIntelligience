
## 2025-08-12

### Podcast: Traka VP Craig Newell Discusses the Critical Role of Key and Asset Management in Data Center Operations
**Publication Date:** 2025-08-12T03:01:00
**Episode ID:** 5065

**Full Transcript:**
All right, welcome to Energy Evolution, where we explore the latest trends and innovations shaping the energy landscape. I'm your host, Taylor Kirkendall, and today we're diving into a topic that's making waves in both the environmental and economic sectors, talking about deep sea mining. Deep sea mining involves the extraction of minerals and resources from the ocean floor. Now, as the demand for metals like cobalt, nickel, and rare earth elements rises, driven by the growth of renewable energy technology and electric vehicles, companies are starting to turn their sights to some more extreme locations to find some of these materials, including the ocean depths. Now, the mining process involves these remotely operated vehicles that are equipped with specialized tools to collect nodules from the bottom of the ground and bring them to the surface. This technology is fascinating, but it also raises significant environmental concerns. The ocean is a delicate ecosystem, and disturbing the seabed can have far-reaching consequences, disrupting habitats and threatening marine life. So do be aware that there are some people that are pushing back on some of the development here. Now, what they're actually looking for when they go underwater is looking for these things called polymetallic modules. They're small, potato-shaped rocks that contain these valuable metals that they're looking for. They form on the ocean floor over millions of years, and can be found in abundance in certain regions, like the Clarion-Clipperton zone in the Pacific Ocean. Now, today we're welcoming back Jared Barrett. He's CEO of The Metals Company. They're working to mine the ocean floor. Earlier this year, The Metals Company formally initiated a process with the National Oceanic and Atmospheric Administration under the U.S. Department of Commerce to apply for exploration licenses and commercial recovery permits under existing U.S. legislation. Let's just dive right into that conversation that we had with Jared now. Hi, Jared. Thank you so much for joining us back here on Energy Evolution. Just to start off with, can you kind of give us a high-level overview of The Metals Company, what we're talking about when we're talking about deep-sea mining, and what it is exactly we're down there looking for? Yeah. The Metals Company are focused on developing ocean metals. Think of the planet as 70% ocean, yet we get no metals out of those very same oceans. We're focused on a deposit about 1,100 miles southwest of San Diego. It sits 4,000 meters below sea level in the form of polymetallic nodules. We go back to that global picture again, 70% of the known reserves of nickel, and cobalt, and manganese sit in this one deposit. If I was with you, I'd hand you a nodule that's about the size of a potato. They literally lie unattached on the ocean floor. What we've been focused on over the last 14 years is understanding the resource, how big it is, understanding what's the most efficient way of picking up these rocks, causing the least impact, and then also studying what the environmental impacts will be of removing them from that ecosystem. We're now getting to the final stages, where we're about to seek a permit to enable us to move from the exploration phase into the commercial recovery phase. We're doing that through the United States government. This resource represents an opportunity for us to ship those metals to the USA and help make America mineral-independent, a little bit like it became energy-independent with the development of shale oil fields. I'm wondering, too, one of the big developments we saw recently was an executive order from Trump supporting this kind of activity. Can you tell me a little bit about what exactly that meant and how those companies' mission fits into what the administration's thinking these days in terms of mineral supply chains? We know that the current administration under President Trump have, at the core, re-industrialization. They need to bring back jobs. They need to secure supply lines. That means you need to go and find where you're going to get these metals from. If it ain't grown, it's mined. Of course, what the West has done, including the USA, is, over the last decades, depended heavily on other parties. China and Russia are two that come to mind. When COVID arrived, it showed what happens when you interrupt a supply chain. It means that those industries that use those metals as their foundation can't continue, can't develop. Of course, in the last several decades, we've seen America lose those industries to other developing countries, particularly in Asia. What the Trump administration are very focused on is, how do we secure the supply lines? How do we bring back those jobs, bring back those industries into the United States? When the Trump administration was elected and the cabinet was chosen, we made the decision that we would seek permitting through the U.S. agency, NOAA, where the exploration and the commercial recovery rules already exist to enable us to lodge that application. We announced at the end of March that, in fact, would be our intention. Of course, we saw the executive order come out on April 24. What that really did was say, now we want to fast-track the development of these resources that we, America, have access to, both in our territorial waters and also in the expanse of waters. That's what we're going through the permitting process now with the federal agency. The executive order was an amazing boost for us, and it certainly focused the minds of those permitting agencies. What sort of timeline do you think you're looking at now that all these developments have happened? That's in the hands of the agency. However, the executive order made it very clear, and I know it's a personal wish of the president, that we see this resource in production during his 47th term. Of course, we are good users. We've spent the last 14 years getting ready for this moment. We've been able to put in front of them a very well-baked application. We've spent more than $600 million carrying out the research, both from the commercialization, commercial recovery system, and the environmental studies. We've really given them a fully-baked application, and we're hoping that we may hear news on that certainly much sooner than people expect, and certainly in 2025. When I first imagined all this, thinking about how far away some of these resources are from the places where they might be processed, thinking about how deep they are in the ocean, and not knowing much about the technology, my first kind of suspicion is, this sounds expensive. Can you tell me about the economics of this? How it compares to mining on the land, and whether this all kind of fits into prices today? Some of these materials are not necessarily at their historic highs. This resource offers many advantages. One of them is the high grade. It's often grade that is the real driver of the economics. It's polymetallic, so it's very high grades of nickel, copper, cobalt, and manganese, plus a heap of rare earths. We're not focused on them in the early days due to the processing method we're using, but in the future we will. If we think about copper, because that's the metal that most people are familiar with. While our resource has 1.1% copper, if we go and put the nickel, cobalt, and manganese into copper equivalent, it's well over 7% copper equivalent. Last year, the average grade of copper mined was about 0.6 of 1%. We're talking about an order of magnitude higher grade. That has a big impact in the amount of revenue we generate per ton of material. What people often forget is, what happens in mining is you have to invest an enormous amount in infrastructure. It might be because the resource sits buried underground, could be a kilometer, it could be four kilometers. I've been to some of the deposits in South Africa where they've spent seven years building shafts to get to the ore body. You've also got to build roads, and railway lines, and perhaps deep water ports, and places for people to live, and water systems, and everything else that goes with the development of a big resource. We don't have those same challenges. We have other challenges, of course, but we don't have those. That has a massive impact on the economics because it means that we can convert a production vessel and sail it on out there and be in production days later. It means that the scale opportunities for us are tremendous as well. It means we put more boats out there. It means that we can take advantage of existing, already built, and permitted processing facilities in various places around the world, such as we announced we're planning to do up in Japan. The intention is to build this infrastructure in the United States. Of course, an abundant supply of energy is a really important ingredient. America has it, and the administration is very focused on having more of it with some of the initiatives that they've announced because abundant, affordable energy is absolutely one of the key drivers to prosperity, and we're going to need a fair bit of it. There's been some environmental critics of deep sea mining. The other day, I saw a release out saying that there's a threatened whale species in the area. I believe a lot of the criticism has been around perhaps the unknowns of what happens when we begin industrial activities in these areas. Can you just tell me a little bit about the metals company's own work to try to mitigate some of these things and maybe your reaction to some of these studies? Look, it's the environmental angle that is probably one of the strongest aspects to support the development of ocean metals. If we go back to first principles, it makes sense that we carry out extractive industries in parts of the planet where there is the least life, not the most life. If you look at the amount of living biomass in the abyssal zone, we're talking about less than 10 grams of living matter per square meter, and most of that is single-cell organisms living in the sediment, about 75 to 80 percent of it. So it begins with the fact that there's not a lot of life there to impact in the first instance. But that still doesn't mean it's not important, and that's why we've been studying about what the impact will be. Will we actually disturb or will we actually help productivity in that area? And there are some signs that will be the case with some species because it's an area of very low energy, low food. And so we found abundances increasing in nearby areas where we had been down there collecting rocks in our commercial trials in 2022. So when I hear people like Greenpeace talking about potential impacts, they are truly grasping at straws now because we have been focused for the last more than 10 years on understanding those environmental impacts, and we've engaged 20 of the world's leading institutions. They've carried out the research. We funded, of course, but it all filled all channels back into an integrated environmental impact study where we look at a whole range of impacts. We look at the pelagic impacts and the midwater column and the benthic impacts, the sediment plume, and how fast a recovery rates and sediment plume is one classic example. And when I say sediment, think of it as driving your car down a dirt track. You'll kick up some dust. The question is how much and how far will it travel? And people like Greenpeace would argue that it's going to travel for thousands of kilometers. Our studies, which we sit alongside people like MIT, who've also published peer-reviewed studies, shows that down in the abyssal plain, the sediment, because we take in the nodules in the front of the machine, we separate the sediment and we spit that out the back, and it creates its own turbidity current. And it only rises two to three meters above the seafloor, and up to 98% of it settles in the area of impact. So contrary to the speculation from desktop studies, when you go out and gather real in-field data, they provide you the answers. And the answers are that the sediment plume doesn't travel those very large distances. It stays very localized. And so my answer to people with environmental concerns is it's reasonable to have them. And of course, our approach has been to address all of those, and the results of those many hundreds of millions of dollars that we've invested. And on top of that, many other contractors have invested since the 1970s. And for your American listeners, NOAA put together a programmatic environmental impact study, which was the work of four EISs done by contractors, plus their own work, which they submitted to Congress in 1995. And it basically showed that subject to more studies on the plume, which we have now done, there was no risk of causing serious harm in collecting nodules from the abyssal plain. Our work has further studied that and come up with the same findings. And we also have to remember that nothing has zero impact. But when you look at the two ecosystems, and you think about the abyssal plains and hills, which account for 50% of our entire planet, so it's the most common environment, there's no alternative use for it. You can't grow things there. People can't live there. It seems to be a perfect place to be carrying out this activity. Because today, where we're getting our nickel from is from tropical rainforest. And the problem with that is you've got to move all of the people who live in them out. You've got to tear down the rainforest and all of the biodiversity and biomass that is in it. And then you've got to dig up the soil where the metal-bearing ore is contained. And that's the beginning of the impacts. Then you have spillage into the oceans and rivers. And honestly, it's such a clear use case to be picking up rocks and turning them into these important base metals compared to the land-based alternatives. Excellent. And the last time we were talking, you were looking at permitting, I believe, for the International Seabed Authority. Can you maybe give our listeners just a little bit of background about that organization and why we're now hearing about you applying through the U.S. instead? Maybe give us a little bit of background on how that came to shift around. Yeah, sure. So in 2001, we applied for our first exploration license under the International Seabed Authority. And in 2012, our second one. So first was with Nauru, the other was the Kingdom of Tonga. And if we look at this industry, it almost got started back in the 70s. And then the United Nations got involved and a number of countries decided that the best way to approach this would be to agree a treaty for a number of countries. And there are now 169 member countries who have signed that treaty. There are more than 20 who have not. And of course, the most notable one who have not is the United States of America. And the U.S. from the very beginning said, we don't agree with this approach. We're not going to sign the treaty. We're not going to abide by those rules because no sovereign has ownership over the oceans. Every nation has freedom of movement, freedom of passage rights. They have access to the minerals on the seafloor. They have access to be able to lay cables on the seafloor. And what America did is stood aside, said, we still want to maintain those rights. The other countries who did sign UNCLOS, as I say, there's 169 of them now, said, well, when it comes to minerals, when it comes to metals on the seafloor, we're going to be bound by these rules. Now, in 1994, the International Seabed Authority was established and it had a very clear mandate. It had to put in place exploration and exploitation regulations to allow the commercialization of these resources. While protecting the marine environment. And so in 2001, they put in place the exploration rules. But here we are 2025 and there are still no rules that cover commercial production. And the main reason for that is that the system got overtaken by NGOs. And so the process was slowed down. Some bad actors became more vocal and they just used the process of delay. Now, at the same time, the United States in 1980 adopted a set of rules. We know them as DSHMRA and they're administered by NOAA, the federal agency, which allowed for exploration and also for commercial recovery. And so we made a decision in the second half of last year that's subject to a number of things. And one of them was getting a strong government in place that we would change our permitting strategy and apply through our U.S. subsidiary, TMC USA. And that's what we're moving forward with. And obviously, the United States are very clear on their rights over these metals. They've held two exploration areas in the CCZ, in the very same area that were granted to Lockheed Martin, and they've been in place for a long time. But it was really the president's focus on reindustrialization, their absolute focus on how to secure supply chain certainty that gave us the confidence that this resource could play a very important role in helping fulfill America's future ambitions. And while the support has come from President Trump, we do see it as a bipartisan issue. We had good support from the previous administration as well, certainly not the strong leadership we're seeing from this one, but we think the critical mineral challenge is one that will not go away. It will only become more important. And I think that for the last couple of years, we've been all talking about bits. We've been talking about AI and these new industries. And of course, what people are now starting to realize is that the atoms are super important because we need to build all the infrastructure to supply the energy to enable this to happen and to supply the industry to attract jobs back into the United States. And so it's an exciting time. And we're delighted with the support we're getting from this administration. And we're just, we're delighted at the prospect that we can get this resource into production on a short timeline. And now related to that, you know, I'm always curious of whether the market is, is that they're following these trends closely enough to know what's going on. And I think it took a while for people to kind of realize the importance of critical minerals, right? But you've recently announced a $37 million investment from Hess Capital, I believe it was an $85 million investment from Korea Zinc into the metals company. Can you tell me a little bit about, first of all, the significance of those investments, but also maybe how have you seen the kind of market evolve and its reception of critical minerals and understanding its importance to, let's say, energy transition? Well, you're right to suggest that people have been a little bit slow on the uptake on the importance of critical minerals, but they are getting it now. We wanted to focus the business more and more on the USA. And so we were delighted to see Michael Hess. And in fact, we announced Alex Spiro to join our board only a week ago. And of course the Hess family are very well known industrialists in the USA. And so to have them commit a very sizable piece of capital behind this idea from their family office was a really important initiative because they help manage some of the stakeholders who are going to be very, very important. And they've got an amazing track record of building globally successful companies. And I'm sure some of that will rub off onto our efforts, and it already has. This is not a passive investment. This is an investment, which I imagine they'll probably invest more in the future if the opportunity arises. But what we're seeing at the moment is lots of effort to help us achieve what our corporate goals are. And of course, Careers Inc. is similar. We've known that company, well, we've known the company for decades, but we've had good dialogue going for the last year. And their chairman is very committed to exploring how they can access more USA opportunity. And from our perspective, they are, outside of China, the only company that we think can help fast track what we want to put on the ground in the United States. And they bring expertise, clearly they bring capital, but it's that expertise which will enable us to move a whole lot faster and with much less risk than we would otherwise be able to. And we have an enormous amount of inbound inquiry coming our way these days. Since we announced at the end of March that our permitting pathway would be USA focused, people see that there's some certainty now about us being able to secure a permit to be able to go and develop this enormous resource. One thing I haven't mentioned, Taylor, is we know we've got a defined 1.6 billion tons on the areas that we've applied for through the US government. And we estimate with some other ground that we've added to the application, there's another half a billion tons. We're talking well over 2 billion tons of nodules sitting on the seafloor that are polymetallic. But if we put them into copper grade, they're more than 7% copper equivalent. There is no other resource on the planet that fits into that category. And so having strong partners who can help us develop this asset in the same way that we have all seas is one of our most important partners who are experts in the offshore industry. And it was their flawless execution that enabled us to run those commercial trials so well in 2022, because for the past 40 years, they've been solving very complex production issues in the deep ocean. They lay pipe 24 hours a day, 365 days a year when they're on production. And so they bring all of that learnings and all of that know-how to help us avoid making mistakes that early stage companies would otherwise make. And we see Careers Inc. and the Hess family coming to help us do that to an even greater degree. So a lot of reasons to be excited over the metals company. To wrap it up, my last question for you is, what are maybe some of the challenges that you see in the horizon in 2025? What are your maybe biggest hurdles that you're looking to face in the next couple of months? Well, we're going to be depending on getting this permit on the commercial recovery, of course. But we absolutely want to get focused on building that infrastructure in the USA. We're very encouraged by the administration's support that they've been offering us on how to get processing quickly permitted. Of course, I don't have to worry about capital so much now, because not only do we have access to lots of capital on the balance sheet, we'll close the quarter with more cash than we've ever had in the bank. I think the pro forma number we gave was about $120 million. So that's a good feeling. But of course, capital management is one of those things that we've proven to be pretty good at. But we've now got more expertise around the table. And the Hess family helped with that, of course. Alex Ferro brings an enormous amount of expertise in that area as well. So we're very happy to have his services available to us as to Careers Inc. But managing that capital so that our equity holders get the best return on their invested capital is always of the highest priority for me. Our shareholders have been amazing to support us. We're enjoying building a strong retail support base as we are welcoming institutions. If you look at the forms that have been filed now, you see some of the world's biggest institutions turning up on our share registry. I think providing we continue to show them that we look after their capital, then we'll continue to see that group of institutional owners grow and grow. So it's going to be an exciting little run-in into first production. And we've been at this for 14 years. And we're just grateful for the fact that President Trump and his administration is clearing a pathway to help us make this happen quickly. Thank you so much for joining us on the day and look forward to having you back for another update soon. Thanks, Taylor. All right, listeners, thank you so much for listening to the latest episode of Energy Evolution. I want to give a special shout out to our incredible podcast team members. I am once again, Taylor Kirkendall, but some of the other people that work for this podcast include my co-host Eklavvya Gupta, Camilla Nasher, Christopher Coates, Dan Testa, Karen Willenbrecht, and our wonderful agency partner, The 199. To stay up to date with our upcoming episodes, be sure to subscribe to Energy Evolution on your favorite platform. If you have any ideas for future themes or guests, we'd love to hear from you. All you have to do is shoot us an email at energyevolution at spglobal.com. And look, if you liked what you heard today, do consider sharing the podcast with others or leaving a review on your preferred podcast platform. We can't wait to bring you more fascinating discussions in the future. Until next time, take care and stay tuned for more from Energy Evolution. Bye. Bye. Bye. 


---

## 2025-07-24

### Leveraging Heat as an Asset in Data Center Operations
**Publication Date:** 2025-07-24T03:00:00
**Episode ID:** 5068

**Full Transcript:**
Hello, and welcome to the Energy Evolution Podcast, where we delve into the ever-changing realm of energy and sustainability alongside some of the influential thinkers and pioneers in the energy transition space. I'm your co-host, Taylor Kirkendall, and today we have the privilege of speaking with Rich Powell. He's the CEO of both the Clean Energy Buyers Association and the Clean Energy Buyers Institute. SEBA boasts a membership of 400 companies that collectively represent over $15 trillion in market cap. As of the second quarter of 2024, the members have secured contracts for 84 gigawatts of new clean energy, so major customers of clean energy. See, Rich is at the forefront of this mission to foster a global community of energy consumers that are committed to advancing a sustainable energy future, and while trying to tackle some of the challenging market and policy obstacles that come along with trying to achieve a carbon-free energy system. As we navigate the current political climate, we wanted Rich to share some of the perspectives on challenges that clean energy buyers are facing over the next four years, particularly in light of this big recent shift in federal government that we've seen in the US. He's also going to highlight some potential opportunities that could emerge during this period. We're also going to discuss how the demand for clean energy is evolving under the current administration compared to previous years, and whether SEBA members are starting to adjust their strategies for clean energy procurement. Rich is also going to elaborate on maybe the relationship between economic growth and this increasing demand for carbon emissions-free electricity, and explore how those two factors are connected. We're also going to dive into the critical issue of permitting reform, and its significance on facilitating some of these clean energy projects. Finally, we're also going to be talking about the federal technology neutral energy tax credits, discussing some of their relevance to clean energy procurement, and also some of the potential ramifications of this repeal that we're looking at in the US Congress right now, and what that might mean for energy prices and job creation. Without further ado, let's hear it from our talk with Rich. Now, a lot of things are moving on the federal government level. We just want to remind you that this interview took place in the last week of May. Hello. Thank you, Rich. Thank you very much for coming on Energy Evolution. So happy to be here. Thanks so much for having me. Absolutely. Rich, to start off, for our listeners that are not familiar with the Clean Energy Buyers Association, can you just tell us a little bit about your group, what you do, who your members are? Sure. So Clean Energy Buyers Association, we call it SEBA, is the largest group of corporate buyers of electricity in the world. And everyone's united with a common quest to find low-cost, reliable, carbon-emissions-free electricity systems. We've got 400 members, 200 of those are buyers. They range hugely across all areas of the economy. So we've got folks from big technology, we have manufacturers of everything from automobiles to consumer devices. We have metals and minerals and retailers and food and ag. If you buy a lot of American electricity, you're probably a member of SEBA. Excellent. And we've seen a really big shift in the federal government in the last couple of months, especially at the top, seems to be a lot less interest in things like the Inflation Reduction Act that supported some of these technologies. I'm wondering, given this kind of shift that we've seen in government, what are some of the challenges you foresee for some of your clean energy buyer members? And on the other hand, what are maybe some opportunities that might exist now that didn't exist a couple months ago? Yeah, well, you know, to take a big step back, the biggest thing that's happened in energy in the last year or so has nothing to do with the election or anything else. It's returning America, at the very least, to gross mode in electricity. So we had pretty flat electricity demand in the last two decades. There were a lot of changes in the margins. We built a lot of renewables in the U.S., we built a lot of gas in the U.S., we retired a lot of legacy assets that changed the electricity mix. But the overall size of the grid stayed about the same through that period. It was actually pretty extraordinary, totally unprecedented historically in the U.S. to have flat low growth. And that's now changed. So 2024 was the first year where we saw significant low growth again since the 1990s. And that appears now set to continue at least through 2030 and maybe beyond. Maybe this is just, again, the new normal that'll take us all the way through 2050. We are out of practice in rapidly expanding the size of our grid. And I mean that on all fronts. We're out of practice on building huge amounts of new generation, we're out of practice on upgrading the grid we have, and we're out of practice on expanding the grid, like meaning building new large wires and expanding the grid around the country. And so we need to get back into gross mode on all those fronts. And so the new political situation, there are absolutely opportunities in all that. There are also some challenges we're working through right now. I mean, the opportunities in this are huge. We have an administration coming in and we have increasing bipartisan support in Congress to do the kinds of fundamental reforms to our permitting processes and our transmission processes that I think people have talked about for a long time, but not taken a lot of action on. We now have an administration, and I think a Congress that is excited about taking deep action on those things. So that is a really exciting opportunity, and we're engaging deeply with policymakers on that. Some of the challenges have been around things like the tax incentives. And so we've been very engaged with folks in Congress around making sure that there's a very clear policy ahead on those tax incentives in order to keep electricity systems low cost and reliable as we get back here into gross mode. And we've done a lot of analysis, which I'm happy to talk about, which shows that if you hastily take those incentives away, you could see significant increases in electricity prices, you could see significant problems with reliability, and you could see wider economic impacts as those electricity prices then translate into a hit for the broader economy, not just for our great big businesses, but for all of our customers across the US as well. Can you maybe elaborate on a little bit? What does your research say the impacts are going to be, just to maybe put a little bit of color and scale around that? Sure. So we've worked with an economic modeling firm called Nira Consulting, and we've taken a look at what would happen if you took the tax credits away. To get technical for a minute, I'm going to say these are very specific tax credits. So they're often called the technology neutral tax credits, and they're production tax credits and investment tax credits for all new carbon emissions-free technology. So this would be for wind, and solar, and nuclear, geothermal, hydro, and thermal resources like gas and biomass, if you put carbon capture on those things. So those things are all now on a level playing field in the tax code. They all receive production tax credits and investment tax credits when you build new stuff. And so we looked at what would happen if those were rapidly taken away. And what our research at a national level found was a pretty immediate and pretty significant price impact. So nationwide, electricity bills go up about 8%. It's a little lower, thankfully, for residential payers. For our customers, we think of them something like 7%. And then for our businesses, and this is big corporations, and this is medium-sized, and even small businesses, the folks who pay commercial and industrial rates, their rates would go up something more like 10%, and this would happen immediately. And then we've double-clicked on that, and we've looked at a lot of individual states. And unfortunately, we see that there are really acute price increases in a number of states. And so places like the Carolinas and Illinois and a few other places, they see much more significant double-digit increases. Some states even looking at in the 20%, Maine, for a couple of reasons, is sort of more in the 20%. I'm happy to go through that in greater depth, but I think it's just important to know when you rapidly increase electricity prices, that also has other follow-on effects. Like electricity is something that everybody buys. It's just a fundamental input for our economy. So if you make a retailer's electricity prices go up, that means grocery prices are going to go up there. Also, if you rely less on these new resources that are going to come out of the grid because they become more expensive because the incentives go away, you're going to have to rely more on continuing to run the old resources that are by definition more expensive because they're about to be replaced. And so when you do that, for example, when you increase the run rates of a lot of older natural gas-fired power plants, for example, not only is that pretty expensive to do, but that also increases the costs of natural gas across the whole economy. So home heating becomes more expensive, and natural gas as an input to manufacturing processes becomes more expensive too. So suddenly you have this kind of compounding economic effect across the whole energy-dependent part of the economy, which is basically the whole economy. And suddenly you see really significant hits to GDP in a number of states. You see really significant employment effects, often in the thousands, even tens of thousands of jobs in key states across the country, and deeper impacts to just residential pocketbooks. If you look at just the electricity prices themselves, you're probably talking between like $100 and $200 a year across the country in electricity price increases. But then when you layer onto that the increased costs of all goods, and you layer onto that the increased costs in natural gas for home heating for so much of our country that heats their homes that way, you start to talk about pretty significant price impacts on a lot of folks around the country. And I think a good point to remember here too is a lot of people talked about these tax credits being relatively safe because of the spread across red and blue states. And you just detailed a whole lot of potential impacts of the repeal. But I'm just wondering, what's that conversation sounding like in D.C. right now? Are you hearing people receptive to this message of these things are important, we need to keep them in place, or is it still a fight on the Hill? Well, for those that are not D.C. policy nerds that were not following the votes that happened last week here in D.C., we had a setback on this issue. And so the bill that came out of the U.S. House of Representatives, so this isn't done, this is the beginning of the negotiation here, but the bill that was finalized, I get my dates messed up, either very late Wednesday night or very early Thursday morning. I think the final vote was really early Thursday morning and then everybody collapsed and went to sleep for the rest of the day. The members kind of did something heroic in getting this thing across the finish line. This is the massive bill that's going through the reconciliation process right now. It's fundamentally a tax bill. So this is the bill that would extend the TCJA, the Tax Cuts and Jobs Act. It was kind of the great big tax bill that President Trump in his first term, working with Speaker Paul Ryan, got across the finish line back in 2018 and made really significant adjustments to the whole tax regime for the country. Net was a very good thing for the country, right? Greatly increased economic growth. Everyone's 401k certainly benefited from this because we made a big, long overdue adjustment to corporate income tax rates. It was a very important bill that was set to expire at the end of this year. So all of those tax rates were going to snap back to their previous levels. And so right now, Congress has the unenviable job of figuring out how to extend that whole tax regime, which is a big, complex, very expensive thing to do. And so they were looking at that extension and then how to pay for that extension. And unfortunately, these technology neutral tax incentives and some related ones for clean manufacturing have gotten caught up in that conversation. Portions of those are being used to pay for that broader tax extension. So what we saw last week was the tech neutral credits and the clean investment credits significantly pared back, meaning that they ended a lot earlier than they would have otherwise. And the terms of the credits significantly changed. So rather than being able to collect the credit if you've just started a project, the project has to be completed by 2028, so the date when they are turned off. That's a really big hurdle in energy. Energy projects take a long time to build and put into service. So it basically means that a project isn't under construction today or maybe several years into construction today, they wouldn't be able to take this credit by 2028. They also added in the legislation some new language, I think well-intentioned language, that is trying to get it making sure that US taxpayer dollars don't enrich state-owned companies from countries that we see as foreign adversaries. So this is called foreign entity of concern language. That's the term of our FIOC. And they added some really significant new restrictions to these tax credits to go through this FIOC process, again, with good intentions, but the new process they've created is so onerous that I think it could be described as a poison pill for all of the credits. So it's not just that they end now in 2028 as opposed to ending in 2033, it's that the ones that can still go through have to go through a process that might be impossible for really almost any project to go through. So that's what happened last week. And we did see, though, that there was significant Republican support in the House for continuing these credits in some way, shape, or form, and dozens of members were very active on that, very public about that. And folks in the Senate on the Republican side, a number of folks have seen kind of what came out of the House and said, well, we'll need to make really significant adjustments to that in the Senate if we're going to preserve American competitiveness and electricity affordability and jobs in our states. And so that process has now gone over to the Senate, where they'll have to take the House's bill, make fair modifications, pass that, and then to finalize the whole process, it's got to go back. Both chambers have got to vote on the same bill. So eventually, they'd have to go back to the House and they'd have to pass it as well before this whole thing is done. And you mentioned earlier that we're not really used to this growth mode that we're going back into in terms of how much electricity demand we need. One of the big things that people talk about a lot in the energy space and how hard it is to build things, as you just mentioned, permitting reform needs to be done to make that happen. I wonder, can you tell me a little bit about what you see is the appetite for maybe getting in there and making it easier to permit and build projects in the US and what the appetite is for maybe tackling that this year in DC? Sure. So we need to do a couple of things on the permitting side. So we need to make it more straightforward if you've got a new generating project, whether that's a new small modular nuclear plant or a new solar farm or a battery facility, we need to make it more straightforward to site and permit that thing. We need to limit the length of the lawsuits around those things because our current permitting process invites the public to come in and participate in the process and then gives the public a chance to sue over the results of that process. And that could often stretch on for years, sometimes even decades or more. So we've got to limit that. And then we also have to improve the process and give more certainty to expanding our grid. So both upgrading the grid we have and then building new long haul transmission lines. So those are the major pieces that we need to do. The good news is that almost all of those pieces were included in a big legislative package that folks in DC called the Manchin-Barrasso Bill, because it was sponsored by former chair Joe Manchin, now retired from West Virginia. And then his equivalent in the Senate, Senator John Barrasso, who drove that forward, both in the Energy and Natural Resources Committee. That legislation moved really significantly last year. And it was actually an attempt to get it fully passed in the quote, unquote lame duck. So the period after the election and before the end of the year, last year in DC, we didn't quite get it across the finish line. But along the way, a lot of people voted for that bill and very bipartisan. So this was traditionally an issue where there were really deep partisan divides. You saw a lot of Democrats that were really concerned about changing the permitting regime, because often environmental activism has been more about stopping stuff than promoting stuff. And so they're concerned about changes to that. And then you traditionally had a lot of conservatives who were skeptical about building a lot of transmission across the country, because they were concerned about state's rights and who paid for that. And this was really sort of a coming together of all the camps, just recognizing we've got to get back into build mode, and we've got this huge new electricity demand coming. And so we're really optimistic that in this Congress, especially once we're through this reconciliation bill, we can get back to a robust bipartisan conversation about getting this done. I think the crucial thing for folks to remember is that this is real energy policy, and energy policy can't move through the US Congress through this reconciliation procedure. It's got to be something that just moves in normal, regular order, particularly through the Senate. And that means 60 votes are required in the Senate, and that means it's got to be bipartisan. So regardless of what it is, it's going to have to be a big compromise where all sides give a little to get a lot. And we do think that there's a solution there that at the very least can make a lot of positive motion forward in this Congress, but maybe even get completed in this Congress. And I think you'd have an administration that would want to sign a big piece of legislation like that, because it knows how urgent it is that we fix this permitting regime and get us building fast. And when we talk about these zero carbon energy resources, I know we mentioned solar, wind, nuclear, those jump to mind really quickly. But we also have an administration that for the first time in a while is talking about things like coal plants again. I'm wondering, does carbon capture and some of those technologies fit into this as far as SEBA's concerned? Or do you guys have a position on whether that's a viable technology for meeting some of these needs? So our position is that we are four carbon emissions-free technologies, and then our various members decide which one of those work for themselves and their approaches and their philosophies. And so we've got folks that have a variety of perspectives on this. Some have come out very publicly and said that things like natural gas with carbon capture would work within their framework in certain circumstances. And so just notably both Meta, Google, for example, have come out and said that it would work to advance their approach, and they've got different projects going in different places on that. And I think the key questions in folks' minds are, what about the methane that goes into the machine? Is that responsible methane? So are we making sure that there aren't a lot of leaks along the way? How much of the CO2 are you capturing when you burn the gas and turn it into electricity? Are you doing something with the CO2 you didn't capture? So are you pulling that back out of the air, for example, with carbon dioxide removal or something else? And then what's being done with the CO2 once you capture it? Is it being permanently stored? And so I think those are the big questions that people are grappling with as they think about this. In terms of economic viability, that does remain to be seen. And so I'm very hopeful. I'm sort of a longtime advocate for more research and innovation in those technologies. The trouble is that right now, we don't have an operating gas plant up and running anywhere in the US with carbon capture. We've got lots of gas plants running, so we know that technology works really well. And we've got lots of carbon capture running, so we know that works really well. We do tens of millions of tons a year over time, about 10 million tons a year in the US of capture. We know that works well. It can be permanently sequestered. It's safe and effective. We haven't put those two things together in the US in a while. We had one back in the 1990s, believe it or not, a small gas plant with carbon capture running in the Northeast. It operated just fine, so we know it could be done. But people get a lot more confident when there's something out there for them to go and to look at, and then ideally when there's a couple of different people with a product offering around it. So there are a number of companies that have live offerings now, and a number of our buyers of electricity are evaluating those offerings, and I think it remains to be seen. Last year was a breakthrough year for nuclear amongst our members, and so we had multiple different members sign some version of power purchase agreements, or bring old plants back online, or make investments in advanced nuclear companies. So companies got to conviction on advanced nuclear last year. We'll see if folks come out publicly and get to conviction on gas with CCS in particular this year. And you mentioned growth again. For the longest time, any kind of economic growth or demand growth necessarily meant that we were going to have increased emissions, and that story is definitely changing. But I'm wondering, are we technologically, are we there at the point where we can realistically say, hey, we can go out here and grow the economy, but also grow electricity demand without increasing emissions, or are we a little ways off from that still? So when you think about the kind of growth we have today, we're experiencing right now, we actually need grids to grow with more capacity through a whole year. So that means that new capacity needs to be added to grids that run with high reliability, 24-7, 365. And so we're getting very close to the point where all of that growth could be met with new low-carbon electricity. But there are portions of a year when we still struggle. So we've got terrific tools in solar, and in wind, and in battery storage. And so all of those things, there are huge amounts of those that are commercially viable today. They're sitting and waiting in interconnect queues around the country. This gets into a nerdy topic, but there's just an enormous amount of this that is already applied to get itself connected to the big regional transmission organizations that run our grids around the country. Our whole grid today is about 1,100 gigawatt hours, or 1.1 terawatts. Waiting in those lines around the country, there's 1.5 terawatts, or 1,500 gigawatts of new projects that want to get connected. So we've got a lot with those. But when you put together those solar projects, and those wind projects, and those battery projects, they typically don't fully cover all 24-7, 365 hours in the year. And so we're still needing what folks often call clean, firm technologies that can actually run all of those times. So they could be dispatched in the particular times when the sun isn't shining, and the wind isn't blowing, and it's too many hours away from when they last did, so our existing batteries can work. And so those clean, firm technologies, we sort of know what the candidates are. We know it's small modular reactors or advanced nuclear. We know it's enhanced geothermal technologies. We know it's really advanced batteries. People call this long-duration energy storage. So things that could store electricity not just for a couple hours, but often for weeks. And we know it's some form of carbon capture, right? So gas plants with carbon capture, for example, best candidate. Oh, and maybe commercial fusion, which, fascinatingly, is closer on the horizon than a lot of people probably realize. So those are the candidate technologies out there. None of them today are running at big commercial scale. So none of them today are quite where solar, and wind, and short-duration battery storage are. And we need to get at least one of those, and ideally a couple of those, up to full commercial readiness very soon if we're actually going to meet all of that low growth clean. But the good news is that we can meet the vast majority of that low growth in a very clean way. So regardless how you look at it, the carbon intensity of our economy will continue to go down through this period of growth. But obviously, the carbon intensity going down is only good for so long. We actually do need to completely reduce CO2 in our economy. And so we need some of those clean, firm technologies to fully beat that gap. Excellent, Rich. Thank you so much for joining us on the show today. Thank you so much for having me. Really enjoyed the conversation. All right, listeners, thank you so much for listening to the latest episode of Energy Evolution. I want to give a special shout out to our incredible podcast team members. I am, once again, Taylor Kirkendall. But some of the other people that work for this podcast include my co-host, Eklavvya Gupta, Camilla Nashert, Christopher Coates, Dan Testa, Karen Willenbrecht, and our wonderful agency partner, The 199. To stay up to date with our upcoming episodes, be sure to subscribe to Energy Evolution on your favorite platform. If you have any ideas for future themes or guests, we'd love to hear from you. All you have to do is shoot us an email at energyevolution at spglobal.com. And look, if you liked what you heard today, do consider sharing the podcast with others or leaving a review on your preferred podcast platform. We can't wait to bring you more fascinating discussions in the future. Until next time, take care, and stay tuned for more from Energy Evolution.


---

## 2025-07-22

### Powering AI Data Centers: Eaton on Infrastructure, Cooling, and Whats Next
**Publication Date:** 2025-07-22T17:02:19
**Episode ID:** 5069

**Full Transcript:**
Hello and welcome to the Energy Evolution Podcast, brought to you by S&P Global Commodity Insights, where we zero in on all things energy transition. I'm your host Ikhlavya Gupte, and in this episode we will try to shed some light on an event that shocked the European electricity industry and sparked debates about the reliability of our decarbonized power systems. The incident in question is of course the recent power outage on the Iberian Peninsula, which plunged much of Spain and Portugal into darkness for several hours in late April. To discuss this in more intricate detail, I'm joined by correspondent Camilla Naschert, who has been closely following not only the event, but also the fallout from the Iberian blackout. Welcome Camilla. Hi Ikhlavya. So Camilla, I guess many of our listeners will have closely followed this story, but can you please give us a quick recap of what happened? Yeah, so the outage happened on April 28th, around 12.30pm Spanish time, when data from transmission system operator Red Electrica showed a plunge in power consumption, from around 25 gigawatts to 10 gigawatts, while there were also reports of a complete loss of load. The blackout disrupted key infrastructure across both Spain and Portugal, think public transportation, traffic signals, hospitals, manufacturing plants, digital payment systems, etc. Banking systems were also interrupted, metros and trains were stopped, and many hospitals had to activate on-site generators and reduce services while power was being restored. Now by April 29th at 7am, the power system had recovered 99.9% of its demand. The restart was also helped by imports from France and Morocco. And since then, we've had an investigation by Spanish authorities into the cause of the outage is ongoing, and power plant operators and the TSO are denying responsibility and blaming each other. And the context of that is that whoever is found liable may have to pay hefty fines. Now to get into the details of the event and its consequences, let's hear from Kerry Tucker-Smith, Senior Analyst for European Power, and from Alexandre Mass, Power and Renewables Analyst, both from Commodity Insights. Kerry, Alexandre, thank you so much for joining us on Energy Evolution. So today we'll be diving into this Spanish-Iberian power blackout that we saw last month. We're now a month on from the event and still asking a lot of questions around the cause and the implications. But do you want to start by describing the event of this outage, also maybe in historical context, how significant was this of an event? Yeah, hi there, and thank you for having us on the podcast. So what we saw on April 28th was a cascade of events in the Spanish grid, which over the course of just a few minutes saw a whole system blackout in Spain, Portugal, as well as parts of southern France. And as a result, there was widespread disruption, everything from communication to payment systems to transport, industry, everything went down. So in Western Europe, blackouts of this magnitude are rare. The blackout in Spain is likely the largest of this century so far, but they do happen. The last major blackout in the region was in the UK in August 2019, with about a million people affected, but it was fairly short in duration, so lasting just a few minutes to about an hour. Further back in 2006, there were blackouts in Western and Central Europe. Now these again were very short duration, but they were very widespread geographically. You have to go back to Italy and Switzerland in 2003 to get an outage of a similar size where a tree touched a power line leading to a series of grid failures and almost the whole country lost power for anywhere between an hour to 12 hours. But smaller scale blackouts are more common in Eastern Europe, for example. I think just this week, North Macedonia experienced a countrywide blackout, which lasted approximately around one hour. And if we're just going back to what happened last year, in June 2024, we have been seeing that the Balkans region has experienced widespread blackouts. And this is a very interesting topic. But going back about the implications of the blackout, I think that we have been seeing multiple impacts in Spain regarding its energy markets. Maybe something we can then show afterwards. Yeah, no, let's get into that. How has the market reacted to this event? So on the day of the blackout, intraday auctions in Spain, which are held continuously and on an hourly basis, were halted because the grid was obviously managed by the TSO for 24 hours basically. And then basically the auction resumed the day after. But we can kind of summarize the impact on Spanish power markets with the two following impacts. The first is basically, I think since 12th of May, we have been seeing the Portuguese TSO REN just limiting the available interconnection capacity between the countries, between two countries. This means that basically Portugal can import less power from Spain. And something interesting is that limitation has occurred from 9am on the morning to 7pm on the evening. And usually during this timeframe, we are seeing that Portugal is importing power from Spain as solar generation is rising in the country and kind of pressurizing prices. And this time, because capacity was lower, we have seen that surplus being kept in Spain. And as a result, the two markets kind of coupled. Something interesting is that the Spanish market has seen its lowest ever power prices since then. I think it was minus 15 euros per megawatt hour, which is a very interesting topic as only two years ago, those negative prices didn't exist. If we are going to the maybe second impact, I think that the Spanish system has been running on the safe mode basis since the end of the blackout, which is something confirmed by the energy ministry in Spain. And this has led to very high redispatch levels in Spain. But this means that basically, even still we aren't seeing no thermal capacity being selected on the day ahead market. The TSO has lifted that amount of thermal generation since the end of the blackout. And this has no impact on day ahead prices nor on futures, but this has profound impacts in terms of electricity bills and gas demand in the country. Maybe if we can take a step back and look at the Spanish electricity grid and the power mix in Spain, obviously, we've seen huge shifts in that across all of Europe, not just in Iberia. But what can we say about how the power mix and the grid in Spain have evolved in recent years, obviously, as the energy transition has gone on? So what do we know about that? I think you're right. Basically, the transformation of Spain's generation mix really mirrors what is the plan for several European countries in terms of decarbonization of their power system. Only a decade ago, Spanish generation mix was dominated by coal, gas, nuclear, and only very little by renewables. And in 2024, wind and solar generation only accounted for nearly 45% of its power demands. And when we include hydro, that figure rise to nearly 60%, which is very impressive. And obviously, as you pointed out, this shift has been driven by a fast increase in solar capacity. Basically, in 2024, capacity was above 40 gigawatts, which is nine times above its 2015 level. So yes, very impressive and fast transformation. I think that a key development has been the rise of decentralized energy assets. Just to be clear now, nearly a quarter of the Spanish solar fleet is installed on rooftops, which is operating kind of outside TSO control. Meanwhile, when the country was retiring coal plants, for instance, those plants were large capacity plants, which were grid connected. So you have also that shift from large centralized plants to very small, tiny decentralized fuel plants. And the final point about the transformation of the Spanish energy system is the fact that Spain aims to phase out its seven gigawatt nuclear fleet by 2035. And it's supposed to be starting in 2027 with the first plan decommissioning. And basically, this is the last nuclear phase out policy standing among the big Western European countries. And despite the recent intensification of debate around that policy, there are currently no plans to reverse that phase out, basically. And when we now start to think about the causes of this audit, we're seeing obviously an ongoing investigation by authorities in Spain. And then this, I guess you could say tug of war between the grid operators and the power plant operators. Now, as we speak, clearly this is a fluid situation. What do we know? What can we say for sure, if anything, about the causes? Yeah. So at the time of recording on 23rd of May, as you said, the exact cause of the outage isn't known. Lots of investigations underway. In the immediate aftermath, there's been a huge amount of finger pointing, blaming everything from the amount of renewables in the grid, creating instability and low inertia, blaming the low levels of nuclear on the system, blaming lack of grid investment, some suggestions of IT and cyber attacks. And as you say, just yesterday, there were reports from the chair of the Spanish TSO, Ari's parent company, blaming large power plants for not doing the job of voltage control, which needless to say is being pushed back on by the major generators. So what we do know is that we have a timeline. We know that around 30 minutes prior to the blackout, there were reports of unusual frequency oscillations in the wider European grid. And SOE, the European TSO Association, has said that it dealt with those oscillations, that the grid was normal at the time of the blackout. So it's currently unclear if and how these oscillations were linked. We know that half an hour later, there was a loss of generation at a substation in the Spanish province of Granada, followed by two successive substation failures in other regions. So these amount to a little bit over two gigawatts loss in generation and resulted in a drop of grid frequency. In the last week, there has been a lot of talk from the energy ministry in terms of overvoltage on the grid. But we currently do not know what triggered that initial drop in generation. Did it cause the overvoltage or was it reacting to the overvoltage? That generation loss then triggered load shedding. That's an emergency measure designed to stabilize the grid. But that was insufficient. We're not sure why it didn't work. Finally, as the frequency continued to fall in Spain, the Spanish and French grids disconnected, which led to another massive disconnection of generation and led ultimately to the blackout. So from this initial generation loss, there is a cascade of events triggering the full blackout, all of which happened in just a few minutes. And there's likely to be a whole web of complex interlinked factors, which if they didn't set the ball rolling on the blackout, at the very least, they contributed to the blackout by making the system more vulnerable. I think that just was what Kerry was saying. We know that renewables on the grid was very high at the moment of the blackout, but they were not exceptionally high. Wind and solar were accounting for nearly 70% of power demand at the moment of the blackout, which is high. But as I said, we have been seeing hours with higher penetration of renewable on the grid. Just in 2024, we have been seeing around a thousand hours above that level. And something interesting is that super recently, right before the blackout in April 16th, Spanish grid at least met all of its demand with renewables, so wind, solar and hydro, which is basically the all time high record. But I think the figure pointing needs to be taken very cautiously. There have been a lot of commentary suggesting that there is a lack of inertia. That topic has kind of reverted back to voltage control recently, but there is a lot of conflicting arguments, as said Kerry, and there is not a clear single reason behind the explanation of the blackout. As said Kerry, it's massively complex. Just maybe opposing what we are seeing at the moment in Spain versus what happened last year. So as I said previously, last year we saw a blackout, widespread blackout in the Balkans in June, and we only got the report of the explanation of the causes back in February 2025. So this I think underlines well the complexity of those events, as said Kerry, it's a cascade of very localized and complex power system defaults. In the case of the Balkans, the cause was something very different to what has been discussed previously. For instance, the initial short circuit has been triggered by a vegetation growth which has been too close from the power lines. So this is interesting to see how those events are being analyzed basically. Yeah, in this case we simply don't know yet, but we can say that a renewable-led future needs a modernized grid and a way of managing the grid in a modern, clean, fast response way. And other markets have been perhaps a little bit more forward-thinking in this area. So just yesterday, California's system operator approved a 4.8 billion transmission plan to support the grid against rising demand, to integrate renewables and batteries, to provide upgrades for grid-enhancing technologies to enhance transmission capacity. Similarly in the UK, which is like Spain, a high renewable island, the grid operator NISA has been working to decarbonize its grid balancing and enhance stability using its Stability Pathfinder projects. So just in March, GB's first of five grid-forming batteries was connected to the grid in Scotland under this initiative. So those are batteries that they're an advanced battery storage system that can independently stabilize and support the grid. GB in general has been able to integrate a lot more batteries onto its grid relative to Spain, and a lot of those are active in frequency services and in system balancing. So we estimate over 8 gigawatts of batteries on the GB grid compared to just over 2 gigawatts in Spain, which by 2030 is expected to rise to 30 gigawatts in GB compared to just 11 gigawatts in Spain. And batteries are crucial to the markets for helping them integrate the rollout of renewable power. And I think the final point about this is that just looking at Spain's position in Europe, you can see that Spain is very isolated, both from a geographical point of view, and this has direct consequences on how the market is operating. At the moment, Spain has only one connection linkage, I'd say, with continental Europe, through France basically. And despite having this interconnection, this interconnection cannot mitigate that isolation as it represents basically less than 10% of its peak demand. We are seeing that in contrast to European countries such as Germany, Poland, which can rely on several other neighbours in case there is an imbalance in their markets, Spain has very limited support from its neighbours. In the case of the blackout, the interconnection just tripped. At the moment, the frequency dropped below safety measures. What does it mean? It means that if we were keeping that interconnection online, the frequency drops will have basically safety issues on electrical devices in France and Spain, and could potentially transfer that blackout in France more widely. And ultimately, the support from neighbouring countries during the blackout was very limited. Obviously, Morocco and France helped during the reconstruction of the grid, but this remained very limited. And looking at how fast the Spanish grid can be reconstructed, it means that further support would have been basically a great help. Just looking at what is happening also, there is a new line in construction at the moment between France and Spain, but it will be only commissioned by 2028. And despite seeing the interconnection capacity doubling by then, we're seeing that this isolation in Europe will not be fully compensated. And it's an area where we could see both blackout potential changes as maybe more political appetite for new interconnection projects could emerge, basically. That's really interesting. And I mean, if there's listeners out there like me who are just frantically taking notes, because there were so many interesting points just raised by you both, there will be a report by your team the same week as we're launching this podcast, so early June. So subscribers to the Commodity Insights analytic services will be able to read a little bit more with the caveats that you just laid out around ongoing uncertainty. So maybe the final point is around the kind of discussion that you just raised already about grid security. We've seen reactions across Europe, executives at every utility being asked about potential repercussions or similar events happening in other markets, even as far as the US. Why was this so impactful that it really prompted this wider debate in other markets? And do you think that is the right conclusion, the right concern to take away from this? Yeah, so gosh, why did Spain make the headlines? It's a really good question. I think it's a combination of a location and impact. Unlike most blackout events, it was a countrywide event, which lasted for over 12 hours from start to finish and led to a significant amount of disruption. Any event like this just highlights how dependent we are on our electricity networks in our modern world. And it's something that we probably take for granted, particularly in Western Europe. We expect this perfect supply of electricity. It also comes at a time when there's a lot of discussion around the electrification story, so the electrification of industry, of heating, of transport, around the build-out of data centers, which have got really high electricity demands and need this 24-7 power. So we forecast electricity demand in Western Europe to increase by 13% by 2030, relative to 2025, by 55% by 2040. And of that, we're going from around 35% of that demand being met by renewables to over 50% by 2030, nearly 70% by 2040. So that's a really big ask on the electricity grid. And it's the same ask that other grids across the world are having to deal with. So irrespective of the exact cause or causes of the Iberian event, enhancing and investing in our grid systems, modernizing them, adapting them for renewable penetration is really something that we need to be doing more of. We need to be putting the right value on what is really a critical piece of infrastructure. That's a great final word. Thank you both, Kerry and Alexandre, so much for joining us on Energy Evolution. We, the reporters and analysts here at Commodity Insights, will obviously continue to keep tabs on this story. So look out for the latest updates as this evolves. Thank you both. In the reactions to the outage, we've also heard an emphasis on the need to shore up energy security, at the same time as pursuing decarbonization. And to give more details into this perspective, we spoke to Alborik Mongrenier, who serves as the Executive Director of the European Initiative for Energy Security. They are a think tank and a campaign group, which is focused on building support for energy and national security across the European continent. Welcome to Energy Evolution. So you head the European Initiative for Energy Security. And I'm interested to hear how this event, this blackout, fits into the theme of energy security that has, of course, been high on the agenda in Brussels and other EU capitals in recent years. Can we already draw any conclusions from the blackout? And would you say this is a one-off event? Or do you advocate for seeing this as a systemic energy security issue? Well, it's not the first blackout in the recent history of Europe. It won't be the last one either. In Western Europe, we had a big one in 2006, which was caused by some routine work. Before that, there was another one in Italy in 2003, which was caused by a tree. In the US, you certainly remember the 2021 Texas freeze, which was provoked by a very strong winter storm. And more recently, and more consequently in Europe, there's obviously the regular blackouts all over Ukraine, which are caused by Russia's missile and drone attacks on the country's energy infrastructure. The point I'm trying to make here is that these events, they don't happen often, at least in peace times, but it can happen again. And the causes can be many. It can be weather events, human error, technical failure, often all of three combined, or it can even be deliberate attacks, whether physical or cyber on infrastructure, which Europe sees more and more of. That said, it's not a fatality for Europe. What it shows is that we need to be better prepared to deal with the increasing complexity of our energy system. And the best way to avoid and prepare for this is to invest in grids, interconnectors, storage, redundancy, especially with so much renewables in the mix, storage is very important. But we also need to invest in cyber and physical resilience solutions and components that meet certain requirements in terms of security resilience. Do you think policymakers are aware and companies are aware of this investment need that you're calling for? Or do you think there's more to do on that front? I think we already knew that we underinvest in the European grid. So this is in a way a helpful, but dangerous and costly reminder. I think we saw estimates that show that this cost the Iberian economy at least 2 billion euros, maybe somewhere between 2 and 4 billion. So it will force politicians to prioritize the grid in the next few years. And it will also push experts and technicians to find new methods to contain potential disruption. The thing is, energy security today means a lot of different things. It's security of supply, the traditional way that we all know, but it's also material supply chains, whether we're talking about minerals or components. It's also infrastructure security, I've mentioned physical and cyber attacks before. And all of these dimensions offer huge challenges at the moment. And they all get a fair amount of attention in the media. And they get attention from politicians also, whether it's Chinese mineral export restrictions, hybrid attacks by Russian and Chinese ships on undersea infrastructure in the Baltic Sea, the industrialization caused by high energy costs all over the European continent, etc. So the problem is less that the issue is not getting attention, but rather than that there are many different and very serious risks that the private sector and policymakers must prioritize at the moment. And it's been constant crisis mode in the world of European energy since 2022. Another issue is that such events also take place in a context of political polarization and debates on the energy transition. And some pundits are quick to comment and politicize debates and certain energy technologies, which doesn't help with a rational assessment of what happened in this case to the Iberian grid, and why this blackout happened in the first place. Yeah, I mean, you just kind of touched a little bit on what I was going to ask next, which is about this, I guess you could say finger pointing, or at least it's very high profile nature of the discussion around this incident, maybe even getting worse, but through the fact that this is a complex, multifaceted event that takes time to resolve and to explain. But when this kind of thing happens, do you think this is a good reminder? Or do you think it's a maybe also a distraction from other issues that maybe occur under the radar and are less dramatic? And if so, what are those things that you think are maybe being forgotten? Well, as I said before, I think there's plenty of things happening in the world of energy security at the moment, and they're getting a lot of attention. It's multi dimensional. So I don't think it's a distraction. It does point to the fact that we need serious investment in the grid. I think that's probably one of the most important dimensions of energy security for Europe going forward, electricity security, specifically, historically, when we speak about energy security, we used to think about getting enough gas and oil into the system. But as we electrify and digitalize our energy system, which is both our megatrends, and they're important megatrends and essential for Europe's energy security. But that means we need to take into account the risks that we have neglected in the past. And that's why we have underinvested in the European grid, we've invested a lot in renewable production, we've invested a lot in other aspects of energy security over the past decades. But we can't say that we are overprepared on the grid front. Otherwise, a blackout of this scale would not have happened. These investments must happen now, because the cost of inaction is very high, both for the European economy and for collective security. Now, your organization is lobbying for more energy security, obviously, also from a geopolitical perspective. You were started in 2022, amid the war in Ukraine, where energy, of course, has been such a core issue. Aside from the grid investments that you just mentioned, what are some of the other key measures that you're asking for on that front, especially on a policy level? Yeah, good questions. So we're actually on a number of fronts. So grids and energy infrastructure, generally speaking, is one aspect, but we're also very much focused on supply chains, the supply chains that power electrification. So I'm thinking here about critical minerals, but also critical components that go into grids, but also vehicles, well, the transport sector, generally speaking. What we see now is that, okay, Europe is moving towards electrification, which is great, can be great for Europe from an energy security perspective, given that the continent is largely oil and gas poor outside of a couple of countries. But that creates new dependencies, primarily to China, which has a very dominant position in, well, critical mineral value chains, but also batteries, solar, increasingly in wind, which is an area where Europe has historically been stronger. And so these are the dependencies that we need to be aware of. And in order to manage them, we need to be able to protect European industry from Chinese overcapacity in a number of sectors, in the sectors that I mentioned before, and also to support them financially, to make sure that we can develop the supply chains, not just in Europe, which is important, but also that we develop ex-China supply chains with other partner countries. And again, all along the value chain, from critical minerals to batteries and wind turbines. And that's one area where we call the European Union and European leaders more broadly, and countries, including Norway and the UK, to prioritise, to make sure that we're in a position to, to French or the supply chains as much as possible. So we're not as exposed to one single supplier, like we were to, to Russian gas, which was not the single supplier of gas to Europe, but the largest supplier, and is still a very large provider of gas to the European continent. So it's all things that European leaders, the European Commission, national leaders are aware of, but it's very difficult to execute in a context of limited national budgets, higher interest rates, and the era of free money is over, at least for now. And so that means that we need to prioritise strategic materials and technologies that are critical for Europe's energy security and industrial competitiveness. And that's not an easy exercise, but that's something that can and should be done as part of the new European Union budget cycle to invest in the supply chains that will allow us to move towards more energy independence. Great. Thank you so much for joining us on Energy Evolution. Thank you. Pleasure. So that is all for this episode. We hope you enjoyed it. As this story is evolving, we will be keeping a close eye on this event. So please check out our various news pricing and analytics platforms, and also go to our website, www.spglobal.com slash commodity insights. Now, do stay up to date with upcoming episodes and be sure to subscribe to Energy Evolution on Apple, Spotify, or wherever you prefer to listen in. If you have ideas for guests or topics, please email energyevolution at spglobal.com. Now, before we sign off, I want to just give a shout out to our other podcast team members. That includes my co-host, Taylor Kirkendall, Dan Testa, Camilla Nashert, Christopher Coates, and Carolyn Winbrecht. Also, a huge shout out to our agency partner, the 199, and the Commodity Insights digital content team. Thank you for listening. 


---



# The Data Center Frontier Show - Transcripts\n\n**Generated:** 2025-08-06 06:48:15\n**Episodes:** 19\n\nTranscripts organized by date (newest first).\n\n---\n\\n## DAILY UPDATE - 2025-08-09\n==================================================\n\n### Uptime Institutes Jay Dietrich on Why Net Zero Isnt Enough for Sustainable Data Centers\n**Episode ID:** 5171\n**Transcribed:** 2025-08-09 14:57:02.032697\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show Podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier, and today we are here with Jay Dietrich. He is Research Director of Sustainability and Energy with Uptime Institute. Hey, Jay. Welcome into the podcast. Thank you, Matt. It's a pleasure to be here and to spend some time with you today. Yeah. Thrilled to have you here. Now, the Uptime Institute is, of course, famous for its tier rating system in the data center industry. I think most people in the audience are familiar with Uptime, but just in case there's somebody who's new to the industry who might not know about Uptime Institute, do you want to just sort of set the table for our audience with what Uptime's mission is within the data center industry? So Uptime, as you mentioned, has done the tier rating system and also does a lot of M&O and other types of assessment to look at business procedures and operations of data centers to help operators design, build, and operate highly reliable, resilient, and efficient data centers. In addition, we've expanded our portfolio to sustainability assessment to help operators figure out, okay, how do I get my arms around putting a sustainability program in place? And then, so we call that the sustainability gap analysis. And then an assessment and certification of an operation as a sustainable operation focused solely on the operations of the data center facility. Understood. Well, it's, I'm glad you mentioned sustainability because, sort of kidding around, but that's why we're here today, obviously. I wanted to ask you today, you know, based on some webinars and intelligence briefs that Uptime has recently brought forth with your name on them about how to build a sustainability roadmap that also works, obviously, in the context of data center operators. So I was looking at the deck from your recent webinar with Uptime on this subject and, you know, Uptime, we know, is advising data center operators on sustainability strategies. And your contention in recent presentations is that there's a lot of confusion and window dressing was the exact quote in data center sustainability reporting. So my first question for you is, what's the most common example of this kind of confusion surrounding sustainability reporting? And how can operators move from optics on the sustainability side of things to real concrete action? So when we look at the sustainability programs, not only across data centers, but across the whole economy, the programs really break into two pieces. And the first, which we focus on, and which is where I really focus my efforts and analysis is on making the data center itself, the actual operating facility, sustainable. So what you want to look at is, what's the energy consumption of that facility? How efficient is it? Is it maximizing its use of carbon-free energy within the context of the grid in which it's operating? And what is it doing overall to minimize its energy and water use in the execution of the work it delivers? And we'll get into some more discussion of this when we look at the metrics. But so what we're focused on is, what are you doing to either optimize the efficiency of your electricity system, your electrical distribution system, and your cooling system? Or when you build a new facility, what are you doing to maximize the CER, the cooling efficiency ratio, so that I'm maximizing the amount of cooling I deliver for each unit of energy I consume into that system? What am I doing to optimize my IT infrastructure to get the most work per megawatt hour? So that's where we're really focusing on sustainability. What we see happening, and what I call window dressing, which some people would consider harsh, is that what we see is the narrative of a carbon-neutral, water-positive data center. And what that's doing is obscuring the reality of what's really happening at the data center level with offsets, both in terms of RECs, or carbon offsets, or energy attribute credits for the energy consumption, and actual carbon offsets, either from direct air capture or supporting the growth of trees, those kinds of things. But what you're doing is you get into a narrative where, well, we're sustainable, we're net zero. You look at the EU-published results from the first data center survey, which we estimate represents only about 20 to 30 percent of the square meters of data center space in Europe. But 80 percent of those operators report that they're carbon-free, based on a market-based calculation. And so, what's happening in reality is that if you go to Denmark, or Germany, or Spain, they're running at probably about 55 percent renewable energy in their grid mix. So, 55 percent of the total consumed gigawatt hours come from renewables in those countries. But that means there's still 45 percent that has to be managed down against the objectives, the EU objectives, and the global objectives to reduce the carbon intensity energy system. So what we're focusing on is, okay, what's the actual operation doing, and how are we improving the performance of each individual data center building? And that should be the measure of sustainability. Yeah. So, what you're telling me sort of harkens back to one of your slides in this recent Uptime webinar, where you say that a net zero data center is not a sustainable data center, which most people would consider to be a provocative statement. So, can you sort of just reiterate what's missing? You kind of just told us, but from net zero goals and, you know, if a lot is missing, what a truly sustainable facility needs to address? Well, so what you have to do is lift up the hood of the engine, right? So the first piece that we're really pushing on is that to measure a sustainable data center, you've got to look at the work delivered per unit of energy consumed. And you want to be maximizing that value. And the value is maximized by increasing the utilization of the IT infrastructure. And so, the point of a net zero data center is not a sustainable data center is if I can have the best energy, I can be in the Nordics, I can have a cooling system that doesn't use any water, right? All things that depict a highly sustainable data center. But if I'm running my IT infrastructure at 10% utilization, I'm not sustainable. I am wasting a huge piece of my capacity. I am wasting a huge amount of data center space and a huge amount of energy because my work per megawatt hour is extremely low compared to what I could do. So if I take that utilization from 10% to 50%, I can take out two thirds of the equipment in that data center and get the same amount of workload out of it and significantly improve my work per megawatt hour. So that's the first piece. The second piece you want to look at is what's the work delivered per metric ton of CO2 emitted. So again, it's looking at location-based emissions. So if I'm in China and I've got a data center that's running at 10% utilization off of grid power, my work per unit of energy and my work per metric ton of CO2 emissions is really low as compared to a Nordic system. Now within that particular locale, there's going to be a limit to how much I can improve my work per metric tons of CO2, but there are things I can do. And so what we want to look at is what is the operator doing to bring more carbon-free or low-carbon energy onto the mix that crosses their meter. I mean, their effort really  a good effort is where the grid is at 20% or 10% carbon-free and I'm bringing 30% across my meter. I mean, that's  we should give that credit, right? Because to get to a low-carbon data center infrastructure around the globe is going to require incremental improvements in the grid mix in each of the jurisdictions where data centers operate. Got it. Now on the carbon-free energy side, I mean, you've contended that true 100% carbon-free energy or CFE won't happen for more than a decade. So let's unpack that idea. And then also, what's the most realistic, actionable roadmap for operators aiming for 80% or 90% CFE by 2035 or so? So at a high level on the carbon-free energy, there's two problems. The first is that the majority of the capacity going in is intermittent wind and solar. And so, that creates two complications in the grid. The first complication is that for where you have a significant penetration of wind and solar, during periods of high wind output and high solar output, you have significant excessive energy. So your supply can become significantly higher than your demand. So to prevent instability on the grid, keeping supply and demand matched, you've got to find something to do with those extra electrons. You know, put them into storage, curtail them, use them to make hydrogen, you know, generate heat. There's all kinds of ideas that are still really in their infancy in terms of technology demonstration and volume availability of the infrastructure to do those activities. So your first problem is you're ending up doing mostly curtailment right now. And if you look at the data for Texas ERCOT, for California, there's huge  their curtailments are going up significantly each year as more and more capacity goes on, and there's all this excess capacity at high output that can't go anywhere. The flip side is that it's about 10 percent of the year when wind and solar combined can't meet the generation capacity of the  or the supply, the demand of the grid. And so, in those situations, you have to have a reliable, dispatchable power. And so, that's  right now, it's largely natural gas. For new installs, and you can keep coal plants on. There's lots of discussion about that. And you've got nuclear. But again, nuclear, there's a set of infrastructure today that can be utilized. And there's a lot of economic details as to how  I mean, the data center industry has done some good things to write PPAs to buy that power, because the nuclear plants are struggling financially, because they can't compete in the market when there's high output of wind and solar. So, you've got the wind and solar going up, which reduces the prices of  on the spot market, or the grid prices in the market, so nuclear plants can't compete. And so, eventually, you start to see them  they shut them down. So, the data center industry entering into fixed-price contracts for that power are bringing those plants back online. And we've seen that with the Constellation deal for Three Mile Island, the deal in the Susquehanna plant. Those are deals that are helping keep nukes on the system. So  go ahead. Well, no, it's just interesting. You know, you point out this sort of loggerheads dynamic between solar and wind and nuclear, which I'm not sure if I was 100 percent aware of. But I didn't mean to interrupt you. The question that I have coming, though, is about microgrids and your opinion on  you know, if we could have, you know, a sea of microgrids everywhere, would that be enough to, you know, impact this curtailment dynamic? Well, let me just quickly make one other point, and I'll move to that. The other significant point is seasonal differences in output from wind and solar. And my poster child for that, which I use frequently, is Ireland. So, Ireland has an objective to be carbon-free by 2035, and they want to do that largely with wind. When you look at their wind output graph, an ear grid plot has a monthly  a graph of their monthly generation capacity profile for the last three years available on their website. The output of the wind generation capacity in January is two to three times the output in July and August. And that difference between the July  between the winter and summer output is 40 percent of the demand. And so, you've got to fill that hole, right? And today, the only way they can fill that hole is with natural gas generation. They don't have enough other carbon-free capacity to do it. And that's why you're seeing data center operators being asked to make their generation  their standby generators available for a thousand-plus hours a year to support the Irish grid, because they need that capacity. It lets the data center operators make some money off their standby generators. It allows ear grid to avoid making capital investments in a portion of the natural gas capacity they need to cover that 40 percent demand hole they've got in the July-August timeframe. So, within that, then, we get into the discussion, all right, what can we do with microgrids or systems that can allow data center operators to get connected to the grid by essentially bringing their own power, whether that be grid-connected or direct line. And I think you'll see most of it be grid-connected, because there's a lot of economic difficulties potentially with behind the meter. But what that  what you're seeing start to happen is that operators are looking at how to bring a combination of batteries, their standby generators, nuclear  you know, that's out in  the problem is that's 10 years out, really. Again, in terms of volume availability, that's going to take time. And so, but  so, I can make a combination of wind and solar assets that I pull on the grid. Where possible, I locate my data center close to those. There's some developers that are locating data center facilities on large wind farms, for example, and near large  you know, they're co-locating them, essentially, within reasonable geographic distance to eliminate the need for distribution upgrades. And so, if I can combine those different assets together for a reliable power system, then I can, in turn, work with the grid operator to take my data center off the grid when there's demand complications, demand supply instabilities on the grid. So, it  but it requires a lot more collaboration, right? So, now, it's no longer a situation where data center operators pointed  you know, get in the queue and say, make me happy, supply me my power. It's got to be a working relationship between data center operators, developers of generation systems, and utilities and grid authorities that are managing the overall supply and demand on the grid that the data center operator is connected to. Yeah. And, you know, I should just put in here  I mean, it sort of looms over everything, but of course, you know, the great catalyst for all of these discussions, and including the one that we're having now, is the AI boom, right? I mean, you know, since  you know, we've all heard the chapter and verse, you know, since CHAT-GPT, you know, it's  you know, we've all seen the charts, the hockey stick charts of, you know, power demand and effects on the data center industry. So, as one side question before I get to our next interview question, you know, I've heard so many people in the industry say, you know, the last  in the last two years, they've seen more activity and initiatives than they have in the last 20. In your research practice, are you also obviously sort of responding to that, you know, this AI tsunami and everything it's bringing, you know, to the industry also in the area of sustainability concerns? Yep. Right. We're looking at that closely. So, one observation I'd make, because I think it's an important one, is if you look at the projections, the growth of AI, energy use, and data center, we project the growth of AI and data center, and its data center capacity, both for training and inference, to grow over the next five or six years at about the same rate as standard compute. So, this growth rate that we projected, about 650 terawatt hours from 2023 to 2030, half of that is standard compute to support the digital development, right, enterprise growth in Asia Pacific, Africa, Middle East, and then the other half is the AI, both of which 10% is about, I got to get my numbers right here, about a third is going to be training and two thirds is inference. And you've got this inference bubble sitting in the middle that nobody knows what that looks like. So, you've got a situation where there's accelerators for inference applications that operate at about the same power demand as a high-end CPU. We've talked with some operators today that are using standard CPU servers to do inference, and then that goes all the way up to the NVIDIA high-power, high-performance GPUs and inference. But we think that over time, so inference decisions are going to be made on how do I get the most computation for the hardware and the energy that I have to buy to run it. And so, we're going to, I think we're going to see, just like we saw in the 2000, 2007, 2008 time frame when everybody was saying, oh my God, your data centers are going to be taking 20, 25% of, up to 50% of the energy consumption over the next 10 years, and that flattened out. We're going to see that same kind of innovation in the AI space and a movement towards better work per energy systems. So, there's a lot of really interesting discussions from the sustainability standpoint of how do you execute on those IT opportunities? How do we make systems more efficient? And really what the developers, the AI developers need to pay more attention to this work per energy. And I think you have a subset that are, and those are the ones that are going to be most competitive over time because I get more work for the investment I have to make in CapEx and OpEx. Got it. Thanks for connecting those dots for us. Now, you mentioned the IT opportunities. So, one other thing that you talk about is the IT utilization gap, pointing out that IT equipment consumes 80 or 90% of facilities energy, yet utilization often remains low. So, I just can't resist asking you, as long as you're here, why do so few operators set clear utilization objectives and how should they start? So, I'm grateful you teed up my favorite topic. But we've done a survey on how operators are attacking their IT efficiency. And what we're finding is right now about 50% of operators have utilization goals for their IT infrastructure. And those operators, by and large, run their average utilizations between 30% and about 80%, depending on their workload types, their focus, how they run their systems. So, you've got about half of the industry that's really taking this seriously. You've got the other half that honestly is concerned about resilience and reliability. So, the general discussion is that I'm going to drive up my utilizations. I'm going to be more efficient. I'm going to save a few hundred thousand dollars a year. I have one hiccup in my infrastructure. I've lost millions of dollars and I've lost my job. And so, there's a tension there between engineering that efficient IT system, which I believe you can do. You know, it's like any engineering challenge. You have to identify the risks and you've got to put the metrics and the controls in place to manage them effectively. And so, when you look at the benefits you can get from increasing your utilization, another question we ask is the deployment of power management. So, about 45% of the operators have a structured process that assesses their workloads as to whether they can put those on power managed servers or if it has to go on a performance managed server. And again, what you find is those operators will run between 20% and 100% of their workloads on power managed servers. So, it's not perfect for every workload. Can't be applied everywhere, despite what some of the regulators think. But it can be effective for specific workloads and it gets you 10%. So, my view is that from a metric standpoint, a CFO, the first question a CFO should ask when an IT purchase comes in, a hardware upgrade comes in is, what's your consolidation rate when you bring in your new equipment? And if it's not two to one or three to one, get out of my office. Because that, you know, the new systems have two to three X the work per energy capacity on them, particularly when you go two generations of CPUs. Go from like a second generation AMD to a fourth generation AMD. You can double, take your workload two to two and a half times on the new equipment at the same utilization as you had on the old equipment. And in return, get back 30% of your space, 30% of your energy and reduce your capital costs. So, what's not to like, right? Yeah, I mean, it's a lot to think about. You know, I mean, I've heard you speak on this topic before at events and it really seems like a no-brainer. But I guess it's sort of an enigma. I don't think we're going to find the total answer here. But, you know, Jay, to land on the sustainability roadmap topic that we sort of framed this whole interview. And I wanted to ask you about scope three emissions, reckonings. In your presentation, scope three accounting gets called out as sometimes being murky and duplicative. So, I wanted to ask you, you know, what's a better alternative to scope three reporting if there is one? And how should operators engage with suppliers to make real progress in this area? So, scope three is a valid area to focus on. It's a question of where do you place your energy? So, if you look at estimates, we've done some research work on concrete, steel and IT. I'm going to use two quick examples. When you look at IT energy. So, within scope three, there's a category for emissions associated with the operation of products. So, an IT operator or IT manufacturer, some of the manufacturer servers and storage, estimates the amount of emissions that their equipment's going to have in use. So, if I follow a logical emissions accounting through, I generate the electricity. So, there's an emissions factor for that. That's my scope one. The operator, if I'm running a co-location data center, I pass the IT emissions arguably to my IT tenant at scope two. I take those emissions at scope three. So, now I've accounted for them three times. If I'm doing that, if I'm hosting a co-location operator, then they are passing the emissions to their IT customer that's running on their platform, and then the hardware manufacturer calculates the in-use, that's five times counting, and then the component manufacturer calculates the in-use energy, and so now I'm at six times. So, you've got to ask, what's this telling me? Other than I'm spending a lot of time doing calculations that are kind of boxing up the issue. When you look at IT hardware, I'll just stay with that example, there's two things you have to worry about on IT hardware. What's the emissions associated with the energy used to manufacture that piece of equipment? So, that's steel and that's semiconductors. Those are the two big counters. Steel, I know I can only get down to 100% if I'm doing electro steel on full renewables, but right now you're limited on how much you can get emissions reduction. But you can work with your suppliers to focus on low-carbon steel. And on semiconductors, what really matters is  so, semiconductors are roughly 60% of the load on the embedded carbon in an IT product, in a server, storage, or network product. About 60% of that load is the perfluorinated carbon chemicals that are used to clean chambers. So, you've got to look at, is the semiconductor manufacturer using a low GWP chemical and are they destructing it as it goes out? So, it's not being emitted to the environment. And different operators do different things. And then  so, those are the two things I have to worry about. And so, that's what I should be querying my suppliers on. What are you doing to reduce the carbon content of the steel you buy and to buy from semiconductor manufacturers that manage their perfluorinated carbon emissions? That's it. Those are the two things I need to do. And so, if I can focus my energy to say, I want these things out of my suppliers and I want to see what they're doing to get to that, as opposed to doing all this accounting stuff. I can take the money I spend on accounting, which, for example, the EU estimates is like 350 to 500 Euros a year for a large operator. I can put that on IT consolidation projects that bring a direct benefit to my system. So, it's about  you need to work on it. You need to pay attention to it. But you need to  really, our viewpoint is that you need to get the suppliers to put the same diligence into their emissions management program as you're putting into yours. That's how you're going to get real progress. And how's that going? I mean, it is You know, it's difficult. So, if I'm an enterprise operator, I'm one of a thousand customers for a major IT manufacturer. So, first of all, for the IT equipment manufacturer, I don't need a thousand customers asking me what I'm doing. And if I'm a customer, I need a better route than sitting down, you know, spending five hours a year talking to my IT supplier about all the things they're doing and what I want them to do for their sustainability program. You know, we need to find a relationship there that  and by the way, most of the equipment manufacturers are working on that, right? So, there's already programs going on. They've got programs. You just need to make sure, are they making progress? And is there somebody that's got a better option that I should use? Well, Jay, I mean, you know, this is such a complex topic, and we really need somebody like you from the Uptime Institute here to unpack it with these, you know, real hard core, real world sustainability considerations and impacts for the data center industries. So, I want to thank you for joining us here today for this talk. I mean, there's  you know, I know we could go on for another half hour here with questions, and we should, you know, on another episode. But I wanted to give you the chance before we close here to, you know, let our audience know anything that Uptime Institute has, you know, that you're working on or that might be upcoming this year that we can keep our eyes peeled for, because I know that, you know, Uptime is just doing a tremendous amount of work in all these kinds of areas for the data center industries. So, you know, what do you have in the hopper that we should be aware of? Well, I guess the one issue I'd bring to attention that I'm finding particularly interesting is the process of developing a data center label in the EU. And that's married to or is in conjunction with some label developments in other jurisdictions as well. Malaysia is building one, and there's one or two other locations. And that is going to  that has a huge amount of risk for data center operators in terms of what's being asked and how do operators consider that information to be confidential and how those labels are going to be used and published. So it's an area that's slowly emerging in terms of operators are going to need to begin to make real operating data publicly available. And there is a lot of angst and consternation within the industry about what that's going to look like. And so, it's a really interesting area to work on and talk about, and we're doing some more analysis of that and making suggestions. We've been active in providing comments to the EU on some of the limitations to what they're doing, some areas where they could be, in our opinion, could leverage the opportunity to make real progress while not overexposing some of the confidential information in the industry. Got it. Well, thanks again, Jay. We'll leave it there for now, but it's always great to catch up with the Uptime Institute. And I really want to thank you for a really informative and enlightening conversation today. I really got a lot out of it. Thank you. I appreciate the opportunity to do this podcast with you. And it's really an interesting space, and I'm having a lot of fun working in it. And how. Thanks again. And to all of you out there, we'll see you next time on the Data Center Frontier Show podcast. Take care.
\n\n--------------------------------------------------------------------------------\n\nn\n## 2025-07-29\n==================================================\n\n### LiquidStack CEO Joe Capes on GigaModular, Direct-to-Chip Cooling, and AIs Thermal Future\n**Episode ID:** 5081\n**Transcribed:** 2025-08-04 23:53:26.507873\n\n**TRANSCRIPT:**\nHello and welcome to another episode of the Data Center Frontier Show. I'm Matt Vincent, Editor-in-Chief at Data Center Frontier. And on today's episode, we're joined by Joe Capes, the CEO of LiquidStack, one of the most prominent players driving innovation and liquid cooling for AI infrastructures. So welcome in, Joe. Thrilled to have you here. Thanks, Matt. Good to be with you. Appreciate it. Yeah, so let's start with LiquidStack's major announcements at Data Cloud and Cons. I know there was a few, not least from the Meta headset your company so graciously sent me to check out, which I really appreciated. But so for those who didn't catch the headlines, though, can you walk us through what the biggest takeaways were for LiquidStack at the datacloud.cons event? Absolutely. So Matt, at the event, we announced a new platform for direct-to-chip liquid cooling called GigaModular. And GigaModular is a scalable two to 10 megawatt single-phase direct-to-chip coolant distribution unit or CDU. We're really excited about the platform announcement because as we looked at the market and really examined the three and five-year roadmaps from the various semiconductor companies, chip TDPs are continuing to increase, rack densities now are being published to meet 600 kilowatts in total power density, and we know going higher. And as a result, the capacity for heat rejection associated with liquid cooling loops is also increasing. We also know that this is a very fluid market, no pun intended. And as a result, operators and owners of data centers are having to make some difficult decisions on where to deploy capital. And we feel like having a modular, scalable CDU platform gives those operators an opportunity to have greater flexibility in their liquid cooling deployments, but also to embrace a pay-as-you-grow or pay-as-you-go strategy depending upon the AI workloads that are being installed on day one or at a later date as the facility expands. Yeah. So can you kind of place the GigaModular offering in the context of all the other systems that LiquidStack has for liquid cooling in the data center and for AI, and also wondering what the feedback was from the announcement in cons? Sure. So LiquidStack is a full-service liquid cooling provider. We manufacture systems for two-phase liquid immersion cooling, single-phase liquid immersion cooling, and also the infrastructure that supports direct-to-chip, single-phase direct-to-chip deployments that are most characteristic with the current scale-up of AI, particularly the NVIDIA announcements that we've all been reading over the last year. The feedback that we got from the announcement was nothing short of incredible, and I think that a lot of interest was garnered from the hyperscale community, as well as some of the larger operators that we have engaged with globally. And I think there was a really warm reception around the flexibility of the solution, but also the scale of the solution, and I can assure you that we had no foresight into Jensen Huang talking about GigaFactories when we trademarked the name of the platform, but we were, I suppose, lucky enough to find alignment with some of those announcements. Yeah, that's a nice harmonization there. So, you know, when it comes down to, you know, placing these systems in tailored for AI intensive workloads, can you talk a little bit about the specific design innovations that are at hand, that are required to meet, you know, the thermal and density demands? You started off talking about the TDP, but, you know, just wondered about the, you know, the design angles, you know, as we're looking and discussing CDUs and cold plates and all the, you know, types of technologies you're working with there at LiquidStack. Sure, yeah, I mean, so Matt, we're finding a lot of variation in the design and deployment of single phase direct ship liquid cooling, and specifically seeing CDUs located either in the row within the white space, so essentially integrated as part of the pod or the rack infrastructure. And I mentioned that because it's very important to have CDU designs that are compact in footprint and in volume, because the white space is precious, and it's some of the most expensive real estate on the planet. So you have that aspect, but you also are seeing CDUs being deployed along the perimeter of the white space, traditionally, where you might see air handling units or cracks or craws. And then I think that as we look at more of the new builds, new designs, we're seeing CDUs being deployed in mechanical corridor in an adjacency next to the white space. And I think that's ultimately going to become the preferred option for this infrastructure, because it's not attractive to have trades in your white space, especially if security is concerned. You know, we know that a lot of operators don't like technicians walking around with laptops and Ethernet cables. So, you know, having the infrastructure placed and deployed in a purpose build or design mechanical corridor seems to be the ultimate end game. Yeah, yeah, I mean, we've been saying, it seems like for the, you know, at least the last two years, we've been saying it's the year of liquid cooling, but maybe this year is the year of the CDU, you know, within that cooling paradigm, as you're discussing. And I know that LiquidStack was kind of out front with your one megawatt CDU design, if I'm not mistaken. But before we're done here, I wanted, I did want to ask you about, you know, your company is kind of known for the industrialization and modern modularization, I should say, of two-phase and immersion cooling. So I kind of, maybe I'll just ask you now what the outlook there is for you in terms of those, you know, your edge platforms. I think it's the micro modular and the mega modular, is that? Yeah, well, we also manufacture modular data centers that incorporate our liquid cooling offers. And we shipped a micro modular unit out in March, first deployment of that particular form factor, super excited about that. As it relates to the pioneering work and R&D that we've done around immersion cooling, I think one of the really cool things about LiquidStack is that as a full service liquid cooling provider, you know, we offer all the modalities of liquid cooling. So when we meet with a client, you know, we can really truly listen to their challenges, what they're trying to achieve as a desired outcome and ultimately help and hopefully guide them towards a liquid cooling solution that's going to work well for them. And, you know, for example, there was a project that we're working on right now in North Texas with a large operator, you know, who's going through that very process of really determining whether immersion cooling direct to chip liquid cooling or both is the desired outcome. And I think also, as we look at roadmaps and sort of where the industry is heading, we're very cognizant that chip total dissipated power is continuing to increase, as I mentioned. And we are foreseeing, you know, a practical limitation around what single phase heat rejection can do. So that brings into play, you know, other modalities of two phase heat rejection that can leverage latent heat removal for, you know, in a multiple of heat rejection capacity and efficiency, depending upon the application. So net net is, you know, we think that we think direct to chip liquid cooling and single phase approaches have a bright current horizon. And we also foresee that two phase technologies also have a good future ahead. Yeah, for sure. You know, with the two phase and especially the liquid, I mean, the immersion cooling, you know, it's I think it's the ESG and the sustainability angles that are the most exciting just for how much how much they can do in in that way. When it comes down to attending an event like Data Cloud, you know, were they asking you a lot about the sustainability upsides compared to, you know, obviously traditional air or just, you know, single phase cooling systems, you know, the question kind of comes from, you know, liquid stack seems to be all over all forms of liquid cooling. So that's where that sustainability question comes from. I think, Matt, that sustainability in the immediate foreground is taking somewhat of a backseat to the frenetic scale up of AI infrastructure, where, you know, we're talking about, you know, new technology, new infrastructure, the likes of which the data center industry has never seen before. So this is a historic moment in the industry. And there's a lot of blocking and tackling on the operation side, just to make sure that, you know, the demand and supply imbalance that we're seeing right now for, you know, power and cooling infrastructure, you know, achieves some level of equilibrium in the future. And I'm sure it will within a couple of years. But in the meantime, it's it's really down to, you know, time to power, power availability and capacity grid interconnects, and then the decisions that have to be made on on, you know, the infrastructure purchases to support these power and cooling upgrades. I think that another big factor is, is as as some of the newer builds come online, and we see outside heat rejection plants that are purposely designed and built for AI workloads, you will see higher leaving water temperatures off the chiller plant or even the use of dry coolers. And in that case, as as these leaving water temperatures increase, the ability to free cool or or to mitigate the amount of mechanical cooling that's that's needed for outside heat rejection will become less and inherently we'll see PUEs heading back down. And also WUE going in the right direction. This is going to take time though, Matt, it's going to be, you know, I think about two years before we see some equilibrium between demand and supply. And in the meantime, some of the newer purpose built design data center designs being constructed and actually deployed. Understood. Thanks for that. You know, to your point, though, about it, being a historic moment in the industry, I just wanted to shift gears for a minute and ask if you could talk about how LiquidStack's partnership ecosystem has evolved in the way of, you know, particularly with OEMs, facility designers, integrators, and how critical are you finding those relationships to broader adoption of your technologies? Yeah, that's a great question. I think that when we pioneered immersion cooling, most of the R&D and investment in proof of concepts and field trials was being driven by the hyperscalers and their ecosystem. And as a result, we were able to form incredible relationships with all of the world's largest semiconductor companies, most of the world's largest hardware manufacturers and integrators. And, you know, even companies doing storage and networking communication systems. I think that with the scale up of AI, we're really deepening our relationships with the CSP cloud service provider and co-location community. And as a result, also working much more closely with some of the primary or predominant consulting engineering companies or firms and also large general contractors, as well as smaller GCs. And we're also seeing kind of a new segment around adaptive reuse, right? So facilities that are power dense and then being converted into white space to host AI workloads. And some of those players are much smaller companies, but some of which, you know, we've never, you know, as a data center community probably even ever heard of, but are coming, you know, online very, very quickly and have, you know, immediate needs. So it's a, I think to get there, we all have to work together. And, you know, sometimes I think when it, when it comes to commercial relationships, the term, you know, partnership can become cliche, but I think in this particular environment, partnerships are more important than ever. For sure. Thanks for that. You know, maybe in the context of, of that point on adaptive reuse type deployments, can you talk a little bit about anything you might be seeing in terms of global adoption trends? Like, you know, are you seeing certain regions or customer profiles moving faster towards one type of liquid cooling than others, or, you know, if that question makes sense? It does. I think we're seeing, I think we're seeing, you know, two trends, right? One, which is, I think fairly, fairly well-known, historically well-known, which is a majority of new data center designs and the ecosystem around new build is, is driven by the hyperscale community. And a lot of those initial decisions are made in the USA and those technology decisions are then, you know, rolled out globally with, with some measure of localization or regionalization. And that's kind of the way that things have gone in the data center industry for the last 10 or 15 years. The one thing that I think has changed is, is the fact that, you know, we're seeing markets globally that are emerging much more quickly than anyone could have anticipated. And a lot of that has to do with the availability of power both the ability to, to find those grid interconnects, but also to to find, you know, power capacity that's also cost competitive. So, you know, definitely, definitely seeing a lot of growth in like Southeast Asia, the Middle East, India and, and markets that, again, would, would be traditionally called emerging markets, but are now taking more of a leadership position as it relates to AI. Thank you. Again, sort of in that context, you know, one trend that we've been following or covering is, you know, the people who say that supply chain and logistics is really kind of a dark horse, you know, to watch for in this AI build out, you know, do you feel like there's any special points to make there in terms of supply chain or your, you know, your manufacturing and distribution response to issues like that? Absolutely. So when, when LiquidStack set up our global headquarters, we landed in the Dallas-Fort Worth Metroplex in the US. Yeah. And we, we've had one factory there operational since the beginning of 2024. And then subsequently did a ribbon cutting on a second factory in the same locality just last week. So the takeaway there is that we, we invested in the supply chain in the United States and it doubled down on that capacity. But I think that the, the recent turmoil around global tariffs definitely heightened our awareness of supply chain risks and in, in the effort to, to de-risk and mitigate those concerns has, has accelerated our global expansion. So, you know, at the moment we're looking at, at establishing manufacturing overseas to supplement our US manufacturing. And I think that will also avail some unique opportunities for, for cost reduction and price reduction that we can pass along to our customers. Understood. So, you know, not to try to pin you down on any kind of exact forecast, but I mean, it's, you know, it's kind of a, you know, quite a historic period, as you mentioned for, for the data center industry, you know, people saying that they've seen more, you know, happening over the space of two years than they, they have over the last two decades for people who've been in the industry that long. So, so where do you, where do you see the road map going for innovation and growth in liquid cooling and AI infrastructure build-out over the next year or so? You know, I mean, it's, it seems to me like it's, it seems to me like it's, you know, it's fairly locked in for the next couple of years. This is what's going to be happening. You know, we covered some news stories earlier this year about, you know, some of the big cloud giants pulling back on, you know, leasing for, for AI. Well, you know, that may be true, but I mean, it wasn't, you know, anything, you know, it seemed more like blips on the radar, but you know, do you have the optimism for, for the technologies involved with AI that, you know, this, this boom is going to continue for a while? Yeah. I mean, I've, I've been in the industry since 1992, so I've seen a lot happen, you know, have, have been through booms and busts. And, you know, I think one thing that's unique about the scale up of artificial intelligence is kind of very similar to mobile, mobile technology, you know, several years ago at the advent of like 4G, 5G, 6G, but also, you know, the associated services that come along with mobile technology, you know, human beings are not, they're not asking for fewer services, right? We're relying on more and more services and apps to help support our daily lives. AI is becoming part of our social mesh and fabric, and it's becoming embedded in almost every tool that we use in the workplace as well as at home. And so, you know, I think it's, you know, if you look at even the educational process, I think the first year that we started to see some impact from AI, there was a reluctance and a resistance from the, from the educational community to, you know, allow learners and students to use and leverage AI. But now, I think most educators are seeing that it's becoming an essential and important, you know, part of the fabric for how human beings will learn and educate themselves. And it's also bringing tremendous efficiency, I think, into the workplace where it's being, you know, used properly. It's still early days, but in line with that, you know, we're extremely bullish on the market, looking to continue investing in R&D and particularly to address, you know, the higher TDP chips and very ultra-high density racks that we're starting to see come into the data center market. Yeah, you know, one sort of very specific question is, because obviously, as you mentioned, a lot of it is these large-scale deployments associated with hyperscalers, but you know, how about the retrofit play for, you know, co-location and smaller data centers who might be getting, just getting into AI compute? I mean, do you see that as a significantly growing segment of your customer base? Are you hearing from those people who are just getting started? Well, I think there is, it's almost split down the middle, Matt, and another great question, by the way, because I think that what we're seeing is decision points having to be made between homogeneous and heterogeneous workload designs for the data center. And there are certainly some retrofit applications where operators are trying to make the best use of their existing air-cooled infrastructure to support AI workloads, albeit at, you know, lower densities. But, you know, it becomes difficult to do so when you have specific limitations around your power stream and your ability to support liquid cooling in the white space. You know, we're seeing some kind of crazy stuff like, you know, heat from a liquid loop just being dumped out into the white space ambient so that the existing air handling units can kind of deal with that additional heat. But, you know, that's causing PUEs to go through the roof and increasing water use. So it's kind of a Band-Aid, if you will. And, you know, frankly, I think the industry needs to rip the Band-Aid off. I think, you know, unfortunately, a lot of the infrastructure that's out there right now is ill-equipped to support AI or higher density heat loads. And I think the faster that some of these operators can move to liquid cooling, ultimately, the better their business proposition and even their profitability will be. For sure. You know, and I can't resist asking this next question because, you know, in the data center industry, you know, for a long time, I don't think right now, but, you know, there's been this question of, you know, what is the edge? What are we doing at the edge? It seems to me that with, you know, with what's happening in this AI moment that, you know, edge deployments are getting very defined in my mind as to what they're going to be for in the context of, you know, inference AI as opposed to LLM stuff. So I just didn't know, since so much of your product mix seems to be oriented towards the edge, if you had some thoughts about, you know, the definition of the edge, you know, the utility of what's going on at the edge and what's to come for the edge. Yeah, it's a deep question. And I think as I was listening to you, I was thinking about my first foray into driving an EV last week and some of the, you know, the great aspects of that ride, but also some of the challenges that I had, you know, with the range of the vehicle and finding charging stations and interact. I can tell you there's a really good opportunity for somebody out there to design an app that accumulates all of the different apps for charging. But, you know, as I think if you think about mobility, it's the essence of the human race, right. You know, movement, whether for business or pleasure, transportation is critical. And I think that as we start to see, you know, a greater proliferation of electric vehicles, but eventually autonomous vehicles, the amount of compute that's required at the edge will amplify drastically. And it's taking a backseat right now to what I think is a greater focus on, you know, centralized deployments to support, to support, you know, NVIDIA, AMD and other companies that are rolling out, you know, AI platforms and chipsets. But, you know, I think as we start to see more and more technology deployed at the edge, in turn, you'll see a more focus put back on edge deployment and how liquid cooling plays a part in those deployments. The edge tends to be typically a pretty harsh place to operate. It's usually, you know, really cold or really hot, really wet or really dry, really, you know, you have a lot of issues with particulates and contaminants in the air. So I think there's a role that liquid cooling can certainly play there. But I think for the next probably two to three years, most of the focus is going to be on the centralization of AI deployments and then see that proliferation out to the edge. Got it. Thank you for that. Well, Joe, it's been a real pleasure having you here. Great interview. Any closing words for our audience from LiquidStack as we close things out here? Well, I mean, I think that, like I said, having been in the industry for a long time, I've seen technologies, new technologies come to the market. We live in a fairly risk-averse industry. Change is not often embraced readily. And I think we're in a moment where we have this perfect storm where, you know, sustainability, technology and services are all sort of combining and intersecting together to drive the advancement of new infrastructure and new investment in R&D that we haven't really seen for a long time. And it's exciting. It's a really momentous time to be where we are. And LiquidStack feels really good about where we've been able to position the company. OK, very well put. Thanks again, Joe. And thanks again to LiquidStack for joining us here on the podcast. Thanks a lot, Matt. I appreciate it. We'll see you next time on the Data Center Frontier Show.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-07-24\n==================================================\n\n### Leveraging Heat as an Asset in Data Center Operations\n**Episode ID:** 5082\n**Transcribed:** 2025-08-04 23:53:26.508237\n\n**TRANSCRIPT:**\nHello and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier. And today we are here with Esty Tierney. She is Product Manager for Centrifugal Chillers at Trane. Esty, welcome into the podcast. Thank you, Matt. This is my first podcast experience being on a podcast, not necessarily listening to one. Well, we are happy to, you know, welcome you here in your debut. So let's dive right into the questions. Trane, of course, is a well-known leader in HVAC. What role is Trane playing in thermal management for data centers in terms of big picture? Well, you know, that's part why I'm here today, right, is I'd like to make that picture even bigger when we start talking about what the future means. You know, high level, of course, Trane has been involved in many ways over, you know, over really the last century when it comes to products and implementation of both cooling and heating systems for HVAC. So that's nothing new. And, you know, my role directly today is focused around water-cooled centrifugal chillers, as you said. But when we start talking about this wider scope and this application of data centers, which is really changing what has been happening in HVAC, it's really important that we are not just looking at the equipment level, but looking at the system and the application. And that's something that Trane has always done, right? We've always assisted and put out a lot of material and guidance on how to design cooling and heating systems. But it's really taken it one step further for data centers. We are moving so quickly here in the United States specifically, but certainly across the world. The development and the rate at which data centers are being built and the future growth projections make the focus on systems even more important. And so we'd like to, you know, make sure that we're having these conversations and we're talking about not just what makes sense for a data center HVAC solution today, but setting all of ourselves up for success 20 and 30 years down the road. So let me just ask you point blank, what's driving the increased heat waste in data centers? Well, it's the increased need for cooling, number one. You know, high level, we have a lot of energy waste currently. And that's not just data centers, that's across the board. When we talk about the large amounts of cooling that are needed for data centers, and I say the term data center, but that means a lot of different things, right? We've got COLOs, we've got a lot of changing industry with the development and increase in AI and artificial intelligence. So we're talking about there's a lot of change in scope, size of data centers, purpose, demand. But overall, they all need cooling and they need an increased amount of cooling. And with any air conditioning system, it's just a management of heat. You're typically taking heat from one area and putting it somewhere else. That's not new. The question is, why are we always throwing away that heat? Heat is energy, and we have the opportunity to recover huge amounts of energy with data centers today. Yeah, can you talk about ways that heat can actually be considered a strategic asset for data center operators and owners? Absolutely. So overall, heat is energy, and we need that energy in other areas, not necessarily at the data center. And that's been one of the main challenges, right, is that data centers have a lot of need for cooling, but not a lot of need for that waste heat. So the question moving forward is, how can we partner with other districts, with other companies, with other processes that do need and utilize that heat? In a very basic level, you look at something at your home and you're saying, you know, even when it's 90 degrees outside, I'm still taking a hot shower. Even when it's negative 10 degrees here in Wisconsin, I'm still utilizing my fridge. You have a certain level of simultaneous heating and cooling energy needs throughout the year. If you were to look at that on a wide scale level, you know, not just at your household, but on the level of a district cooling, district heating, or you start to include industrial processes, municipal needs, you have a huge heating and cooling load opportunity where you can strategize how you're utilizing that heat. So you bring in a data center that has a lot of cooling needs, but isn't using any of that heat or any of that waste energy. And you start thinking, how can we be a little smarter about how we utilize this moving forward? And so a lot of that would involve being integrated with the municipality in order to combine the heat users with, you know, the people who are not using that heat currently, the data center. And that's where the real opportunity lies. Yeah, you mentioned, you know, municipalities. Can you talk about ways that data centers can give back, so to speak, through thermal recovery and redistribution of thermal waste? So Europe is a great example of this. They're doing a really good job, not historically just with data centers, but in general of heat reuse. And there's different factors that play into that. Climate is one of them. We don't certainly have the same climate across the United States that Europe does or other areas of the world. So you have to think a little bit specific to where the installation is. But overall, if you were to look, you know, there was a huge opportunity in Sweden where they were taking all of the waste heat from a data center and utilizing it to heat, I think it was something like 120,000 homes. That's huge. It's a huge improvement in energy efficiency, not just for the data center, but when you look across, you know, that that installation, that country and the need for power, truly. Now, you could have been heating with gas. In that case, you're really talking about decarbonization. So there's there's multiple benefits there. Now, you take that same example and you bring it to the United States and it's maybe not as easy to implement. We don't have as many areas that are set up for district heating. So when I give the example Sweden's heating 120,000 homes, we look at our area. Very likely, you know, I can tell you for for my house, we have our own heating system. Right. We have our own our own furnace. We are not set up on a district heating system throughout our city. So you have to say, first of all, what could you do in this location? It's if you were to design a city from scratch today, it would be easy to look at it and say, you know, maybe we could strategically implement and design and build a data center near where the city is located. And that's one of the trends, too, in certain areas, these edge data centers, which tend to be smaller but provide a localized source. If this is built in conjunction with a city, you can bring all needs of heating and cooling together if you were to design it today and be in that city planning scenario. So part of it involves, you know, what is the current existing infrastructure? That's very, very important to say. What could we do? But just because the infrastructure doesn't exist today doesn't mean that isn't something that we should be looking toward the future. I think that's really important is to say not just, you know, what have we built, but how could we do it better and what do we absolutely need to do better as we look to the future for decarbonization? Sure. For energy usage overall to be smarter and more efficient. And also when we start to talk about power considerations and where that power is going to be used and setting everyone up for success in the future. Thanks for that answer. So you mentioned the infrastructure. So I wanted to ask you what's involved technically in recovering waste heat and how can train help? So in any data center, you do have some sort of cooling mechanism. That's typically air cooled chillers or water cooled chillers. Either one can be a good solution. Either one can be utilized. It kind of depends on, you know, your different application. Where are you being located? What are your water power, you know, climate considerations? It's easiest to reuse heat in water, which would mean you're utilizing water cooled chillers. So in your water cooled chiller application, you always have an evaporator and a condenser side, two different water loops. The evaporator is typically your load. So that's going to be providing the cooling to the data center and the chips as needed. And then the condenser heat in captured in that water is typically thrown away. So you're typically doing a cooling tower or a dry cooler, perhaps rejecting all of that heat and that wasted energy essentially to the atmosphere. You don't have to get rid of all that heat. So if you have already have all of that heat captured in your condenser water loop, the question is, what do you do with it? If you can send that water to an area that needs that heat, then you could immediately use it. Now, I say immediately there's a couple of factors that are holding us back here. Number one, if you have a really large data center, it's typically not built in town. These are not installations that people want in their neighborhood currently. And you're not going to run miles and miles of pipe to run some low temperature heat. And that's the second challenge is a lot of it is what I would consider low temperature heat. And, you know, there's certainly a wide variation as we move between cooling towers and dry coolers. But let's say it's somewhere between, you know, 95 and 130 degrees Fahrenheit. I'll consider that pretty low temperature. A lot of existing systems today are not set up to utilize low temperature water as their direct heating source. What do we do with that? Well, number one, new systems that are built can and should be built to utilize low temperature heat. And then that heat could be directly utilized right out of out of a wasted heat loop. The second part of that is that you can always boost up that temperature utilizing, you know, another water source heating application, you know, a water a water cooled heat pump itself to take a temperature of water and boost it even higher to the temperatures that are needed for the application. You know, other other ways of doing that as well. And then, of course, you start getting into other things which involve industrial process that might actually need high temperature heat for a very specific process, not just general heating needs. So, you know, there's a lot of different ways to think about it. We can also be creative. Are you talking about carbon capture, which could utilize, you know, low temperature water, low temperature heat? There's a lot of ways to think about it, and that's what I want to emphasize is we're not talking about one solution. There isn't one right way to do it. I just think it's really important we are going into it with our eyes open. And, you know, there's a lot of reasons why we aren't doing it across the board right now here in the United States. Number one, we're moving really quickly. Data centers are being built very fast and a lot of plans. I think it was something, you know, in the first six months of 2024, there was something like 78 data centers projects that were being kicked off. That's huge. That's huge. When you start to think about it, you look at the maps of where these data centers are being built. It used to be certain localized areas, you know, a decade ago, and now it's all over. There's plans for all over. So as we start to think about where we're implementing these data centers, you know, another challenge is, are we connecting the right parties with what really have to be the city planning officials? And that involves a lot of coordination, right? Probably anything involving coordination with a city or with a local government is probably challenging. So that doesn't lend the data center to probably reach out and try to coordinate that. So, you know, really, we're moving so quickly and it has been harder to implement because of existing systems and the low temperature water that doesn't take away from the future potential of utilizing this energy. So to these points that you're making, a couple of questions. First of all, you know, we hear that optimized systems in the data center reduce energy usage and make a data center more sustainable. So can you speak to Trane's lens on system optimization and why it matters? So system optimization is very important. And, you know, when you're talking about that for a data center, you're often talking about your PUE, your power utilization effectiveness, which basically is kind of saying how effectively are you utilizing the power in your data center and applying it to the actual IT needs and utilization? So that's important. And, you know, I think there are, you know, data centers throughout the United States are working to lower their PUE in general. I don't think that we are quite as aggressive as other areas of the world, most notably Europe in that area. But it is something that we're slightly improving on. Now, across that, you know, when we start talking about Trane and how we can help overall utilize that, it is very specific to the data center. And I say that because when you look at a water-cooled chiller and the energy that it uses in general, when you have higher lift, so basically a greater temperature differential between your evaporator and your condenser, the chiller itself utilizes more power. It's more work the compressor is doing. It's higher lift. Now, that might be true on the chiller level, but one of the benefits to operating at that higher lift could be, number one, that you're utilizing a fluid cooler application. And that means that you are saving water usage. You're not going to have those same rates of water evaporation. It also can be still quite a bit more efficient than an air-cooled chiller application, even though you're operating a water-cooled chiller at higher temperatures and higher power draws. And then lastly is that factor of heat recovery. If you are operating a water-cooled chiller, typically at a higher lift in order to reuse some of that heat and water-cooled chillers do have the option of, let's say, multiple paths of heat rejection. You know, we do have condenser options that essentially give you two ways to reject heat where you could utilize partial a dry cooler solution and partial heat recovery usage. You know, that's where overall your system efficiency improves. But it may not look that way if you're only utilizing chiller efficiency numbers. And that's why it's really, really important to look at it at a holistic view overall. Right. It's not just about your chiller KW and power draw. What are you using for fan power on your cooling towers or dry coolers and your pump energy and across that system? So when we start talking about optimized system design, you can't look at one piece of equipment. It really is how you intend to utilize those pieces of equipment. And that's really where train and our systems knowledge comes into play. Thanks. Now, the next question on the stakeholder side is what could or should decision makers in terms of city power data center owner operators do differently in terms of being more strategic in constructing cities or communities when it comes to these topics that we're discussing? Wow, isn't that the question of the day? I guess my first piece of advice would simply be everyone needs to come to the table. Everyone needs to come to the table and be on the same page that this is something that they want to go after together. And I think that if we have additional regulatory pressure over time, this is going to be even required. I don't think it really is today. We see more of that in Europe. But when people are forced to come together and really talk about what would work for them, and like I said, it's going to it's going to differ on location part. You know, is it what type of city are we talking about? What type of climate does that city have? Do they have access to water? What are their power constraints and what are their opportunities? Are they building a small edge data center? Are they you know, is this a big colo? There are so many different ways to talk about it. But if people come to the table and say, you know, here's what I need heat for. Here are the times of year that I need it. Here are the actual capacities. And you have the data center representatives coming and saying, here's how we could help. We could talk more strategically about where we build these data centers, what sizes they are, how we cool them, what type of equipment is being used, and then figure out ways to utilize that rejected heat energy. It really is that simple. And every little bit that we do, I think, helps. And that's where we really should set the infrastructure up for the future. It doesn't mean that we're going to recover 100 percent of heat today. That's probably not realistic in the majority of installations or the majority of locales. But if you start those conversations today, maybe we do have some opportunity to continue to improve the energy usage as time goes forward. Thanks for that really valuable points there. So to sort of wrap up with the surge of AI and data center growth projects over the next few years, we know that heat will continue to be a challenge. So, you know, what are some ways that owner operators can begin to shift their thinking about it strategically as more of an asset? I think we need to recognize how quickly the power demand is increasing. I think data centers accounted for something, you know, a little over 4 percent of power usage in 2023. And by 2028, it's expected to be double digit as high as, you know, 12 percent. That's incredibly high. So we definitely need to focus on solutions now in order to get there. So with the surge of AI and the data center growth projects over the next few years, heat will continue to be a challenge. So how can data center owner operators begin to shift their thinking about it in a fundamental and strategic way as more of an asset? I think the data centers need to recognize how badly cities are going to need that energy in the future. And so while right now it seems like the fastest path and probably the initial low cost path to building a data center in its traditional means, that looking toward the future, it will both be an advantageous efficiency play, power play and money play to strategize with cities, with heat reutilization, with energy reutilization. And that means that we don't just leap toward the easiest solution now. There's been a lot of headlines that we are even moving toward more gas powered data centers. That's the exact opposite of some of the decarbonization movements that have been happening. I'm not saying there aren't areas where it makes sense. You know, high level here in Wisconsin, we get very cold. Completely eliminating gas probably doesn't make sense long term, not with the type of heat that we need in low temperatures. Great. Let's utilize gas when and where it makes the most sense. And that's why when we start to talk about heat recovery and the heat and energy that's not being utilized by data centers, it makes complete logical sense to partner with anyone who utilizes that heat. And it is a little bit, I think, more work. Maybe it's not even a little bit. Maybe it's a lot of more work up front for the data centers to design differently, to think about size and application differently and to connect with whether it's an industrial process or carbon capture or a city about needs. But it's more strategic in the long run because we're already seeing that in Europe where the more efficient data centers are actually not only being recognized, but are getting that payback in money. And really, there's opportunity there. And maybe we're not quite there yet where data centers are going to see the return on their investment for partnering in order to reutilize that heat. But I think it's coming. It's around the corner as soon as we start to run into more of these power constraints. Well, I will leave it there for today, but I want to thank you, Esty, for a really insightful and timely discussion today. It's always great to catch up with Trane. Well, thanks so much for having me here, Matt. I look forward to working with you again and continuing to listen and read about the data center developments. Absolutely. It's really been a pleasure. We'll see you next time on the Data Center Frontier Show.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-07-22\n==================================================\n\n### Powering AI Data Centers: Eaton on Infrastructure, Cooling, and Whats Next\n**Episode ID:** 5083\n**Transcribed:** 2025-08-04 23:53:26.508260\n\n**TRANSCRIPT:**\nCool. Hello, and welcome to another episode of the Datacenter Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Datacenter Frontier. And today, we gather to talk a bit of shop with two pros from Eaton. We've got here Doug Kilgareff, who handles strategic accounts in the company's Datacenter segment. And JP Bezell, he's Eaton's VP and Chief Datacenter Architect. So welcome in, Doug and JP. Hey, Matt. How are you? Hey, Matt. Great to be here. Yeah, it's great to have you both here, and I'm doing very well. You know, sort of the genesis for this podcast is, of course, JP, you and I sat down and met at Datacenter World and talked a bit about, you know, this incredible moment the Datacenter industry is in right now on the heels of AI and, you know, power realignments and everything else that the industry has been undergoing. And then, Doug, you and I just met and had a good stand-up talk at 7x24 Spring in Orlando just the other week, so it seemed like a good time to do a podcast and get us all together here and to talk all at once. And I just really hoped that I'd be able to ask you guys about, you know, what you're seeing on the ground in terms of real-world stuff and issues you're facing coming out of the data hall, how AI and cloud are changing power and design, and all kinds of stuff like that. So I've got a bunch of questions here for you. I guess the first one I'll ask is about AI. You know, in the stuff that you're working on, how does it come up when, you know, AI is being introduced into a build? Is it usually at the outset and in the design, or is it, you know, a retrofit consideration that comes, you know, midstream? And just sort of asking about what the AI playbook is for you guys as you go out for Eaton. So I'll go first, JP. We've kind of seen it both ways, Matt. We do a lot of business in our group, especially with the multi-tenants, the colos, and there has been conversation around purpose-built data halls for AI customers of theirs, and also, you know, the carve-out of existing space within some of their facilities, and the approach to each one is a little bit different in that, you know, when we start talking about a carve-out, it really is, you know, how much availability of power do they have, have they looked at the density, how is it going to affect the other mechanical and electrical systems that they already have in place, specifically around cooling, and egress, and those kind of things. When we see it as a purpose-built, it's a little bit different, because from that perspective, our customers are looking generally at what their loads are, and kind of working backward on what the infrastructure needs to be to support it. Got it. JP, did you want to add to that, or color that in a little? Yeah, so I think, you know, Doug did a good job of, like, when it comes up, it's whenever the customer wants it. I think that's a key there, but thinking about AI, like, the AI workloads have already been around, right? I think that this is more prevalent. So, if you're thinking about how you construct or design the data hall, if you're in the H-100 space, or if you're in, you know, an A-100, which might feel a little antiquated at this point, that's still air-cooled, for the most part, obviously, with Colossus being, you know, an exception, where they liquid-cooled those A-100, or the H-100s. But I think the big delta, where the big rub gets in the piece is when you have to do air-cooled versus liquid-cooled, because it changes the design a little bit more significantly, and causes the densification to go up. Thanks for that. When it comes to, you know, designs on paper, and what actually happens in the field, you know, is there a discrepancy there, particularly in this moment, where, you know, liquid-cooling is sort of on the rise? I don't know, the As-Builts are never the shop drawings, right? So that's never the case. Well, I don't know, has there ever been a data center that you have not had a design drawing match 100%? So, you know, you can have the containment be three feet off, or a foot off, and then it throws off the entire design, right? So that's never changing. I think that's still fairly prevalent. I think the impact of being a foot off on containment has a larger impact to the overall design, because it's much less of a two-way door, as it's been in the past, and things have been known. When you're going through that one-way door, it has a little bit more significant impact, because you have more than just containment, you have to move around, you have pipes, and a lot more electrical gear. Doug, what's your thoughts? No, I agree. I mean, you know, there's always that chance of an egress issue, especially when you're talking about overhead conveyance, and busway, and power, and, you know, containment and racks. And I think to JP's point, a foot doesn't seem like a lot until you multiply that by 150,000 square feet, right? It becomes significant. So I think, at least in my perspective, you know, those little egresses tend to, you know, throw a monkey wrench more than some of the bigger things you might think of. Understood. And one thing I'm always curious about when talking with, you know, guys who actually work inside the facilities doing technical stuff is, when it comes down to working at the level of co-location versus working in hyperscale data centers, you know, is it any substantial difference, and, you know, also in the volume of orders and stuff that you're going out on? I just, that's the question, colo versus hyperscale. I'll tell you, at least for our group, the group that I'm part of is, you know, primarily focused on the colos. However, the two are really intertwined, right? When you think about it, because the colos are building for the hyperscales. And although we do business directly with them as well, there is, again, there's that balance of how capacity planning for any of our customers goes, right? It's really, you know, what their forecasted build schedule looks like versus what's our availability of production, regardless if it's colo or hyperscale, it doesn't really matter at that point. JP, did you want to weigh in on this one? So if you had asked that question maybe a few years ago, there might be more of a difference with the retail colocation customer versus a building-level customer. But now you're looking at, you know, trends where you see campus-level customers, and that's, you know, public information where you're looking at, you know, Crusoe, right? So I think that's been really publicly announced that Crusoe is a large-scale campus that they're building throughout the world. They've had a couple pretty interesting press releases recently that talks about that. And, you know, that's being purpose-built. So I think, you know, you might want to think about different classifications where you have now the NeoCloud colocations that are for hyperscalers, colocations for retail customers in hyperscale, but it's a matter of mix and match. I think it's a lot more nuanced than one or the other. That's interesting to hear. I have to ask you about behind-the-meter power because on the Data Center Frontier site recently Eaton was in the news for, you know, stuff that's going on between Eaton and Siemens Energy, you know, getting modular natural gas going. So I just wanted to ask if you guys are working actively on behind-the-meter designs or if customers are asking you about getting going with on-site powering. Yeah, I think I'd go back to, like, the data center value chain. I think I posted on that back last November about, you know, you have four different assets in my mind. You have land, power, water, and fiber as asset classes before you decide to become a colocation provider or a vertically integrated provider. Then you have to design and build. After that, you can get to ready for service where the colocations make money. The infrastructure as a service level where you might be renting some GPUs or the software as a service where you're actually using end customer engagement. If you think back what we're trying to solve, which if you look at the release, it talks about that. Cyril did a really great job with the Siemens Energy partner discussing that. But it's all about solving for the power, whether it's, you know, what type of power you need in order to support it. You know, thinking through it, I would think about data center selection more of you follow the eyeballs or wherever the people might be. Then you follow the fiber or the photons. Then you follow the electron. You follow the molecule. And then hopefully one day the atom. So right now it's just a development of following the molecule wherever that natural gas might be. I guess another way to phrase the question too, and thanks for that, but is, you know, whether it's a grid connection or behind the meter, does that, how much does that affect the design that's happening inside the data center in the white space and the power rooms? You know, is it, I mean, does it, does it make a difference or, you know, just sort of curious about the interrelation between, you know, the power that's coming in from outside and what's going on in the design inside the data center? I don't know that there's a real difference there, Matt. You know, it's, it's, it's really about the availability of power, regardless of where it is and how it's going to be fed into the data hall, right? That's why you see more and more, you know, we, we, we talk a little bit about, you know, site selection and availability of power, but there's also the, the aspect of, you know, speed the revenue for these customers to how they, how they actually get from point A to point B as fast as they can. And, you know, from our perspective, we, we've seen a, we've seen a shift from, you know, that traditional stick build into more of the modular applications, right? So again, from, from, you know, when you're looking at the power into the building, I don't know that there's a tremendous design change there. I think it's that the design change really comes as part of like electrical rooms and that kind of thing. You know, how can we modulize them? How can we do them faster? How can we get them to site? Yeah, yeah, yeah. You got to forgive me. It's the kind of question that only a former English major could probably ask, but, but I thank you for bearing with me, you know? So I wanted to ask you, you know, regionally, you know, we talk a lot about the, you know, the major data center markets, Doug, I know you're based in Atlanta, I think, and, you know, we keep hearing about what's going on in Atlanta. And of course, there's loads of activity in Texas and Phoenix and the usual suspects. But, you know, I just didn't know if there was anything either of you could say about, you know, where the projects are taking you and also, you know, the different challenges you face in building in different parts of North America. That's not too huge a question. No, I mean, look, there's, you know, if you look at the traditional, you know, data center spots, right? The Northern Virginias, Phoenix's, Silicon Valley's, you know, Dallas, those kind of areas, right? There's an influx of data centers in all those spaces. And the constraint is always is during the availability of power, right? I mean, it was always, you know, fiber, land, and so forth, but now it's power. So we're seeing a lot of tertiary markets, maybe that's a way better way to put it than these, you know, secondary markets. But, you know, the old days of following the, you know, NFL cities is gone, right? Now it's where can I get power? Where can I get land? Where can I get access to fiber? And regardless of where that is, if it's in Columbus, or, you know, Idaho, or wherever, we're seeing our customers go to places we didn't anticipate, you know, 10 years ago, for sure. Yeah, yeah, we've really seen so many stories with, well, maybe the hugest one is Louisiana, you know, I mean, I met a building that a huge campus down there seems to have really put them on the map. But, JP, as the chief data center architect, anything you can say, or any note you wanted to put in on, you know, regional construction, any trends you've noticed, you know, this year, any more than any other time? I think I've just followed it wherever the tax payment is. Yeah. Yeah. It's that sometimes that simple. Yep. So, well, you know, sort of along those lines, or maybe not along those lines, I'm looking through my list of questions. And one thing that I was curious about when it comes to implementing designs, and, you know, if you're working on projects where it's multiple vendors, multiple disciplines, you know, how does that all work when it comes to putting the data center together? You know, do you guys have any best practices when it comes to working with, you know, the entire, you know, team that's involved in putting together a facility? Any insights you could share there? Well, I think from our perspective, you know, it depends on what your approach is there, Matt, but if you're looking at from conception to, you know, build, right, there are blocks along the way. When you're talking about conception, we've got teams of engineering assets within the company that will actually go with our customers and help them in formulating what's the best design strategy for them. As we kind of see that through design and then manufacturing and then to the site, they're one of the biggest assets that we have from that perspective is our PMO organization, which is our project management organization, right, that allows us from a communication perspective to really follow from design to turnover on the site. And that for us, I think, has been, you know, clearly something that's helped us, you know, define the roles and responsibilities and schedules and timelines and milestones and all those kind of things that happen, you know, as we start looking at, you know, building a data center. Again, I think, you know, there are facets of that thing, but, you know, ultimately it gets back to, hey, do you have good communication between you and all of these little pillars in between? If you do that, you clearly can and consistently can engage all of the stakeholders, whether in your company and the end users or, you know, the trades, that really helps us to get to a successful project. Another question I wanted to ask both of you is really just with liquid cooling, like, what's, where is it coming along in the discussion with customers now? Is it the first thing on the list, like, are we going to need, you know, I mean, obviously I know it depends on the size of the, you know, the density and if you're actually, but, and then, of course, you know, we know that there's a few different, you know, And then, of course, you know, we know that there's a few different choices with liquid cooling. I mean, I think, you know, a lot of it is, you know, or maybe most of it is single phase because of those NVIDIA designs that hinge on that. But as long as you got you both here, I really just wondered if we could get the up to the minute on the conversations that you're, you know, with the understanding that it's, you know, a blend of air and liquid cooling now, what the conversation is around liquid cooling when you're with customers. That's my question. It comes up, I'll tell you from the sales side perspective, that it comes up because as they start planning a data center, it's, you know, how engaged is Eaton with the, with the, you know, liquid cooling companies do and understand it. From a power perspective, we need to understand, you know, what the supporting infrastructure looks like. As far as the physical cooling itself, I mean, JP's probably got a much better handle on this than me, but the conversations are around, you know, hey, we're going to deploy, you know, X amount of these GPU clusters. And because of that, we're going to a liquid cooling system that looks like this. We're going to need to support that infrastructure with X amount of power and how does Eaton interconnect with that? Because keep in mind, although, you know, we talk about liquid cooling, we are primarily an electrical company supporting it, right? I gotcha. Yeah, important distinction. Well, you know, along those lines, I mean, you know, maybe you can just take me, JP, I'm thinking back to your presentation at Data Center World and it was very specifically talking about AI and the electrical infrastructure in the context of that. I mean, how, you know, within the last two years since the AI tsunami started, how has that changed your conversations with customers about, you know, what they're going to require or, you know, what the options are from Eaton in your designs? Yeah, I think I go back to like, what's the purpose of the data center? And the purpose of the data center is to support the chip. Right, so you have to get the electron into the chip, you have to remove the heat out of the chip, and then transport data from the chip for a human experience improvement. For over 50 years, since 1971, we had a commercially available CPU or central processing unit. Since 1999, we've had a graphics processing unit. And now we have a quantum processing unit or QPU available to the market. The liquid cooling piece, you know, if you're dealing with GPUs, and you're trying to get an opportunity there, if you're going and crossing over the threshold past H100 equivalents into a 325, AMD 325 or a GB 200 or beyond, that's when liquid will be required for that scope. So it's understanding what the customer need is, and then designing for it. Thanks for that. Doug, did you want to put anything in there? No, I think JP said it all. Okey doke. So as long as we're here now, I wanted to ask you guys about power usage effectiveness, PUE. In your minds, is that still one of the real gold standard standards? I mean, you know, it seems pretty ubiquitous. You know, I don't think it'll ever go away. But it just leads into a larger question, maybe in this age of AI, into the parameters that you're looking at, along with PUE. Just wondered if you guys had any perspective there? You know, because you know how it is, you hear people saying that PUE is becoming less important these days. So it's a two part question. It's like, why are people saying that? And then also, what are the other things that you're looking at in that context? So I think the concept is a good one, right? And I think that it became a real buzzword within our industry for a couple of years, right? Everybody was trying to measure and get to a certain number. And then, you know, however, some companies were looking at how they were measuring it different than others. I think ultimately, what it got us to was really what everybody really wants is, how effective are we using our power? What's our availability? How are we managing that? Are we utilizing stranded assets out there, right? And I think that that may be a big one. I mean, if you look at, you know, traditionally, when you talk about stranded assets, a lot of that used to be around cooling systems, right? Stranded capacity. But also, from an electrical perspective, it's, you know, there is a finite amount of power that is allocated to a data hall or a data center, right? How are you going to effectively use all of that in the places that you need to? And again, get from my customer's perspective, it's like, what's our speed to revenue? How effectively are we using this? How can we cut down on those stranded assets that are out there, regardless of what kind of matrix you put around it? It's really about, you know, being efficient and being, you know, economical with what you have. Thanks, Doug. JP, did you want to put anything in there on that point? I think PUE is very useful when you pick the right boundary conditions. If you don't pick the right boundary conditions, it doesn't become as useful to what Doug was pointing to. Okay. Now, what do you mean by that? For me, you know, who's kind of a dummy, the boundary conditions. The boundary conditions. Yeah. So, if there's where you sense the one, the denominator and the numerator on the PUE piece, that matters. And I think that's the conversation back to how effective utilization of power. It comes down to how effective are you using that power to cool the IT load? But are you having a significant amount of the overall power to the site, not going to IT? And that's really what that PUE is getting at. If you have metrics around that, that's what's useful. I think TUE and that white paper that was released, you know, I think over a year ago now, is probably much more in line with where we're trending, but the nine KPIs are useful. Thanks for clearing that up for us. So, this is a question that I usually like to ask folks. You know, what question am I not asking you guys that you wish I was asking you that we could be talking about here right now, in the context of, you know, your work here over the last two years? I guess maybe a better way to say it is, what would surprise people, you know, about, you know, the type of work that you're doing in the data center, if that's not too open-ended a query? I think it's just, you need to rethink about the labor markets, and that people need, you still need massive craftsmen. It's a master craftsman who builds your positronic brain and lays your fiber, and that's important. Yeah, that's a great point. You know, I was going to ask you about workforce issues. I mean, you know, in the more general sense, I mean, you know, because, you know, anyone will tell you, I mean, we were just at 7x24 last week, and I went to the office, and I went to the data center 101 session, and Dennis Cronin had a great slide showing just the jobs that were available at that moment in the data center sector on LinkedIn. And I mean, it was like 300,000, you know, electricians, you know, 20,000, you know, guys for the MEP yard. I mean, it was, you know, thousands and thousands of jobs. So, in your day-to-day, are you feeling that? Are you feeling that workforce piece, you know, that urgency to get more people working in this industry? I think there's always that, right? You know, again, to that point, you've got a campus that's under construction. Think about all of the trades involved at that site. At the same time, there's five more of these projects in the same geography going on. Yeah. Virginia has always been a good one to look at. And then, you know, it's how do you replenish or add to that talent as that talent starts to go through their, you know, their career cycle and get to the end of it, right? So, we've got a bunch of older people in this industry, too, that need to be replaced at some point. You know, how are we refreshing our bench? You're starting to see a lot of organizations that are supporting, you know, data center specific, you know, at that community colleges and programs even at the high school level that say, you know, just to educate, this is who we are, this is what we do, this is where this industry is going. And these are the kinds of opportunities for career payoff. I mean, for us, too, I mean, we are manufactured globally. So, from a labor perspective, we've got factories all over the place that need to be supported with labor. They need to be supported with that bench, right? So, as we promote people and move them through the Eden cycle, you know, how do we replenish that and strengthen? And so, that's a constant flow for us as well. But again, from the electricians and plumbers and, you know, excavators and, you know, the guys that are splicing cable and fiber and all that fun stuff, that's a lot of labor force out there. Yeah, it sure is. It's just such an important, really, aspect of what we do, you know, raising awareness of this industry, too, because there's just a lot of jobs on this side, as you note. So, maybe the last thing I'll ask you about here, as we get down to the end of our time for right now, is about events. You know, Data Center Frontier, as you know, has our event coming up in August. I know that Eaton does an event every year, and I hope to make it out to the Eaton event maybe next year. But maybe, JP, starting with you and Doug going to you, can you just tell me, actually, a little bit about that annual event that Eaton has and just, you know, what the gist is? You know, I assume it's talks and sessions and stuff, but I didn't know if there's anything you could say about that. I'll go first. So, we have an event called Data Center Vision, which is specific to Eaton. And the concept is that we invite customers and folks related to the industry to a two-day seminar, especially. And it's a learning seminar where, although, you know, it's obviously Eaton-heavy, we try to get the agenda to be more about what's going on in the industry, what are the trends, what are the issues that we're facing, how are we as a company and we as a partner to the space trying to affect positive change in those areas, whether it be through supply chain, whether it be through, you know, deployments of modular systems, whether it be specific technologies or, hey, what's the next generation? And it becomes not only that seminar focus, but it becomes a really good think tank for a lot of folks in our industry to come out and give us perspective on what they're needs are and what the needs of the industry are going forward. Yeah. Thanks for that thumbnail, Doug. And thanks really to both of you, Doug and JP, for joining us here today. It's great to catch up on the podcast. I hope it's not the last time we do it. Any closing words before we sign off here from either one of you? Don't mean to put you on the spot. No, thank you so much for having us. I really appreciate it. Yeah, it's really exciting to be on the podcast and love that Eaton Tech Summit and Eaton Vision is getting some traction out there. It's a pretty special event. So I really enjoy it. So we think we're going to be expanding it out as well. So it should be good. Yeah, it sounds good. I really hope to make it out there for the next one. So, well, thanks again to both of you. And we'll see you next time on the Data Center Frontier Show podcast. Thanks, man. Bye-bye.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-07-15\n==================================================\n\n### EdgeCore CEO Lee Kestler on Designing for Density, Energy Discipline, and the Future of AI Infrastructure\n**Episode ID:** 5084\n**Transcribed:** 2025-08-04 23:53:26.508274\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show, where we explore the future of digital infrastructure, one conversation at a time. And today, we're joined by a seasoned leader in the data center industry. It's Lee Kessler, CEO of EdgeCore Digital Infrastructure. So welcome in, Lee. Great to have you here. Matt, thank you. And it's always a pleasure to talk to you. Yeah, so let's dive right in. Let's talk about EdgeCore, you know, I really wanted to ask you about EdgeCore's growth strategy in this tremendous era that we're in right now, as defined by AI workloads and hyperscale demand. Right off the top, you know, we've covered EdgeCore, you know, over the years. So it's time for a thumbnail update for your state of the market here. Yeah, well, listen, thanks for that. You're right. The data industry right now is at probably an unprecedented level of enthusiasm and anxiety. The enthusiasm is around all the opportunities that technical evolution allows us through internet data centers. The deployment of artificial intelligence today now is giving us that new first inning, so to speak, like we were with cloud maybe 10 or 12 years ago. And so when you start thinking about that, and you think about the long trajectory ahead of all of us, you need a little bit of a sobriety check to understand this is not a video game. Okay, so this stuff takes a long time to coordinate. There are a lot of unanswered questions that we and the customers and the market itself have to answer to. And that takes a lot of time, intellectual property, collaboration, and of course, trial and error. And so when I think of EdgeCore, and I think of our role in this, and you can see from my design for density topic, we rebranded ourselves in the second quarter of 2023 with the expectation that 300 MVA plus campuses, multiple buildings with density and air cooled chilling systems, and obviously the ability to use closed loop liquid cooling in the future were the forecast for what our basis of design should look like. So we worked with industry partners like Holder Construction, and Beth Lowry, who's the CEO and president, and I have had many discussions over the years about the direction EdgeCore will take and their teams have been helpful for us. We also take a look at our constant interactions with customers who are informing all of us in the industry. There's no secrets to be fair, right? Everything is something that's leading to what a customer is asking, and they're asking all the people that they believe could be informing to them. And also providing a solution. So EdgeCore, intentional, deliberate, focused on very basic designs, things that customers are telling us that they want. We're not trying to be the innovative engineering firm. We are out there delivering and executing to what we think the customer wants. And you can find us in specific locations, and we're not going to veer too far out of those lanes. Yeah, you mentioned how EdgeCore was kind of ahead of the curve on, you know, I mean, I sort of envision you sort of sitting back like a proud parent. I mean, are you watching your liquid cooling strategies come to fruition now? And like, what is that like? Well, I think the answer to the question is nobody knows for sure exactly what the liquid cooling solution is going to be in an operational setting. There are still like different trials, I would say, are happening in the industry. You know, we're very close with Divertive, folks. We made a big commitment in our supply chain to use Vertiv technology, and so we spend a lot of time with them. We obviously spend a lot of time, our sales team is very good at listening, and we do a lot of work on that front, you know, what the customers are talking about, and we talk to the vendors. And so again, I get back to liquid cooling is an evolution. I think for now, today, air-cooled systems are going to be the dominant force, because the majority of liquid cooling and the applications that will need it and the types of infrastructure that's required, even down to valves, are all still being like contemplated amongst all the folks that are talking amongst themselves. So you know, there are no walls up. It's an industry that's focused, it's an industry that's collaborative, and it's a lot like, I would say, in the educational field or the government, you know, field, where people have to come together for the good cause of what is the customer and the end result. And so I don't see it as competitive as much as I see it contemplated. Well, you know, I was just at the 7x24 trade show in Orlando this weekend, and somebody made that very point that, you know, the vast majority of the cloud workloads are being driven by CPUs and air-cooled. So as much as we, you know, are carrying around the liquid cooling banner, and it's fascinating to talk about and track, you know, what, especially what NVIDIA has come out this year with, your point is very well taken. Not to jump around too much, and feel free to stop me if there's, you know, something that we should drill down into more, but we have to obviously talk about power, power procurement, and all the options and all the considerations, you know, from your desk at Edgecore is really what I think we'd like to ask you about. Yeah, listen, I love talking about power, and it always gives me a chance to talk about my involvement with power-related organizations from the past. So, you know, I've obviously been in the business for a long time. I'm proud of that, met a lot of great people. But, you know, in between jobs, so to speak, I spent a lot of time with energy. And so when I left DuPont, you know, after eight and a half years, I spent time at Oak Ridge National Labs in the innovation crossroads as an advisor to their first cohort. So I established a lot of relationships around what is going on in and around the lab and energy with the Department of Energy, et cetera. And then, of course, you know, after I left Vantage, I spent a year plus at NextEra Energy in Juneau Beach, which is the world's leader in development of renewables. And I'm a very big proponent of carbon mitigation strategies. But I also have a very sobering approach to how do we allow those technologies to become part of the solution? So when you fast forward to today and Edgecore, we are committed to sustainability. We are looking at water as the commodity, so to speak, that we don't want to use. So we are focused on air-cold systems, closed-loop systems. And from an energy perspective, you will use energy or electricity more efficiently if you've got a controlled environment. And so for that reason, you know, we've developed an insourcing program to hire everybody into Edgecore to run our facilities. And we think that that is going to give us a leg up, so to speak, and keeping our business running effectively to what the customer wants. Now you go back into the energy side and you think about what's going on with the utilities. Those people have a hard job. The integration of intermittent resources, and we've all had, I think, you know, the best that we can get out of what is there. But this evolution of electric vehicles, you know, manufacturing, advanced manufacturing, coming back into the United States, data centers and AI demand, we're putting a big challenge out there in an industry that can't move very quickly. So I think the opportunity for us, Edgecore, folks in the industry as a whole to help partner with utilities on what is coming and how is it coming and when, we spend a lot of time doing that. I personally spend most of my time talking to CEOs of the energy companies, working with Teresa Kerfoot and Michael Shaw on long range planning for how Edgecore is going to be a good citizen inside of their ecosystem and still get the things we need to drive the business, which is what the customers want. Lee, the next thing I wanted to ask you about is partnership models. You know, we do a lot of coverage at Data Center Frontier of how, you know, that's really becoming crucial in a lot of areas of the industry right now. So what can you say about that? Well, that's actually a good segue into Edgecore 2.0. And, you know, one of the original ideas that we had when we first decided to do this was that we had when we first decided to pivot the company with partners group support, you know, partners groups are leading investor. And quite frankly, they do more investing in energy infrastructure worldwide than they do in data centers. And so when we took a look at that, we decided that to accelerate and reduce coordination risk, we were going to establish partnerships with things that were tried and true. And so, one, we started a general contractor relationship, full service, withholder construction. Again, I mentioned Beth earlier, you know, we do our equipment procurement. We do a lot of things through them. And so they are really our go to partner. And that there are plenty of good construction companies out there. But for us, we needed a full service GC. We also needed a partner on our infrastructure side, like Vertif, who can offer more than just one product. And we also know exactly where we need to go to learn about what they're doing and how they're doing it, what it means to us, so we can translate that into all the signals the customers are giving us. Because I go back to this industry is about predictability, not us being innovators. There are plenty of folks that choose that course. So our partnerships are based on capital leads. We want best in breed of the people that can help us because we don't want to necessarily build a team out ourselves and we want to get the advantages of those relationships. And by the way, that goes right to community engagement. We hired Bill Jabginiak, you know, he retired from Mesa Economic Development. We hired Bill to be our national representative in public policy and community engagement. It's been instrumental for us in establishing ourselves in the markets that we intend to go big in and that we intend to be good citizens. So we build the data centers or design the campuses or choose the market based on how we think that the locals are going to feel about our presence there in the community, not just what meets the features and functionality of what a good campus looks like. So there's a lot of work that goes into those partnerships and Bill and Teresa Kerfoot and Michael Shaw and the list goes on even to our operations folks who are teaching in some of the community college data center specialty courses. Yeah, that's the level of commitment that we're making. And that goes even to women in technology across the country. We're committed to all of these types of partnerships. Tremendous. I really do genuinely think that the data center industry is a tremendous vehicle for good. And, you know, I think that because, you know, I hear this from people like you in the industry about the, you know, and of course, you know, the the statistics are out there about, you know, how many jobs are in the data center industry that really desperately need to be filled going forward. So one point on the particularly in the area of construction, we're talking about partnerships. One thing I heard at 7 by 24 this week was people talking more about, particularly in the context of of larger data centers, how logistics is like, it's kind of one of those issues where it's it's not the sexiest thing to talk about, but it's becoming central in the thinking for, you know, this building spree that the industry is going into. So I wondered if I could get some some reflections from you on that. Yeah, no, you're you're right to even bring it up, because five, six years ago, 30 to 36, 48 megawatt buildings were the biggest you could see. We saw some inklings of that, you know, changing in Ashburn, you know, with the digital realty and the cloud HQ builds. And so as we contemplated what we believe the customers were going to want from us, and of course, we have to explain that to our investors who are very good partners with a board that's set up with a lot of industry expertise that we rely on. Heather Dooley is our lead operating director, and she plays a critical role with us and some of our women in technology programs and our hyperscale understandings, et cetera. And so when I think through this, you know, aesthetics on the building became a primary example of what I mentioned earlier. What do the locals want to see? And so we've got a mural program. You know, we're working within the community and the neighborhoods, of course, to try to make sure that our aesthetics before we go to the board of supervisors are acceptable. And so aesthetics of the building are becoming more and more important. So screening for rooftop units, for air-cooled chilling, you know, sound, deafening sound down and bringing sound down. We work very closely to figure out what the decibel level would be for any kind of sound that may be coming out. As a matter of fact, I was in a meeting this morning and we're talking about wind direction as we figure out the design so we can go back to the municipality and say, look, we looked at this. Here's what we believe is OK, and here's why we think it's OK, and here's what we're going to do to make sure it's OK. So aesthetics are super important. The buildings are going to keep getting bigger. I think that is the direction we're going now. Maybe footprint, they won't get bigger, but density will be a bigger factor as we continue to get into the liquid cooling solutions. Racks are going from 25 kW to 95 kW and above in the future. And we're future proofing our facilities to be able to handle those things with thermal storage tanks, etc. So all of this plays into those aesthetics. And it's important what the community thinks that we're going to put on those properties. Yeah, yeah. The waste heat handling has been a real focal point this year. I know in the context of aesthetics, you know, we don't always talk about architecture, but some of the slides I saw this week, you know, Gensler's name was prominently up there. Are there architectural limits? And I heard somebody make the remark in the context of looking at Mr. Wonderful's Alberta data center that, you know, and he showed, you know, his movie is great. And but somebody said, you know, well, there's a reason that that data centers are big concrete boxes. But and I just kind of took that in because, you know, I don't know if that's true. I know that there can be some very aesthetically beautiful data centers. You know, but are there limitations in the architecture? Are there certain things that, you know. Is there a limiting factor in how, you know, architecturally complex you can make a data center, or is it just a balance with the engineering? Well, it's it's important to understand what are you building for, you know, and and I like Mr. Wonderful, by the way. He likes watches. Partners Group owns Breitling. So I love it. Yeah. Freddy's watching this video from Partners Group. Listen, a data center, a data center can come in all shapes and sizes, but there's a couple of key things that drive the materials. Number one, we need to be able to handle the heavy duty weight of what is going into the building on multiple stories. Number one. Number two, we can manufacture the aesthetic side of it on the outside, you know, the skin. We can manufacture some things that make that better to the eye or even just look better than industrial, which is what, you know, kind of the early days of, you know, the Ashburn corridor data center alley where so I feel really good about that. But the buildings themselves, they need to be extremely physically secure for a lot of different reasons. They need to be able to be built with materials that are going to last a long time and handle heavy duty weight loads and heavy duty, you know, wind, etc. And remember, the equipment on the roof is not lightweight either. Yeah. You're going to go wider with the land, which we don't want. We want to be very, very conscious of land consumption because we're trying to we're trying to impact our footprint, so to speak, as minimally as possible. And so, yeah, I think you'll see the building stay around the same size. I think we all are doing a better job. And I appreciate, you know, the folks that are in the commonwealth legislature in Virginia or even the folks in Mesa's, you know, government who are helping us to understand what the citizens would like to see. That's possible. And we're able to work alongside of them to make that happen. Understood. Back to the power discussion, you know, behind the meter powering, it's been a big story, a trend that we're following, particularly this year with, you know, big deals like and companies like Volta Grid, where, you know, the expectation is that there's going to be a lot of more natural gas building. And then, of course, you know, within the industry, you know, there's, you know, a trophy now that natural gas is a bridge to nuclear. And so that's what I want to ask you about is just generally, you know, because there's, you know, some people will tell you that they think that, you know, nuclear is still kind of a pipe dream, never see it until 2035. And then, of course, you know, the SMR manufacturers, some of them are projecting to, you know, they say have it before 2030. So it seems clear that SMRs and nuclear are going to be a part of the mix, which will always have to be a mix, you know, going forward because you can't have all kinds of, you know, some places will be OK for nuclear, others won't be. Anyway, getting sort of circuitously lost in my question behind the meter powering, wanted to ask you for your take on what's going on right now. Yeah, listen, I have I have an opinion and it's a strong one, and I know there are a lot of qualified folks on the engineering side who've looked at this. I think there's even more qualified folks in the Department of Energy and in our utility partners who are very well versed in this. But I have a good perspective because of my background with Oak Ridge and paying attention to what's going on in SMRs. I mean, Bob Berlinski, who is our EVP here, you know, he and I were very much looking at SMRs back in like 2008, 2009 and saying, wow, I hope we're around long enough for that to be possible. So I believe, you know, the Xenergies and the Kairos and the TerraPowers, you know, and I've met Jeff Waxman, you know, who's running Project Pele for the ODNI. I would not dismiss any of this. I think they're all very realistic. I think there is a series of steps that are still necessary for all of us to maintain a level of optimism that they will get through, whether it's 2035 or it's 2032. I don't know and I don't think they know, but they know how to get through what it is that they need to do before the end result is a go. And I believe that behind the meter and, you know, I worked on that in Florida in the past and, you know, I think the bridging power solution is a better way to describe this. I think behind the meter, while there will be a solution to behind the meter in some capacity, and I love the way you framed it up and I think people are starting to say the same thing, that it's not one size fits all and there's not one solution that will work. For us, behind the meter is going to be bridging power. I don't believe and I don't think the industry should want to build power plants specifically for a data center that can't consume all of that power day one. I think we, because of our base load capabilities and our ability to show long-term development cycles, can enable the utility to do the planning. The utility folks are the experts at this. And while we understand all of it, my caution for all of us in the industry is, look, let's not insult the subject matter expertise of these folks because they're doing it every day and we're there to help them. So for us, Edgecore, we're there to work with the utility partners. Rappahannock Electric, we've met with their staff. We love talking to them. We love talking to Dominion and SRP. We are going to be supportive as a big base load customer that enables them to deliver these resources. But I do believe there will be a future behind the meter. It won't be exclusive. I also think SMRs are when they're ready and it won't be driven by us. So we need to be there in the territories for the utility so they can afford to earn the revenue that will allow the R&D to make these new insertions of generation possible. And that includes wind and solar. I'm not a big fan of wind. I'll go on the record for that. So but I will tell you, solar and storage are definitely the way to go. Wind and only a few key places where it matters. SMRs are coming. I think it's going to be a very iterative process. And I think base loads for utility companies are going to drive their ability for the R&D to buy from Kairos, to buy from Exxon, et cetera. So that's our take on it right now. And I think the partners group folks who are, you know, lead investor will tell you they're behind all kinds of energy opportunities as investors. But there is a stage here where science has to lead. And it just can't be wishful thinking like we all aspire to have. Understood. Before we wind up, I wanted to take you back to the data center side of the questioning and just ask you about Edgecore's regional focus, what you've been seeing, what you've been doing. Yeah, listen, we're very deliberate, intentional, like I mentioned at the opening of the podcast. We want to be we want to be very good in a couple of key places where we believe customers, when they look, they're going to look for a handful of partners in each geography. I don't I don't believe anybody is necessarily like better than the other. It's it depends on timing. And so we're going very, very big in Arizona. And in Virginia. And there are reasons for that. And so as you see, you know, we still have our headquarters in Denver and we have a large contingent of operational folks that are there, but we don't have a presence in Colorado. We like the incumbency of the relationships with the utility. I think when the customers decide, hey, look, when we need something, Edgecore is definitely one of the vendors that we'll look to, because they have already proven their ability to execute. And you can predict what Edgecore is going to have available. That could be based on long term land purchases. And we're getting ready to make an announcement in the next three weeks. You know, today is what, June 12th. So sometime before the end of June, you'll hear another significant announcement. That is the trend that you would think of Edgecore for large campuses. 300 MVA are bigger. You've got a building that's growing in size of 72 megawatts. You know, we've built 108 megawatts already. We've got 144 megawatt design. We have all of this in our pipeline. And so our job as real estate developers and focused on this industry is to take properties from where it is today to a stage where the customers can see a light into the horizon of which they can buy from us. And we know that with supply chain and coordination risk, we have to be experts on those things for a customer will consider us in that shorter window that they will buy in. Thanks. I can't resist a lightning round question. Do you see, you know, places like nearby places like Salt Lake and Reno, like mirroring what's happening in Phoenix? I mean, you know, sort of just seems like there's a lot going on. Yeah, listen, I am. We obviously own land in Reno, you know, the Edgecore 1.0 group. We're very good at picking key markets. And this is 2018 and 19. And so I think we got a good start because we're in Reno today. And so we've owned land there for, I don't know, four or five years. We do not have land in Salt Lake, but I'm very positive on Salt Lake. You know, Michael Shaw, I think, spends a good amount of time looking around that region. We really do like the SRP territory. It's nothing against APS and Phoenix. It's just we've established a pretty big presence there. We just announced another parcel of land that we acquired that's adjacent to our current site that's already fully operational, you know, right off of Eastmark Parkway and Elliott. And so we think that Salt Lake, Reno, you know, when you take a look at places like Ohio, it's been very, very hot. We're not looking to go there because we think we will be late to the game. But you look at Utah, yep, high on the list. We think it's good. You know, you look at some of the places in the southeast, like Atlanta. We think there's opportunities in places like Georgia. But, you know, the sky's the limit. But you have to be very focused and understand the capital needs. And so we're going to be very deliberate in the locations. And we've picked Arizona and Virginia for today. And you may see a few other surprises. Okay, well, it's a good thing that we do ongoing coverage of EDGE Corps. But, you know, there's a reason. And it's the insights that we're getting. And I think the reach, really, that the company has. So we've got a little bit of time left. Earlier in the interview, you talked about how committed your company is to sustainability. So I wanted to ask you about that in terms of, you know, big picture, you know, what makes for actionable sustainability, you know, because I think you're exemplifying it at EDGE Corps. And then also, you mentioned the investor angle. And I am curious about sustainability in that context, too. Sure. Listen, Partners Group, you know, as part of a commitment to us, we get to take advantage of resources within their organization globally to inform us on all fronts of these types of conversations. Sustainability and safety are paramount and primary for us. So we get that advantage, you know, taking that feedback from them. When I think about sustainability as it pertains to EDGE Corps, day one, when Clint Hyde and I sat down and said, what is EDGE Corps 2.0 going to look like? We're not going to use water. We're going to make a commitment. Our test will be in Mesa, where Arizona is the best place you can test yourself for what you really think you're going to be capable of. So we decided to remove water. So the evaporative chilled water plants that you've seen, you know, that dominate this industry, while highly efficient, we don't want to consume that precious resource. So technologically, we've challenged ourselves, you know, working with Holder and our engineering firms that we outsource to to make sure that we stick to that. And by the way, the customers, SOQs or RFPs or OPRs, whatever you want to call them, you know, everybody sees this stuff that's in this space. And everything is telling you, be a good steward of the environment. Give us the service that we need. Yes, we know densities are going up, but if you can't raise your bar and you can't participate in this, then you won't be servicing us. And so it's a very high bar. From the energy generation perspective, listen, it has to be iterative. You cannot do without energy. I don't care how strong you feel about sustainability or renewables. Everything creates pollution. It's how you use it. And without data centers or heavy baseload users in your territory, the utility companies cannot make investment in the iterative and ongoing sequence of operations to develop renewables. We're all pulling for them, but we need energy to create commerce that can create tax base and create revenue for the utility companies to make these investments. Partners keep 100% behind it, and so are we at Edgecore. Such a valuable perspective. So you've been in the industry for decades and are at the cutting edge of it right now. So I wanted to ask you sort of a fun question in terms of what you might characterize as the most misunderstood or commonly misunderstood trend in data centers right now. Yeah, that probably is a long list, but to pick one thing, I think what's most misunderstood about our industry is what we just talked about. The collaboration and the collective energy that we are putting together through groups like the Data Center Coalition, which is based out of Virginia, through our work with municipalities and economic development folks in each geography, like you're missing the boat if you don't think that we're in that to talk to people and learn more. So I believe some people misunderstand us as we're coming in to consume. No, we're giving and we're giving a lot. And we're giving a lot in ways that not only is it intellectual property to help the places where we're putting a flag in the ground. We're paying tax revenue. We are employing people. The biggest misnomer in this space is there's no jobs. I spoke to 1600 employees who have been employed for 19 months on our Mesa campus, and the majority of them are going to be there for another 14. And then we start building on the property next door. They'll be there for another six years. So there is permanency and jobs for construction. And the industry that we are in is enabling that to occur. And it's career pathing. You know, I had lunch with those folks. They're buying houses in the area, right? Because they have a steady job and there is a future with data center. So to misunderstand the contributions that we make from an economic development perspective is probably the biggest thing that everybody should pause and have a sobriety check and be more appreciative of.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-07-10\n==================================================\n\n### CoreSite Expands in Denver with Strategic Acquisition of Iconic Carrier Hotel\n**Episode ID:** 5085\n**Transcribed:** 2025-08-04 23:53:26.508321\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier. And today we are here with Yvonne Ng. She's VP and GM for the central region at CoreSight and accompanied by Adam Post, who is SVP of Finance and Corporate Development for CoreSight. And today, the topic of today's podcast is we're going to be delving into CoreSight's recent strategic acquisition of the Denver Gas and Electric Building, widely recognized as the most network-dense carrier hotel in the Rocky Mountain region. And we know that this move not only cements CoreSight's leadership in interconnection with the expansion of its DE1 facility, but also streamlines access to the Google Cloud platforms on-ramp and the NE2 Denver Peering Exchange. So during today's talk, we're going to unpack what this move means for CoreSight and the Denver market and its cloud and network providers, and for the future of high-performance computing in the region. So welcome in, both of you, Yvonne and Adam. Thanks for joining us on the podcast. Thank you for having us, Matt. Yeah, absolutely. Thank you, Matt. I really appreciate you having us and providing us with an opportunity to discuss what Yvonne and I view as a really strategically important acquisition for CoreSight. I'd also say it's near and dear to us. As you may know, CoreSight has been headquartered in Denver for over 20 years now. Yvonne and I are both local to the area. So we're excited to be able to acquire such an important asset that's in our own backyard. So thank you for your interest in learning more and for having us on today. Absolutely. I always love to catch up with CoreSight at Data Center Frontier. So let's start in with some questions for both of you to chime in on. With CoreSight now owning the Denver region's most network-dense facility, how would you say this sharpens your edge in interconnection as AI, multi-cloud, and real-time workloads reshape demand? Well, with AI, especially AI inferencing, with enterprises progressing from having not just cloud connectivity but use of multiple cloud platforms and the need to interconnect between them, and then just our everyday, day-to-day lives demanding real-time access to just about anything, interconnection is more important than ever. Yet, our 2025 State of the Data Center report, which launched today, by the way, found that only 19% of the respondents say their co-location provider offers interconnection to a variety of cloud providers. CoreSight, we, as you know, pride ourselves on our interconnection capabilities, and the acquisition of this Denver Gas and Electric building really strengthens that proposition, as we can now offer our clients direct connections to 24 cloud on-ramps across our 11 markets, including Google in Denver. Absolutely. I wanted to sort of zoom out a little and ask you both, what lessons does this move with the acquisition of the Denver Gas and Electric building, what lessons does this offer about topics like real estate, control, vertical integration, and the value of carrier hotel ownership in today's data center markets? Yeah, Matt, I'd say that this move to acquire a carrier hotel is right out of CoreSight's playbook. Our strategy as an organization is to operate data center campuses in each of our 11 markets. That means operating multiple buildings within close proximity to one another that we can connect with fiber, and typically dark fiber, and at the center of those campuses is a network-dense asset with a very rich ecosystem, which Yvonne just described. We've actually operated out of this building for over 12 years now, so we know the building well and the customers within the building really well. To address your question about ownership and owning real estate, although we've operated in the building and we've known the building for quite some time, owning the real estate now provides us with more flexibility and control over our assets, how we connect them to the other assets in our portfolio, and ultimately provide the most seamless offering to our customers as we support their digital transformation. I'd also offer that through owning the building, we're able to uncover and provide more additional capacity and density to our offering here in Denver, so we're able to serve more Denver area businesses and support everything from global business to medical breakthroughs to education for underserved communities. Really important for us to own the real estate and have the flexibility and the ability to operate as we best see fit and best fits our customers. Absolutely. Thanks for connecting those dots for us. Yvonne, by simplifying access to the Google Cloud on-ramp and such, does this move position Denver as a new cloud-peering hotspot, or how would you characterize that? Yeah, I would say that Denver has already been growing as a cloud-peering hotspot, but this move certainly enhances that position. With this acquisition, Quartzsite is the only data center provider in the region to offer direct connects to the major cloud providers, so we are the best position provider, I would say, to offer the multi-cloud connections our customers tell us that they're searching for. And the next thing I wanted to ask you is, in light of the new acquisition, how does DE1 now fit strategically alongside DE2 and the under-construction DE3 facility, if that's correct? And ultimately, the question is, is Quartzsite building an ecosystem across multiple generations of infrastructure, or how does that all shake out? Yeah, that's exactly right, Matt. Our approach at Quartzsite is to build interconnected data center campuses in the markets where we invest. So as you described it, it is building an ecosystem across multiple generations of infrastructure. So to do this, we start with a carrier hotel, such as DE1, which we just acquired, and we then pair that network-dense facility with other enterprise-class and purpose-built data centers, such as our DE3 facility, which is currently under construction. This allows us to then meet the changing needs of our customers and support their digital transformation. I'd also add that we're designing our purpose-built data centers for modern workloads, things like AI, that require higher densities and the latest and greatest supporting infrastructure technology. And then adjacent to these purpose-built assets are our carrier hotels, which are typically of an older vintage, older generation, so they accommodate less high-dense workloads. So by combining these two strategies of having multiple generations of infrastructure, it just enables us to offer a variety of options to our customers, and we can tailor those options to our customers' specific needs. Sure. Well, those remarks lead me to my next question for both of you, maybe starting with Yvonne. I wanted to ask what investments or upgrades are being planned to modernize the historic DE1 facility to support high-density AI and enterprise compute? It might boil down to a liquid cooling question. Just kidding. But I assume that's part of the mosaic here. Yeah. We're certainly actively evaluating the building's current infrastructure. And plan to make investments where necessary or beneficial to give our customers the data center product that they have come to expect from CoreSight. With that said, specifically for DE1, as you know, it's a Denver Historic Landmark. It's on the National Register of Historic Places. Our intent is to preserve this piece of Denver history. And so we'll follow all the requirements, design guidelines, to remain true to the building's history, particularly on the facade on the outside and then on the inside. Again, evaluating to see the right investments, make sure that our customers have the right products from us. I will mention that, you know, the Tempus idea that Adam has been talking about, having DE3 and the Tempus there, tethering back to DE1, DE1 facility will largely serve as the interconnection hub, and then we'll have low-latency fiber tethers to DE3, which, as mentioned, is purpose-built. That will be the building to do the liquid cooling, serve the high-density workloads like AI and enterprise compute. So we're not necessarily expecting that DE1 will become the liquid cooling asset in our portfolio, but it will, together with the DE3 Tempus, form that whole ecosystem that will bring the scalability and high density to our customers. Yeah, definitely very much understood there. But, well, I mean, I wanted to also ask about the benefit of downtown area-located interconnection hubs like DE1 in a world of sprawling hyperscale campuses and stuff. Can you talk about the utility of that a bit? Yeah, I think interconnection hubs and area hotels remain critical in enabling the ecosystems that serve our digital lives. Like Adam shared, they're still a very important part of Forthyte's portfolio, particularly given our focus on serving as the quote-unquote core site for our customers to build the cost-effective, secure, low-latency, competitive edge that they need to succeed in today's environment. And really, as you know, there are different use cases being fulfilled in different data centers. So, you know, sprawling campuses on the edge of markets, hyperscale-type data centers, fulfill different use cases from what these interconnection hubs will do. And ultimately, whatever workloads, use cases, machine learning that's happening in those campuses, eventually that data needs to get to users, right? It needs to get to us, to businesses. And we can't do without the ecosystem in the interconnection hubs to bring that to the eyeballs, to the users. So I think they remain as viable as ever. Absolutely. So from reading the press release, it looks like CoreSite gains about over 100 customers through the acquisition of the Denver Gas and Electric building. Can either of you talk a little bit about CoreSite's game plan for integrating these new customers that come along with the acquisition? Yeah, the integration, we're actively working on that, working cross-functionally. And it's all the customers of the systems, of processes. And fortunately, I would say we already have relationships with many of these companies, either through existing partnerships in the building or in other CoreSite markets. So we are certainly actively making sure that this integration will be smooth. And again, we'll deliver the product that our customers have come to expect from us. I'd also just take the opportunity to provide a shout out to some of the CoreSite team. And we've got really good systems in place, a great IT team, a great integration team that's in place to help us through integrating this asset and all of our customers into the CoreSite portfolio so that all the customers that are there can be part of CoreSite and get the same experience in this building that they do in any of our other CoreSite buildings. So really thankful for the team that we have in place. Absolutely. So I wanted to ask, most in the audience probably know that CoreSite is owned by American Towers. So how does American Towers ownership change the calculus for investments in urban interconnection sites and carrier hotels like you've just acquired? And what's the forecast for possible strategic acquisitions in other US cities? That's the question. Yeah, yeah. Great question. And American Tower has been a great partner to CoreSite over the last three years, three plus years that they have owned us. They are very supportive of all of our growth goals. As we talked about, this acquisition is right out of the CoreSite playbook. And so they're very supportive of us continuing with our existing strategy and the strategy that we've been implementing for the last 20 plus years. And with the financial backing and support really of both American Tower and Stone Peak, there is no doubt we have access to the capital required to meaningfully expand our campus footprint and meet our customers' growing needs. As we've been talking about, CoreSite's core expansion strategy is to invest in development opportunities that replenish sold capacity in our existing markets in order to meet current and future customer demand within the CoreSite ecosystem that we've been describing. So we're always exploring opportunities to make strategic acquisitions like the purchase of this Denver Gas and Electric building. Yeah. And maybe to wrap up, Yvonne, I especially wanted to ask you, but Denver is being mentioned more and more as a growing data center city, especially in the AI moment here. Kind of my perception is of Denver as like a really strong and growing tier two market. I just wanted to ask how it looks on the ground there for AI and data centers in terms of your perception of the development and the uptake. Yeah. I mean, it certainly is a growing tier two market. I agree with you hearing more and more data center providers coming into the market. And the annual absorption, at least for the last three years, have been doubling. So we believe in this market. As Adam mentioned, this is our own backyard. And with our DE3 campus, we think that more and more of different workloads, not just our use cases, not just of AI, but also cloud companies, multi-cloud connectivity will come into the market. And especially with some of the more well-known tier one markets running out of power, and with Denver geographically being in the center of the country with great fiber connectivity and great tech talents, it looks to me, it looks to us, that this will be a great data center market that's growing. Definitely. So final question for both of you, just maybe asking for some reflections. I mean, we're sitting here at about the halfway point through 2025. And I just wondered, maybe if you could, in closing, offer us some words of perspective or what you've been seeing for this year in terms of how it's matched up. Obviously, I think the AI boom continues to expand, although we've seen some ripples in the news with stories about Microsoft and Amazon and others pausing on leases and stuff. I just, you know, how do you see the industry from the mid-year point? Is it the, you know, is the unstoppable kind of freight train roll on here with AI or just any other perceptions from CoreSight's business? CoreSight, of course, is one of the really, to me, one of the, not just to me, I mean, we know it's one of the foundational operators of the whole game. So just kind of wanted to ask, round up, close on a big picture question like that. Yeah, I think that the industry in total has seen a lot of demand over the last few years, and that has continued here into 2025. A lot of the demand that we have seen and has been reported about over the last couple of years is really on the hyperscale side of the sector. So we're talking about 50, 100 plus megawatt types of deployments in buildings. And that's not necessarily who CoreSight is. We are a multi-tenant co-location provider, and we're signing leases that are, you know, typically less than two megawatts in size. And we try and fill up our buildings with dozens, if not hundreds of customers, rather than just one or two customers. So I give that background because we, with some of the news of hyperscalers and other providers pausing on some of their buying needs, that hasn't really impacted CoreSight, because that isn't the opportunities that we've been chasing the last few years. I also think there is a lot of demand that will continue to be out there as more AI gets deployed, machine learning gets better and better, and there's more inferencing capabilities. Once those inferencing capabilities are really developed and deployed, they will likely go into a CoreSight type data center. So I think there's a long tail here for CoreSight to be able to continue to be a meaningful player within the data center industry and market. Got it. Great insights, and I'm glad I asked that question. So, well, thanks so much to both of you, Yvonne and Adam, for joining us here today. As I said before, it's always great to catch up with CoreSight, and we will continue to do so in the future. So just thanks again. Thank you, Matt. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-07-01\n==================================================\n\n### Hunter Newby and Connected Nation: Kansas Breaks Ground on First IXP\n**Episode ID:** 5086\n**Transcribed:** 2025-08-04 23:53:26.508580\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief of Data Center Frontier. And today we're heading to the heartland of Wichita, Kansas, where a bold new project is taking shape that could transform the way connectivity works across the region. Connected Nation and Wichita State University have just broken ground on Kansas' first internet exchange point, and that's an open-access, carrier-neutral IXP that promises lower latency, reduced costs, and better performance for everything from cloud services to AI applications. And so joining us here today are two leaders at the forefront of this initiative, Tom Furry, CEO of Connected Nation, and Hunter Newby, a pioneer of network interconnection and co-founder of CNIXP. Welcome to both of you to the podcast. Thank you. Great, Matt. Great to be with you. Yeah, it's great. It's great to have you both here. So let's dive in. Tom, in the press release for Connected Nation, you said that this new IXP is more than just a facility. It's foundation. I wanted to ask, what does that foundation really enable for Kansas, and how will this change the region's digital future? Sure. Well, Matt, it's great to be with you and your listeners today. More specifically, I'll start by clarifying that our joint venture is Connected Nation Internet Exchange Points, and that's the marriage of just two incredible organizations, one Connected Nation, a 25-year national nonprofit that's dedicated our mission to solving the digital divide in every expression, whether it's geography or adoption and literacy. We tackle it all from soup to nuts. We have the good fortune to have now at our altar, and just made official here, I guess, last couple of weeks, our marriage with Hunter Newby. I say that just last week. We've had a long courtship, I think Hunter would agree, over the last seven to eight years to get to this point. Let me say, it is the perfect marriage of competency, know-how, and passion with mission. When you put all that together, you talk about establishing a foundation. Let me say, what Wichita represents, not only for Kansas, but as an exemplar for the rest of the country, and we'll get into what our expansion plans are. It basically, in a word, is empowerment, Matt. Particularly groups like Wichita, who don't have an IXP, an independent, carrier-neutral IXP in the state, with this action now, are putting themselves quite literally on the map, figuratively and literally, to begin to invite investment, invite networks, invite all the quality attributes that your suburban and metropolitan areas already, and have had for a long time. When we look at what the digital divide looks like, what it looked like when we started in 2001, versus what the new dawn and new frontier represents from a digital divide, it's no longer binary about, is the network there, or can you access the internet there, and Hunter does really good to get into the deep weeds on that. Accessing it and accessing it are two different things, I guess I would tell you. We are literally bringing the internet to Wichita, and that has profound, and particularly in the neutral venue that it represents, invites all comers. That's something that has been part of our creed from the very beginning. It's network, neutral, technology agnosticism. We want anybody who has an opportunity or a solution to bring it to the fore, so that we increase competition, we improve resiliency and quality. Only when those three tenets are met, our community is going to be on equal footing. AI and some of the things that are coming down the pipe, if communities like Wichita don't have the proximity to these peering opportunities and co-location facilities, that divide is going to be so expensive and so crushing to those that don't benefit from the proximity facilities like this, one that we're so excited to be launching there. We have just an incredible partner in Wichita State University. We'll get into that in a bit, but that expression of our two organizations, but then the third leg on that stool, is an incredible partner with Wichita State University. Like I said, we couldn't be more excited about it, and I should probably shut up and cede to my partner here to tell you his point of view, but we're very excited about it. That's the foundation. From this, so many things grow. Yeah, Hunter, did you want to ... Thank you, Tom. Could you chime in a little here on the partnership and the project? Sure, and thanks for having me on. I echo everything that Tom just said, and from my perspective, my half of the joint venture is my history in the industry of designing and building and owning and operating and acquiring buildings where networks reside, fiber networks, disparate multi-tenant interconnection facilities. That's what I've done for well over two decades now in several different ventures. That's the know-how, right, as Tom was referring to. I've done a lot in this space, in the industry, things that were not done before. In many instances, they were groundbreaking in and of themselves, and every time I did one in the first place, it was New York City, and it had never been done before. People started hearing about it and saying, no, that won't work, or you can't do that, you're not allowed. Then after I did it, they said, well, did you ask permission first? I'm like, from who? No, it's a solution. Everyone in the network's needed, created it, necessity's the mother of invention, and here we go. Then replicating that original neutral interconnection business, which means not owned by a network operator itself. The real estate is separate from the network. The real estate is independent, it's truly like a landlord-tenant type situation, but for the purposes of organizing disparate physical networks in a room, because they all need to be interconnected to each other, and there's an entire process around that entire thing, shipping, receiving, install, tech support, power, cooling, operation. Not a data center, not a single tenant thing for servers predominantly. It's a multi-tenant and very focused on fiber and outside plan and on optical transport interconnection. Putting that aside, the first acquisition that I did was of an entire building in a different city. That was 56 Marietta Street in Atlanta, Georgia. That was in May of 2004, so that's over 20 years ago now. This whole history is pretty well documented by myself, and I guess others have written about it and whatnot, but as now we're evolving, the industry's evolving, the country's evolving, and I always had this thought in my mind, this challenge for myself of how do I take what I know how to do and bring it out to the cities where it doesn't exist at all? I was used to following long-haul fiber route maps, finding other carrier hotels, that's what they're known as, like 60 Hudson Street in New York, where I started, and then building relationships with the owners of those buildings and eventually acquiring them, the building and or the room in the building where all the networks meet, which is referred to as the meet-me room. I was successful in doing that for the past 20-plus years, many times over, like a couple dozen. But now going to places where they don't exist at all, like Wichita, is a new challenge. That's how I met Tom and Connected Nation and their mission, as Tom referred to it, is to bridge digital divide and close that gap. The problem is it isn't just about fiber or just about broadband. These are words that have a lot of different meanings and definitions when you really dig into that. What does it mean? What is broadband? It's defined by speed, and it's different in almost every state. What is fiber? Is it dark? Is it lit? Is it for lease? All these things. What it lacks, though, in the states that, very specifically today, do not have an internet exchange in the state. What does that mean? A neutral exchange for the IP traffic to reside in the state to keep that traffic local to that state. There are 14 in the US that don't have one. I've actually discovered all that through the research I've done while I'm buying and developing these buildings. Kansas happened to be one of them. Wichita happened to be one of those cities. We targeted that, and we developed a plan and outreach to the school. That's how we got to where we are. Explaining this concept to people from Wichita, it's like explaining something to people that never heard of it before, even though all their IP traffic, the vast majority of it, all routes and back calls to Kansas City, Missouri, on the Missouri side, different state, which is a building I used to own and be a partner in, to an internet exchange that's in that building that I used to actually be involved with. They don't know that. By developing one in Wichita, it will help keep traffic local to Wichita. The absence of the component with Connected Nations mission, with fiber and broadband for rural America, it was lacking neutral real estate, a neutral host home for all those networks in those states to actually interconnect in and exchange traffic. That's what I know how to do, and that's really where, as Tom referred to, our marriage comes from. I knew that they are always focused on rural, and all the governors and all the senators and all the congressmen and all the mayors and all the presidents of all the schools, and now we can target the cities that literally do not have the internet present in that city or in some cases that state, build a physical venue for it that's neutral, and then bring the networks to it such that the traffic can be localized. I'll just finish with this. Today it is more important than ever with the advent of AI, and particular flavors of AI that are just now starting to come to the fore. Inference and gentic AI, particularly inference in the case of the low latency requirements exist today. It didn't exist commercially the way it does now three years ago. Can you imagine what it's going to be like in three years? The necessity for localized interconnection, the localized handoff of IP networks talking to each other that are carrying AI applications will be acute for the cities that do not have it. There will be applications that simply do not work there, period. That's almost like looking at video of the internet was back in the dial-up days. It just doesn't work. Then they needed to upgrade, and cable modem came, and fiber, and everything. It's like, okay, now we have video. Now we have duplex video. There are applications that do not work if the latency isn't low enough. There's lag, there's jitter, there's packet loss, and you could feel it. It's affected in the video if it's duplex, and the one person's lagging, and you don't hear them, and the picture's bad. Compound that to AI applications that are happening in milliseconds by themselves. If the distance is too great between the send and the receive, all sorts of problems are going to happen. We're really addressing that. If you want to connect the physical aspects, the ground, the manholes, the building itself that we're constructing, what's actually taking place in there is effectively creating a platform where applications can actually function in these cities and locales, or they just won't. It'll take too long. The other analogy is mobile phones. You have 5G in the cities. It's a number and a letter, and then if you go a little bit outside, it becomes LTE. Sometimes even in cities, it kicks down to LTE. Then when you leave the city, it goes down to 3G, and then maybe even you'll end up on a 2G somewhere, and then nothing. That is exactly how AI is going to function at inference. It's going to work great in cities where there's exchange points, and everyone's blissfully ignorant and taking for granted that this infrastructure is there, and as soon as you start to graduate and move away from that, it declines. The service level and the speed and the access just simply drops off to the point where it's non-existent, and that's really not acceptable for banks, for healthcare, for insurance companies that are trying to run these applications everywhere, car companies. There's so many. It just simply doesn't work, and mostly, unfortunately, rural America. We're married to solve that, if you want to put a finer point on it. Absolutely. Tom, we know that this project in Wichita was made possible by a state grant, and Hunter just emphasized how important public sector support can be. How vital would you characterize the government investment in neutral infrastructure such as this in the way of Kansas setting a way, setting an example for the rest of the country? Any comments on that? Of course. Well, if you trace back to our very beginning, Matt, the reason Connected Nation, one of the big reasons that it got started as a nonprofit, it was clearly there's a missional bent to what we're trying to achieve, but if you go back to 2001, we had a dynamic, especially in the Commonwealth of Kentucky where we were born, where you had industry and you had government, and they needed to work together to solve some of the challenges that the Commonwealth was facing at the time. Well, for whatever reason, one was distrusting of the other, and there were challenges in getting them to work together, so Connected Nation, by organizing as kind of a safe harbor for the two to find each other and to begin to work in more what I would call not pure public-private partnerships, but a variant of that, where they were working in collaboration versus kind of an audit and check dynamic, that's really why we wanted to create. So to be metaphoric, we created that neutral venue, and this is almost the manifestation that is what Hunter is talking about with a physical structure now, and so it necessarily can find home in a government partner to help seed these things, particularly where the market dynamics have broken down and have not pulled this kind of independent investment into a community like Wichita. It's no coincidence that nationally, we are in the midst right now of the $42.5 billion BEAD program, the Broadband Equity Access and Deployment Grant program, part of the Infrastructure Investment and Jobs Act, of course, that is prioritizing, just like the Eisenhower Interstate System, just like REA before that, where infrastructure is key to so many national prioritizations right now, national priorities. The race to AI dominance, you won't achieve that goal, that end game, without widespread deployed kind of middle-mile infrastructure, as well as the last-mile infrastructure components there. So yes, very much, we are grateful for the, I should say, the foresight and the progressive thinking that Kansas has demonstrated in this. As BEAD gets off the ground again, it's been a little bit frosted now with the transition government in D.C., but as that begins to clear, the logjam and some of the whatever tweaks are going to be made to that program are revealed, and states can now move out with more confidence. I think you could see a rewriting of some of the rules that will probably even begin to point to, and especially, so timing is impeccable here with Kansas coming up, because now states are going to be able to look at this and say, hey, what do you, wait a minute, do we have the density that we need from a co-location middle-mile type of infrastructure? We can similarly invest some of these public dollars in ways, and that's a very important distinction, Matt. This is not just public investment for public investment. It is a catalyst. So please understand the economic development follow-on, which is with any good government dollar, if it's just, if it doesn't lever private investment, then those aren't as good as the ones that do. This one is that latter version. It is a public seeding, if you will, that will leverage and catalyze tremendous economic development. I would add to Hunter's earlier part when he talks about applications coming to these outposts. Guess what else doesn't come to those outposts, if this doesn't exist? Opportunity, economic development. From our perspective, this is quite literally an existential threat to the viability of some of these small and rural towns across America, which are gems of our nation, right? There are more of them than there are major metropolitan areas, and if the talent or the economic development or the opportunities start to only coalesce around these metropolitan areas, not only do we lose an incredible treasure in America, but we lose quite literally a transfer of wealth that is probably unfathomable to what we would be able to comprehend right now. So this quite literally is a sustainability play as much as anything else, because as Hunter says, latency is the new currency, and if you don't have it, you're not really participating in the digital world. Yeah. Yeah, Hunter. You know, when you were just talking about the real necessity of local peering for AI workloads and getting those connected, I wondered if you could comment more on that. And I also wanted to ask you about how this IXP represents more than just a fiber meet-me room. We know that it's a modular, storm-hardened facility design right across from Wichita State University, WSU's Innovation Campus. I just wanted to ask you about, again, about the necessity of local peering and the way this IXP facility has been future-proofed. Okay. So I'll break down to three things. The first is definitions. I mean, this is the, our industry, many others too, but certainly this industry is wrought with acronyms and words that are so overused and have so many definitions that they almost have no definition at all. Like the term data center. It doesn't mean, it means so many things, but it means nothing. You have to pre-qualify when you're talking to someone. What do you mean by that? You know, is it single tenant, multi-tenant, enterprise, hyperscale, blah, blah, blah, blah, blah, blah, blah. You can't just say data center and then continue on. There's a lot of different kinds. So again, I'm, historically in network interconnection, there's a term, carrier hotel, and it means something. It's not a hotel with beds. So we don't say that when we talk to people out in, you know, anywhere outside of the major cities. And then there's a room in that called a meet-me room. And that's where the room in the building where the fiber networks all extend into to get connected. These terms, they don't land exactly correctly in rural America. When you say meet-me room, carrier hotel, they think you're in the dating business and you're facilitating, you know, rooms where people could meet or something. So we refer to this as an internet exchange point, this meaning the real estate. So we'll talk about that, IX, internet exchange versus IXP, internet exchange point. So there are two schools of thought about that, and it depends on who you ask and what day of the week it is and where they're at. But an IX, internet exchange, is an ethernet switch. It's a switch that facilitates the VLANs between AS networks, so autonomous system networks. And an ASN is a number that basically represents a whole IP network, and all the major IP networks have it. You know, AT&T and Google and Netflix all have AS numbers. Most people don't know that. And that's how those networks communicate with each other, largely, depending on whether or not they connect to public internet exchange or called public peering switch fabrics. That would be the ethernet switch itself. The ethernet switch that resides inside of a building that's purpose built for the purpose of housing of the equipment, optical and electrical equipment, that interconnects and interfaces into it, predominantly, and routers. So DWDM, optical, ethernet switches, layer two, and then routers, layer three. That's a room or a building, which historically was referred to as an internet exchange point or IXP. So we look at the IXP as the room or the building, and that's what we're building. That's what we got the grant for. Anyone can go read the grants, the public document, and you can see what the money is for. Okay? So we're designing a building that has generators and UPSs and conduits and concrete and steel, and it's a building. It's not for simply a layer two ethernet switch. And then when people realize that, they go, oh, yeah, wow, $5 million actually isn't a lot of money. It's like the building should cost more than that, right? So there's this delineation between the switch and the room of the building. Let's talk about that first and then put those aside. So you're talking about future-proofing. What we're designing in a modular building sense is like the starter kit. It's like how do you go into a city that doesn't have anything and start? Do you build a million-square-foot building? No. Do you build a 100,000-square-foot building? No. We're building a modular building that's basically a couple thousand square feet, and it has all the attributes and key elements that exist presently in larger facilities elsewhere that would be the minimum required as a seed starter kit to plant and grow from. You've got a man trap and an area just for physical fiber panels for interconnection. We call it meet-me area. You've got a little conference room. You've got a power room, and you've got cabinets in the enclosed, hot-aisle, cold-aisle containment sort of thing. That is the... And a couple other attributes. That is the basics that you need, and we've shrunken that down to where we can penetrate a market cost-effectively. This is not a waste of funds. This is very needed, and it's the minimum increment to deploy that actually would be sensible. It isn't a closet or in the basement of a building. It's purpose-built. It's designed for this purpose. So from that aspect, the physical materials are all modern. It's hurricane-rated, et cetera. But it's future-proofed in the sense that I know the design of a physical neutral meet point from the manholes in the street all the way through the points of entry into where the fiber terminates, the patch panels, and where the active equipment goes. That's what I do. I understand it. I design it. These are my things. Then within that environment, a Layer 2 Ethernet switch will be co-located, and that switch is going to be operated by D-KICS, and D-KICS is the largest internet exchange operator by sites, countries, in the world. And they are very professional. They are the best at what they do. I can't say enough great things about them. Every major AS number in the world is connected to them wherever they are in the cities that they're in. Their reputation precedes them. They have 400 to 500 clients. They have carriers. They have ISPs. They have everything. And their technical prowess, if we added another entity to this marriage, got to talk about D-KICS because they're the heart and soul of the actual exchange point that is the exchange. So when you have an IX that's in a room that would otherwise be called a neutral co-location facility, or a data center, or a meet-me room, or any of those other terms, it becomes an IXP because the IX is there by default. Once there's an IX, that room can be called an IXP. D-KICS, as IXs go, is the standard in the world of who everyone knows, quality, performance, reliance, support, 24-7, multiple languages, you name it. So there you go. Now, that is what we're producing. We're bringing that entire circus to town, that structure, the design, the know-how, talking to all the fiber network operators, organizing them, coordinating them, helping them do their network planning and engineering, provisioning, so that they can bring their equipment in and terminate in light circuits. The enterprise customers can come in and connect, and they could use the cloud, and they could deploy AI. The whole thing happens, and it starts at this epicenter. Now let's talk about the application, sort of taking this in reverse order. So we talked about the difference between IX and IXP. We talked about D-KICS as the IX function within the IXP, thus making it an IXP. And now the applications. I'll give you one. There are many. But in the bank world, when I'm talking about consumer banking, but it's a very large bank, they have this one application that they're running on AI today. They're running on NVIDIA black coils, and it's called Fraud Detection. And Fraud Detection, they want to capture at the keystroke on the phone as it's occurring. In order to do that, they need sub three millisecond latency, which is referred to as RTD, another acronym, round trip delay, from the phone, the keystroke, as it's occurring, to the switch, to the router, to the server, to check if, is this, do you see this? Is this happening? You know, are you seeing this? Check, check, check, check, check. They're checking with other banks. And then they go back to the source and they determine as the keystrokes are being punched in, if this is fraudulent or not. And then they flip over to, from Fraud Detection to Fraud Protection, which is to stop fraud as it's occurring. They're literally saving billions of dollars a day, capturing fraud and stopping it before it occurs. How many people do you know, yourself, maybe personally, or any of the listeners, have been a victim of bank fraud, credit card fraud, somebody stole money, somebody ran a scam? Oh yeah, it's so common. It's everywhere. It's rampant. These guys, you got to ask yourself, and I do too sometimes, I mean, Jensen comes out with a new Nvidia chip every few months and they're more expensive than the last one. He's like, well, how can they be more, you know, where's the market for this? The market, one of them, is right there. The banks are saving billions of dollars, protecting us from fraud, from fraudulent transactions. Now, in order to do that effectively, they have to deploy this entire kit, this whole entire network rack, from transport to routers, switches, all the way up to the servers. And they need them to be located in places where they can access all the inbound access networks, eyeball networks, whether they're mobile, fixed line, from PCs and computers and whatnot. So they're actively seeking neutral meet points to deploy into. And that bodes very well for what we're doing, because Wichita and other cities like it, absent having a neutral place for them to deploy in, will necessarily keep them from being able to get to that three millisecond RTD that they need to capture the information as it's occurring. Fascinating stuff. And thank you for connecting those dots for us. It's been such a pleasure to have you both on, and we'll have to do some continuing coverage on this project at Data Center Frontier. But Tom, any closing words on the opportunity matrix that the IXP offers to Wichita and the Midwest? Oh, Tom, you're on mute there. Yeah. I just wanted to, again, express our gratitude and enthusiasm for what this represents, not only for our organizations, but also for Kansas, for Wichita, for Wichita State. We've got 125 of these type of locations that, through our collaborative efforts with Hunter and his research prowess, is diligenced across the U.S. And we love universities, post-secondary institutions, land-grant universities. There is another kind of synergy there. They need these things to be the epicenters of research, development, and innovation. And so that's why we'll be... More of these are coming. Matt? Thank you so much, Tom. And thanks to you, Hunter. As I said, we will be tracking this story. So thanks again, and we'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-26\n==================================================\n\n### Engineering a Cool Revolution: Shumates HDAC Design Tackles AI-Era Density\n**Episode ID:** 5087\n**Transcribed:** 2025-08-04 23:53:26.508682\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Datacenter Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Datacenter Frontier. And today we're joined by Darren Shumate and Steve Spinozola. Darren is the Managing Principal and Steve Spinozola is the Director of Shumate Engineering, who recently took the stage at Datacenter World to present a talk about a new approach to high-density cooling. Welcome into the podcast, both Darren and Steve. Thanks for joining us here. Thanks, Matt. Thanks for the invite. Yeah, we're thrilled to have you here. Of course, we're going to go on to talk about hybrid dry adiabatic cooling, or HDAC. That's your bold new approach to high-density data-centric cooling. But before we do that, Darren, maybe you could just build out for our listeners a bit about Shumate Engineering and your history in the datacenter industry. Well, I've been designing datacenters since 1999 when I was at EYP Mission Critical. And frankly, before that, I was at a large national engineering firm and one of our contracts was with IRS. And I was doing some investigative work at a datacenter in Detroit for the IRS back in like 1997. So I've been in and around the datacenter industry for quite a while. Shumate Engineering basically is the culmination of over three decades of my experience. And it was my COVID project, so to speak. In fact, we're right now celebrating our fifth anniversary. But it was a culmination of my experience and my desire to finally chart my own course and have my own firm. And we've actually had quite robust growth. And frankly, Matt, even since the last time you and I met, we've picked up some other very, very large datacenters. So Shumate Engineering is really my project to design a firm that is the firm that I always wanted to work for when I was a young professional, giving young engineers real responsibility and an opportunity to grow and to assume leadership at a very early point in their careers. And we're working very hard to make that happen. And in the meantime, soon after I started the firm, my old boss, Steve Spinazzola, from back in the RTKL days, called me and said, let's put the band back together. So now we've got what we think is a real datacenter juggernaut. Excellent. Tremendous. That is a great story. And so now on to hydride-dry adiabatic cooling, or HDAC. Your session at Data Central World laid out how, with HDAC, existing technologies can be sort of reimagined to support the rising demands of AI workloads. So I wanted to ask you both, what inspired the Shumate team to pursue hybrid-dry adiabatic cooling as a new approach to meet the demands of high-density AI workloads? Well, it all started November of 22, when I got an email blast that contained a link to HP Cray, Hewlett Packard dash Cray. And I was, if you're old enough like me to remember back in the late 90s, early 2000s, military and intelligence community datacenters used either Cray, SGI, or IBM mainframes, and they were all water-cooled. So I read the brochure from HP, and I think on page 15 out of 16, in little tiny print, it said, our design works with 32C water, which is 90F. So I said to myself, wow, 90F. I wonder if I can come up with a design that has one loop, that has two taps in it. One tap makes 90 degree water, and the other tap makes 68 degree water to make 75 degree air to cool legacy equipment. And that was it. I just wrote that statement down, and I said, well, let's solve it. And it was a lot of hurdles. Every time you think you have got something close, you'd have to say, oh, wait a minute, those chillers, I'm asking them to do something that we've never done before. Take 90 degree water in and spit 68 degree water out. That's not easy. But I did manage to get three manufacturers of chillers to be able to give me performance data for their machines. So that's kind of what inspired us, and that kind of put us down the road. And here we are, two years and six months later, talking about it now. Yeah, so that's fascinating. So your conception of employing HDAC sounds like it kind of coincided with the inception of Shoemate as a company. And is that accurate, Darren? Well, absolutely. And of course, the rest of the story is when Steve recruited me, which is over 20 years ago, to RTKL, we in the data center industry knew of Steve because he had invented what he was calling the Tower of Cool. I think he'll probably talk about that in a little bit. But he was an inventor, and I was fascinated by the whole concept of really leaning forward as a designer, having spent a lot of my career specifying products that exist or processes that already existed to go work for an engineer who was an inventor and a thinker. So it was kind of a natural fit for what we're trying to do at Shoemate. It also coincides with the literal huge growth in AI, and everybody's talking about AI and that driving data center demand, which was on top of an already robust demand in data centers. So two of the themes, and I think if you were walking around data center world in, frankly, any follow-on conference, there are two main themes right now in all these conferences, and that is, number one, how do we cool the high-density racks that are a result of the NVIDIA chip and similar chips? Number one, how do you cool those racks? And number two, where are we going to find power? And to power up data centers that are no longer 10 megawatts or 40 megawatts or 50 megawatts, not even 100 megawatts, we've just picked up a deal where we're going to be doing two 200-megawatt data centers. So when developers are developing data centers and the users, the big hyperscale users, they're buying at gigawatt, and the reality is right now there's not a corner on the grid that has a gigawatt just hanging out. So the rest of the picture is how we're going to be powering these data centers and this demand. So HDAC actually addresses both of those issues. Absolutely. So let's get back to brass tacks a bit on the technology. Your DCW Data Center World session mentioned how HDAC leverages currently available products in new ways. Can you walk us through, Steve, the architecture of HDAC and what makes it fundamentally different from legacy systems and how it compares to current air-cooled chilling technology? Yeah, right now, the flavor of the day, and I guess it's been the flavor of the day probably for since 2010, when the industry realized that AB power was not going to be the solution because it was too expensive. So what it became then was A, B, C, D, E, F, and then you would take any two of those to a rack so that each rack would get two power supplies. So instead of A, B, it's A, B, C, D, E, F, G, H, 7 to make 6, 8 to make 7, whatever works out for the size of the data center. So if you needed, let's say, 30 megawatts of ITU power, you would break that down into 10 three-megawatt generators with three-megawatt power supplies. And magically, if you do that, it's a 600-ton chiller. And that became the module for deployment today. Three-megawatt transformer, three-megawatt generator, and getting two megawatts to the IT and powering up the mechanical with the rest. And it just is an energy hog. And it uses up a lot of the connected power that could be going to use to power equipment. So you mentioned earlier that one of the compelling aspects of HDAC is the technology's ability to provide two chilled water temperatures in a single closed loop. Can you sort of connect the dots between that capability and advantages when trying to integrate direct liquid cooling and air cooling in the same data hall? Yeah. Just to go back to your earlier question, and I'll wrap this up into it of using current technology, basically the design that we're using uses a conventional chiller, conventional pumps, and what's relatively new to the market, which is called a hybrid adiabatic core. And what does a hybrid adiabatic core do? It is basically a dry core that has trim adiabatic as part of the design of the product. And the industry, the data center industry, loves innovation as long as somebody else does it first. And that's really one of the obstacles to deployment of this technology is everybody's waiting for somebody else to do it. So how does it work? You basically take your, and how does one loop satisfy that? And how do we make one loop satisfy? So the basic loop is taking supply from the hybrid adiabatic core, pump that to two loops that are all part of the same system. The one loop takes 90 degree water as a maximum and allow, and as low as 68 is coming off the hybrid core. The other tap off that discharge from the hybrid adiabatic core goes to either the fan wall directly, if we can make 68 degree water, or if we can't make 68 degree water off the hybrid cooler, and that water temperature creeps up from 69, 70, all the way up to 90. You're now taking that water, pumping it through the chiller. So you're taking water as high as 90 degrees Fahrenheit and making 68 degree water with the chiller, putting that into the fan wall, which is for the conventional air cool equipment. Return the water at 88 from the fan wall, pass it through the condenser bundle on the chiller, pick up the heat of compression, and take that water to the fan wall. So it's basically one loop with one tap for direct liquid cooling, and one tap for direct liquid cooling, and the other loop that goes through the chiller and picks up the fan wall. And both of those loops converge together and go back to the hybrid cooler. So it's one giant closed loop supplying direct liquid cooling for either cold plate deployment or a immersion system that also has a heat exchanger so that whatever fluid is being used to process and take the heat off of the servers, there's a plate and frame or a shell and tube heat exchanger so that the building loop never directly touches the loop that's cooling the IT equipment. And that's important because it now kind of puts the onus on the design team that's designing the direct liquid cooling component to be able to work together with the engineer that designed the building to make the system work properly. Understood. Now, Darren, so when you're talking with new customers these days, are you like, leading with the HDAC proposition, you know, and what's the implementation curve look like so far in builds that you're doing? So that's a great question. That's a great question. I'm going to kind of frame up, you know, what we're doing and how we're pitching this and, of course, you know, what kind of obstacles we're seeing in the industry. I think as you've heard us say before is  and we're both  Steve and I are both engineers. Engineers are conservative. We don't like to necessarily  a lot of engineers don't like to stick their necks out and design something new that may or may not have been proven and face that kind of an exposure. So that's like the obstacle that you see. We, for example, have presented this to several of the large developers, and we have interest by  and we're interested in the design by several of the large both data center developers, property owners, and the hyperscale users that like it. Others say, you know, we use these air-cooled chillers. An air-cooled chiller works in Phoenix. It's going to work in Dallas. It's going to work in Ashburn. And we've already got our, you know, our design, and we really don't feel like looking at anything else. So that is what we get. So we'll pitch the HDAC, and what I think that shows is that we're not just your average engineering firm. You know, we're really looking at the real thermodynamics and how best to cool. Because, as I alluded to before, this dual temperature loop does two things. It gives you an efficient way to get that 92 degree or 90 degree water to the direct chip cooling. It also does the air side, the 68 degree that Steve was talking about. But what Steve hasn't talked about yet is how efficient this is both from an overall water usage and overall power usage as manifested in a very, very low PUE. So maybe I'll let Steve talk about that a little bit. But once you start looking at that, and then I'll just give an example. We know of, and we're involved with one deal that is a gigawatt of IT load. So a gigawatt is 1,000 megawatts. That's pretty big. That's the IT load. The design for these buildings or these campuses are air-cooled chillers with a connected PUE of 1.5. So that means that these campuses need 1,500 megawatts at a 1.5 PUE because of the air-cooled chillers. If it were advantageous and there was water available, and if the water's available is always a big question, our PUE connected is about 1.2. 1.2 times 1,000 is 1,200. That's 300 megawatts of generation you don't have to build, or 300 megawatts of power you don't have to, or infrastructure you don't have to buy from the utility. 300 megawatts. I mean, to me, that's where this thing lies, and that's the future of this. Once we see that, and once we're able to get in front of the right people, I think they're going to buy in. The myth, there is a myth that, and I think Steve shared me an article this morning about Microsoft's completely waterless design, and I have to dive into it, but even if you're waterless at the site, you're using water at some point in the whole power cycle because you're buying power from the utility that's generating either by natural gas, nuclear, or coal, and those are all water intensive power generation methods. So, I think if we're wanting to be completely water responsible, this will prove to be the best system, and maybe I'm going to steal Steve's thunder. All we're asking for is a sip of water. It's not a lot of water, and Steve can talk about the bands up to 60 degrees, 60 to 80 et cetera, how we're sequencing the system. Yeah. Yeah, Steve, did you want to fill in a little on some of the water and energy efficiency gains you've modeled or observed with HDAC versus traditional air-cooled chilling strategies? So, basically, at the end of the day, there's only two ways to dissipate energy from a data center. Actually, there's three, but the third one is very site dependent, which is you're near a river and EPA is going to let you use their water to cool the facility, but sans that, you either have to blow air over a coil or evaporate water. That's it. There is no door number three in data center cooling. So, once you say that and you look at the product called a hybrid adiabatic cooler, it's basically a dry cooler with trim adiabatic. And the selection on those work with what's called an eight degree approach. Now, what's approach? Approach is the delta temperature between the ambient temperature and the discharge water temperature that you get off the coil. So, for example, if it's 60 degrees outside, the hybrid adiabatic cooler can make 68 degree water magically, which is what we need for air cooling, right? So, zero to 60 outside, you're in a condomizer mode and you're cooling the data center without mechanical compression. And that's what free cooling means is that you're still using pumps and fans, but you're not compressing a refrigerant as part of the cycle. From 60 to 68, you're still in a condomizer mode, but once you go above 60, you're now in trim chilling. So, if you can't make 68 degree water and it starts to cool you up to 70, 71, 72, you now do the trim chill for the air cooled component and take that water that's ranging from 7, 68, up to 90, and out of the chill it makes 68 degree water. So, you're making the 90 degree water at the highest for the direct to chip and you're making 68 degree water all the time for the conventional air cooling. And the other component is, okay, why do you need the adiabatic component? Well, if you remember an 8 degree approach, if it goes above 82 as the ambient temperature and the approach is 8, 82 plus 8 is 90, the highest temperature ambient I can make my 90 degree water is 82 degrees Fahrenheit or below in the ambient weather conditions. From 82 up to 90, you're now in adiabatic mode and you're evaporating water to pre-cool the air before it hits the closed-loop oil in the hybrid adiabatic cooler. And we can make 90 degree water just about anywhere that's as long as the design wet bulb is below 83. So, those are the kind of parameters that we're working. And from big picture standpoint, if you compare HDAC to conventional chiller with cooling tower and a water side economizer on that system, this system uses approximately 90 to 92% less water than a conventional chiller cooling tower approach. And the water utilization is around 0.07. And to put that in terms of water efficiency, a conventional cooling tower chiller approach has a WE of about 1.6. So, you can see there's a quantum leap in water use efficiency. And what Darren said is to remember is that it's highly likely that if you're, let's say in Virginia, you're getting a mix of power from nuclear, from solar, from gas power generation. And believe it or not, there's still some coal out there. All of those systems use water, right? And if you're not, if you're, the pitch on air-cooled chillers is it's waterless. Well, that's just not true. You know, you factor in the water used at the power plant with an air-cooled chiller design versus what we've got, and we're using half the water for the mechanical cooling with HDAC at the power plant. So, even if we use that water when it's above 82 Fahrenheit in Ashburn, Virginia, that's only 780 hours out of the year. So, you're only evaporating water about 9% of the year. So, it just, it saves a lot of water, but what it does is it buys you half the energy consumption of conventional or air-cooled chillers. And there's something that I call collateral benefit. So, Darren, do you want to explain what a collateral benefit is on this design? Well, the collateral benefit is, I mean, this, the brainstorm for this design came from how do we get, how do we, with one loop, provide cooling water to the 90-degree loads, the direct-to-chip cooling, and then as well as the legacy air-cooled equipment. So, that was kind of, that was the genesis of this design. The collateral benefit is the fact that your annualized PUE is less than 1.1. In fact, the more you go direct-to-chip, the lower the PUE is. And I call this a no-BS PUE. This is considering every piece of the mechanical equipment that's in there running, you know, at its peak. Annualized PUE is 1.05-ish or 1.06 for the direct-to-chip and maybe 1.09 for if it's mostly airside. And the connected PUE is 1.2. So, the collateral benefit is you don't have to buy the infrastructure or build the infrastructure for the cooling aspect of this. And that's just from a big picture. If you're a developer, if you're building a one-off building and you have to buy, and I'll use a great example because this is one that we're designing, where you're buying 36 megawatts from the utility. That's what you've got available. Traditionally, you would be able to just find in your Proforma 24 megawatts of IT load, which is your revenue. With our design, you can get close to 28. So, that's three and a half to four megawatts more revenue on that investment. Plug in the numbers of the cost of money. That's why you see a lot of private equity getting into data centers because it's big money and big, big demand. So, the collateral benefit is with not much more investment in equipment, if any more investment in equipment, and with a lifecycle cost that's actually less, it adds instant value to your real estate. So, collateral benefit is buy less power, invest in less infrastructure, and your building is worth more. Yeah. So, we're getting down to our last, excuse me, a couple of minutes here in the interview, but it really sounds like the play with HDAC is equally leveraged for retrofit potential or also probably suitable for greenfield builds. Is that accurate? Yes, it is. There are buildings out there that have no chilled water in them. They use what's called direct adiabatic, which is basically you bring in air that's water, air side economizer. So, you've got to maintain 68 degrees. What are they targeting? 75 degree supply air. And if it goes above 75, then they use an adiabatic pad to pull the air back down to 75. Thank you. Can you repeat that question? Well, it was just a question about retrofit versus greenfield. Yeah, you can retrofit with an existing system that has chilled water, but you're not going to gain the benefit of being able to do free pooling for the directed chip in the future. I got you. Yep. Well, this has been, I want to thank both of you, Steve and Darren, for joining us here today on the podcast. And it's such a new idea that it almost sounds like something that would bear checking back into. So, we'd love to circle back with you down the road here to do some continuing coverage on hybrid dry adiabatic cooling as produced by Shoemate Engineering. So, thanks again, both of you for joining us. And we'll see you next time on the Data Center Frontier Show podcast. Take care.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-24\n==================================================\n\n### Safe, Scalable, Sustainable: Enabling AIs Future with Two-Phase Direct-to-Chip Liquid Cooling\n**Episode ID:** 5088\n**Transcribed:** 2025-08-04 23:53:26.508724\n\n**TRANSCRIPT:**\nHello and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier, and today we are here with Dr. Rich Bonner. He is Chief Technology Officer of Ascelsius. Rich is a distinguished heat transfer researcher with 18 years in thermal product development, specializing in two-phase cooling, and he's authored over 50 papers and holds five U.S. patents, having designed cooling products for over 125 clients in various sectors. And today with Rich, we're going to be discussing the ways in which two-phase direct-to-chip liquid cooling, as embodied by Ascelsius NuCool, is prepared to enable the future of data center workloads for AI and HPC, both safely and sustainably. So that's a big mouthful from me all to just get to the point of saying hi, Rich, and welcome to the podcast. Thank you. Appreciate the invite. Yeah, we're thrilled to have you here. It's good to catch up with Ascelsius. So let's dive right into these questions I have for you. I wanted to ask you, how will a universal cold plate ease the transition to two-phase direct-to-chip for data center operations? Sure. Well, there's a challenge right now in that single-phase cooling, usually with 25% polyethylene glycol mixtures, is kind of becoming an industry standard for most servers that are being released today and probably even in the next 12 months. And because of the duration of time it takes to acquire servers and go through the decision-making process for your thermal solution, a lot of those decisions are being made now for what's going to happen in the future. A universal cold plate would allow those decision-makers to sort of delay the choice of cooling solution until they get closer to deployment. So what we're envisioning is having a cold plate that gets designed into a server that can use either two-phase cooling or single-phase cooling, such that when you get closer to deployment, then you can make the decision, do I go with the two-phase CDU or do I go with the single-phase CDU? So really, the importance of that universal cold plate, in my mind, is getting us to the point where we don't have to make decisions so far up in the front end that we might regret later on, because as we get closer, we may want that performance that two-phase offers. And right now, it's sometimes a challenge due to that decision-making process. Yeah, that is a fascinating window into a strategic type of design. So let me just ask you specifically, how does thermal performance differ in single-phase versus two-phase when using that universal cold plate? Well, this is what's interesting, Matt. We did a study with Intel. Actually, Intel led the study, and we contributed to the study. They took a single-phase cold plate that they had optimized for one of their emerging GPUs that they were developing, and then they tested that same exact single-phase cold plate, single-phase optimized cold plate, with one of their two-phase working fluids. And then they asked us to do a second two-phase working fluid. And what we found in that study is that even though the cold plate was not optimized for either of those two-phase cooling working fluids, both of those fluids tested better in the single-phase optimized cold plate. So what we're seeing is that, basically, I guess what that proves is that the universal cold plate concept has some legs, because you can design it for single-phase first, and you will still get more performance out of two-phase, again, going back to that decision-making process that I talked about before. If at the end of the day, you get to your data center, maybe the cooling water available is too warm to allow for single-phase, well, now choose a two-phase CDU and use that warmer facility water that you have available. It sounds like a really flexible sort of approach to designing the data center for AI and HPC. So to that point, I wanted to ask you, given NVIDIA's recent announcement of 600-kilowatt-plus racks, how is your team prepared to enable these projected increases in rack densities and power demands in the data center over the next three to five years? So we saw a number of things come out of that NVIDIA announcement. One is that you're talking about is that high power density. So I'll address that first. If I go back 12 months ago, we released our first CDU for two-phase cooling at the 80-kilowatt power level. Now we're shortly here releasing and actually already deploying 250-kilowatt CDUs. That's a threefold increase in about 12 months. For us, to get to those power levels, it's primarily an engineering challenge related to sizing. So it's sizing up larger pumps, larger heat exchangers, so that you can remove that extra heat. So along that line, you know, tripling in 12 months. I think if we triple in the next, say, 12 months, we'll be hitting that 600-kilowatt level well in advance of what NVIDIA is coming out with. And there's other things, too. There's other challenges related to the orientation of the servers and other things, and we're working on those as well. Thanks for connecting those dots for us. So just to zoom out a little, how would you say that the Celsius New Cool cooling technology is enabling, you know, big picture, the cutting edge of AI, particularly as the technology largely switches from LLMs to inference models, as is anticipated in the market? Yeah, I mean, AI is a very broad term, and I think a lot of the first things you have to do if you're dealing with some of the main AI applications is do training. I think a lot of the capital investment that's going into AI right now is on those training-related data centers, and they have particular nuances in terms of the types of racks, how close they need the servers, the power densities that are optimal for those training exercises. But certainly, as you move towards inferencing, it's still kind of the wild, wild west. I'm not sure anyone knows exactly what inferencing means, but certainly there's going to be a desire to bring that inferencing closer to where the inferencing is happening. And when you do that, it's probably not going to be the same 100 megawatt, gigawatt-type data centers that maybe you're using for training. Maybe it's smaller, more modular-type data centers. And maybe there's unique requirements even for on-prem and other things. So one of the products I'm excited about for those, I'll say, smaller, more closely located computing efforts is liquid-to-air with one of our refrigerants. We actually call it refrigerant-to-air because there is no water when we do it. But I'm excited about that because the two-phase part of it, the refrigerant-based part, allows us to cool those still very high-power GPUs that are going to be used in those inferencing applications. But by doing refrigerant-to-air, we get away from all of the fluid conditioning and maintenance requirements that you typically are going to have with PG25. Maybe you can handle that in a gigawatt data center because you have the technicians and the support there to handle all of that maintenance effort. But if you want to go on-prem or have smaller data centers, you really can't afford to do that maintenance effort. So I'm pretty excited about that. Yeah, yeah, it's again, it just seems like a very agile type of approach. So, well, let's reel it back into sort of the white space. How is the SLCS solution prepared to handle upcoming changes in the standard server's form factor, especially regarding shifts in cold plate orientation? Sure, yeah, and I think I referenced that before regarding that NVIDIA announcement. I mean, they talked about higher power, but one of the other things that just jumped off the screen was the orientation of the servers. Instead of being horizontal, we think about the pizza boxes that get stacked up in a typical rack. They sort of have three or four rows of now vertically oriented cold plates. So it's a new architecture. There's certainly going to be challenges regarding the different kinds of manifolds and how you route fluid within the rack to these different locations. For two phase in particular, one of the science kind of questions we always have to deal with is that due to phase separation, because liquid is always going to be more dense than vapor, and when you're flowing horizontally, it's more clear where that liquid is going to be. It's probably going to be up against the chip, which is where you need it to be to boil. If you go vertical, you're not so sure. Right. How is that vapor and liquid going to separate? If the vapor is up against the heat source and the liquid is on maybe the other side of the cold plate, you could potentially have performance challenges. What we actually did to get more comfortable with these vertical oriented cold plates was run a test, took our current cold plates, simply tested them at different orientations, including vertical. And we presented that at the OCP meeting in Dublin just a month or two ago. To spoil it, basically, they performed very well. We saw the same performance in those different orientations that we see when testing horizontally. And that was even for a cold plate that really wasn't optimized for vertical orientation. So we think there's probably other things we can do. So it's a concern. It was a concern, but it's something we've tested. I think we're going to be ready for it when that day comes. Understood. I love a great engineering discussion on the podcast and you're giving us one here today. So I have two questions left for you. One is pretty specific and the other is another big picture sort of question. So I'll start with this specific one. Rich, I wanted to ask you, how does the TCO of new cool, how does that total cost of ownership compare to a standard single phase directed chip solution? And as a follow on the meat of the question, what are some lesser known variables that data center leaders should consider before choosing their liquid cooling solution? Sure. So the TCO analysis that we've done shows that two phase cooling compares very favorably to single phase cooling. A lot of the benefits come from not only the slight increase in performance that you get, which ultimately leads to chillers not having to operate as often, not having to use as much water and other electricity, other resources. But a lot of it, too, is just the issue with leaks and the damage to hardware. For instance, if you have a leak with the two phase system, you're not going to damage the hardware. And when you're talking about even a small percentage of leaks on three or four million dollar racks, that cost adds up. So when you look at the energy efficiency plus the minimal risk exposure due to leaks for our system, the total TCO cost looks quite favorable. You talked about other unique things. I think the second part of your question, I'm I think one of the unique things to understand is the maintenance cost itself. I mean, these fluids, they have to go through treatment. There's biocides, algaecides, corrosion, filtering. Those costs tend to add up that the hassle of dealing with that maintenance tends adds up through personnel and other things. Refrigerants that we use are actually the same refrigerants and have the same maintenance as you would expect from your HVAC system. We've all have HVAC, most of us have HVAC systems in our homes. Most of the companies, the offices we have have those. You don't have to do anything with the fluid. As long as the system is made and it's leak tight, it'll last until it rusts. Right. Which can take tens of years when it's outside. Our system in a highly coddled data center environment, we expect to last a long time with near zero maintenance. And on top of that, we've designed a product with some of our experts from the server side that's very easy to maintain. For instance, even if one of our pressure transducers or pumps needs to be replaced, it's all on quick disconnects. We don't need even an HVAC certified person to come and maintain those things. So in the event there is a part that fails, the maintenance is quite simple and done with low skilled hands. Got it. Thanks for that answer and for explaining that to us. So we're now at my final question. And I just wanted to ask, how can effective liquid cooling in the big picture enable data centers to be a good neighbor, so to speak, to their local communities? So how does liquid cooling. Oh, OK. So in terms of impact to local communities, how does liquid cooling compare relative to air cooling, I suppose, or to other. Is there a line that can be drawn, you know, from effective liquid cooling, you know, to data centers being better neighbors in the long run. Oh, absolutely. I mean, it starts with the noise. I mean, if you ever were to walk past a liquid cooling system operating, you would, if you're used to air cooling, you'd probably have to touch it, make sure that it was even on. I mean, the noise that comes off of a liquid pump compared to, you know, hundreds, dozens of screaming fans. You've seen some of those studies where some of the data centers have this hum that can bother neighbors. You won't have that with a liquid cooled data center. The other part, I think, is the and we see this when data centers are going even for permits and other things, the amount of electricity that's used. Also, the tend to use a lot of water for cooling towers and other things that can be noticeable to the communities around in terms of taking an unfair share of the resources that are available to the community. With liquid cooling, because you have a higher performance, you can get away with less or no water usage. You can your overall energy efficiency is higher, so you don't need as much electricity. So you'll see that impact on the community, too, at a resource level. Thanks, Rich. Well, this has been a really valuable discussion, and I want to thank you for coming here to have it with us. Like I said, it's always great to catch up with a Celsius. So I just want to thank you for a great interview. Thanks, Matt. Great questions. Always welcome to come back and appreciate the opportunity, like always. Yeah, we'll definitely be following up. So thanks again, and we'll see you next time on the Data Center Frontier Show podcast. Thanks, Matt.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-19\n==================================================\n\n### Why MOOG is focused on Liquid Cooling and Motion Control for Data Centers\n**Episode ID:** 5089\n**Transcribed:** 2025-08-04 23:53:26.508764\n\n**TRANSCRIPT:**\nHello and welcome to the Datacenter Frontier show. I'm Matt Vincent, Editor-in-Chief of Datacenter Frontier. And today we're bringing you a special edition of the podcast, repurposed from one of our signature quick chats. In this episode, I sit down with Ray Walsh, Program Director of Datacenter Cooling at Moog to discuss why this precision engineering company, best known for its work in aerospace and industrial motion control, is now bringing its expertise to the world of datacenters. Over the course of our conversation, Ray offers a behind the scenes look at how Moog is applying intelligent motion systems and advanced liquid cooling to meet the performance and sustainability demands of today's compute-intensive infrastructure. We also touch on Moog's new cavitation-detecting ultrasonic sensor, purpose-built design philosophy, and how the company's history in mission-critical environments uniquely positions it to innovate in the AI-powered datacenter era. So let's jump right into the conversation. Hello, everyone, and welcome into another Datacenter Frontier quick chat. I'm Matt Vincent, Editor-in-Chief with Datacenter Frontier. And today we are here with Ray Walsh. Ray is Program Director of Datacenter Cooling for Moog. Welcome, Ray. Great to have you here. Great to be here, Matt. Thanks for the invitation. Absolutely. So for anyone in our audience who might not know, can you tell us where did Moog originate as a company, and how has its expertise in other industries influenced its capabilities? Sure. So just one thing, it's Moog when we're across on this side of the pond. It's Moog when we're in Europe, but anyway. So a bit of background. So Moog is a, at its core, it's a fluid control company. It has its origins in aerospace and space applications. Founded in the early 50s by a gentleman called Bill Moog. He invented a revolutionary product, which today we call the servo valve. And what that gave industry was an ability to precisely control fluids. In his case, he was focused on hydraulic fluids that go into aircraft, missile fin control, and so on. So that really is the origin of the company. Over the last, I guess, 75 or thereabout years now, we have progressed through a variety of different industries, aerospace, space, defense, and industrial, and medical. So just kind of an anecdote that I use is that when you're flying, if you look out the window of an aircraft and you see any of the surfaces moving, that's Moog hardware that's moving them on every modern aircraft, be it a Boeing or an Airbus or an Embraer. That's a great data point there. And I can see how all that experience in aerospace and defense can translate to mission critical reliability in data centers. But so given Moog's legacy in designing and manufacturing motion control products, what drove the decision to enter the data center market? So I think there was two factors. One thing that's a little bit unusual about Moog is that we're very much a collaborative organization with our customers. So if you look along, we work on a very strong principle of customer intimacy. More so than the other two pillars, let's say, of operational excellence or technology leadership. So we pride ourselves in engaging with customers. Couple that to our history in fluid control and motion control. And we found ourselves in a scenario about a decade ago where we had a pioneering company in the realm of data center and equipment provision who were looking to move from air cooling. And we had been providing them some air cooling products historically to fluid controls. And it was very much a collaborative activity that started us down this road. So we had all of the technology heritage. We had a customer who was trying to break new ground. And those are really the two things where we find our natural point for thriving. So really, that led to the development of a series of different pumps for that customer. And that really is the origin of where this came from. That's fascinating. And it seems like an apropos type of entry into the industry. So let's drill down. I wanted to ask you, how do intelligent cooling controls and maintenance services, in your view, contribute to overall data center sustainability? Okay. So I think there's a couple of facets to that question. So maybe just let's unpack it. In terms of intelligent control, at the end of the day, if you have a product and our drives that are server drives that ultimately cause the pump to run, they have electronics baked into them that look at various key characteristics of the operation of the pump. That could be the amount of power it's consuming, the temperature it's running at, the pressure it's seeing, and so on. So those should all operate within certain predefined confines. The fact that we're able to monitor, observe, and communicate that back to a data center operator allows us to flag early if something is going wrong. So it gives you that ability to provide some prognostic data into the management of the data center itself, where you can intervene before something has truly become a problem. On the other side of it then, in terms of maintenance and sustainability, so our pumps are, and this is maybe something that's a little bit unique in the industry, they started out life as data center pumps from the very beginning. That was their origin. So we have designed them with that unique or that specific set of requirements in mind from the get-go. And I think that does separate us a little bit from the industry as a whole, where there are lots of very fine products on the market, but they are products that had a generic product evolution. So as a result, we've been able to optimize size, we've been able to optimize performance, we're able to look at what are the key things that data centers care about. It's uptime, it's lack of maintenance, and we've been able to bake that into our product from the beginning. A very simple thing is our pumps have no rotating seals, and they run on what we call a pseudo-fluid bearing. So theoretically, there are actually no wear points in our pump, provided it's running within its prescribed characteristics. So there's no schedule maintenance. There's not a case of, I have to pull out of the wall after X number of hours, change a seal, change a bearing, whatever the heck it is. That doesn't exist for us. So once they go out, and as long as they're running within parameters, they theoretically run forever. So all that means, there's less cost of ownership, they're optimized, and one thing I didn't mention is they actually use a lot less energy as well. So from a PUE point of view, when you're thinking about the overhead associated with running a data center outside of the energy that's consumed in the computing activity, our pumps actually reduce that requirement. So we're moving more fluid per watt than pretty much all of our competitors, largely because it is that design for purpose product. Got it. Well, that's all really interesting to think about in terms of differentiators for the Moog system. So can you share any recent advancements or breakthroughs in motion control and cooling systems for data centers that you might have heard about? I'll actually maybe pivot a little bit. So we've just brought to market a new sensing product, which is a little bit adjacent. It comes from work we've been doing actually in the medical industry for years in terms of ultrasonics. So we have an ultrasonic sensor that we can now deploy into data centers to measure cavitation or air bubbles in the fluid in the end. So that to me is another one of those things. That's a very hard thing to detect normally. We've used an adjacent set of skills, a bit like our uptime, our reliability, our mission critical capabilities to send something that a data center cares about. So if you've got cavitation going on in your fluid, it's usually a sign that something is going wrong. Again, it's an early warning into the system. In terms of the product itself and the pump that we have, maybe this is a bit of a conceit, but I actually think we're 10 years ahead of everybody else because we come from that root condition of, it's for this market. So again, we strived to make sure that our products are fit for purpose and are also designed for the requirements that the data center operators or the hyperscalers really care about. So it's not a, maybe an analogy, it's not a, a lot of our competitor products are standard industrial pumps that have been modified, but they're designed to live in an industrial environment, in a factory, whereas whatever, whatever the hell it is. Whereas within a data center, it's quite a different and it's quite a controlled environment. You've got UPSs, you've backup generators, you've quite a different infrastructure that your equipment is sitting on. So our pumps and drives support that from the get go. I think that gives us an advantage. That's actually an, I think that's an advancement that everybody else will be looking to achieve as data centers, particularly liquid cooling data centers become more prevalent and become the industry norm. But I think maybe we just made the advancement a little bit earlier. Understood. So next kind of a people-oriented question, what are key qualities and skillsets define the Moog team behind its expert reputation as you've laid out for us here? Okay. So I think there's, there's a couple of bits. One is there's obviously all the hard skills, you know, electronics, software, compute, you know, fluid dynamics, management, all that good. But you know, that's, lots of people have that. We're not the only ones who do this stuff. I think that it's really the DNA of the company that sets us apart. So we have a, we have a tagline. If you go on our website, moog.com, shaping the way our world moves. And that really is about focusing our core value is on making motion control more sustainable, you know, doing it the best way that it can be done. The other tagline we have is for performance really matters. So again, those two things are just how we exist as a company. So therefore we, we, we spend all of our time thinking about doing hard problems. Usually when people show up at our door, it's not because they've got a catalog they want to compare our products to. It's they haven't been able to find anybody to provide a product that really meets the requirements that they have. So they'll show up and we, and again, I go back to the collaborative customer intimacy model that we operate upon. It's that, it's that really hard problem mindset plus working with customers that really are the element of the people piece that differentiates us from, you know, more standard product providing companies. So it's more of a culture thing, I think more than it is just hard skills because, you know, there are very talented engineers and lots and technicians in lots of companies. Yeah. Yeah. Very good. Thank, thank you for, for that perspective. So now we're, we're at the bottom of our questions. This is a, a bigger picture question maybe to end on how do you see the data center industry evolving in the coming years at Moog and what role does the company aim to play in this transformation? Okay. So I think that the big change or the big pivot that's going on is the inevitable migration of data centers from air cooling to liquid cooling. Yeah. That that's just a mega trend. It's going to continue. We're early in the process. You know, some of the Colos haven't gone there yet, but all the hyperscalers are going there. So we will see liquid cooling becoming the de facto standard as the chip power densities increase this, the racks, the, the servers, everything, the power density is just going up. So you have to have liquid cooling. So that's the, the big shift that's coming at us. From a Moog perspective, I think, obviously we have our pumps. We continue to evolve that product line. And, you know, I would like to think that we will be a premier supplier of those pumps into data center equipment manufacturers as time goes on. But I think the other piece is part of our evolution typically as a company, as we start with, okay, we've made a product that's really good. And then we look to say, well, what else can we do? So if I go back to our aerospace heritage, we started providing valves to Boeing Airbus today, and this was a long-term strategy. We provide all of the flight controls from the stick to the tail on a Boeing 787. So it's all Moog hardware. And I see exactly the same thing happening here. We will continue to evolve our capability. We will look at the problems that data center operators, CDU providers face, and we will use our systems capability and knowledge to not just provide a pump, but provide an intelligent pump or a subsystem, or even maybe a partial system that solves problems that if you just bolt stuff together, you really don't get the same result. So it is really coming up from that product to a subsystem, to a system provider over time. I think that's how our company has always operated. I see no reason why it won't do the same this time around. Got it. And very good. Well, thanks so much to you, Ray, and to Moog for joining us here today for this really pretty enlightening discussion about your company. No bother at all, Matt. Thanks very much for your time. We appreciate it and wish you a great rest of your day. Thank you so much. We'll see you next time here at Data Center at Frontier for the next DCF Quick Chat. Have a great day.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-17\n==================================================\n\n### Leading with People, Process, and Performance in Digital Transformation\n**Episode ID:** 5090\n**Transcribed:** 2025-08-04 23:53:26.508801\n\n**TRANSCRIPT:**\nHello and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier. And today we are here with Jenny Zahn. She is the newly appointed Chief Transformation Officer at EdgeConnects. Hi, Jenny. Thanks for joining us here today. Hey, Matt. Very nice to meet you. Yeah, great to meet you too. So let's dive right into the questions here. What has been the most significant adjustment for you in transitioning from your previous role as Chief Accounting Officer to Chief Transformation Officer at EdgeConnects? Funny you ask. It's quite a journey. A lot of exciting things to share. I guess I would just start by with any transition, you know, there's always something inspiring but also trepidation to look to. For me, having been here seven and a half years, knowing the people, the culture, and I have the leadership support and the organization's capacity to know that we need to make the change at a massive scale, it was a big plus. So that at least laid the fundamental environmental foundation to start for me. So not too hard. However, moving into CTO transformation, there's really not a well laid out roadmap as to what it entails. So it does take forming a vision to what we are transforming ourselves to at the enterprise level. So managing towards the business objectives that are very clear to all of us based on our current and shifting business risks are, you know, at the end of the day, a very clear foundation as to how I look at this journey. I think the other thing when you look at it from a business perspective, what's important for me, it's always to hold a big picture view, the intellectual ability to align leadership on the enterprise goals and the capacity of myself to be able to dive down to 3,000 feet if I need to, to understand what's out there is always very critical from my standpoint, reflecting on the journey. I think overall, it's challenging, but really, it's so exciting. It does open a very wide playfield for me to design and execute a pathway, you know, based on the understanding of the business and the critical operational areas that requires attention and inputs and dependencies. So having all of these, I think it really forms the prime condition for me to go into that role and leading the journey with all the support from the organization. Absolutely. It sounds like a fascinating role. I wanted to ask you, how has your background in finance and accounting shaped your approach to leading digital transformation efforts for EdgeConnects? Really good question. I think at the end of the day, it really connects me to the whys and hows. You know, when you look at corporate transactions activities, for some reason, most of them, right, somehow translate, always translate and lend into the financial impact. So I think having been in that part of the business world really trains in me a very logical way of thinking in categorizing, you know, what seems to be data clutter or noises really into priorities and proper sequencing. At the end of the day, you know, when you think about transformation, it's really change of scale. There's so much to do, so little time. So I think it's really the logical way of analyzing and putting things into the right buckets, connecting the whys, the hows to really get the pathway to where we need to be. I think it also naturally leads a way of looking at the issues, the dependencies, the inputs all in the way of translating into solutioning as to what the problem requires, what the business challenges really drive. And these data-driven investigatory exercise and decision-making, I think, ultimately really help me make a difference as to how I lead the pathway. Excellent. So, Jenny, in notes for this interview, you emphasized the importance of people, process, and performance in driving success for organizations. Can you elaborate on how these principles guide your current initiatives with EdgeConnects? Well, thank you for asking the question. Again, like I said earlier, it's massive just in terms of how we need to scale as a business to support the growth, not just today, but really future state. I think, you know, for me, it translates into basic principles as to how you tackle the various things that require us to scale to a different level. At the foundation, it has to be people. But I also need to really make sure the message is simplistic and understandable readily by, you know, a very diverse but very smart team out there. I think, you know, when I think about people, a lot of times people also naturally translate into culture. But really, in my mind, it's culture based on people that lead to organizational effectiveness. It's always easier said than done. You know, the kind of behaviors, ultimately, when I want to drive that forms the foundation for us to be able to execute the other two P's, the process and performance. I think fundamentally, it's dictated by a set of behaviors, right? So, you know, it's how do we want the organization to effectively demonstrate, but also cultivate the kind of behaviors that form into the big looming sort of foundation as to how we do things. It goes so much beyond the structural sort of conventional way of how we think about human resources, but, you know, manifests more in the way of how we conduct our day to day interaction, working together, collaborating to solve problems. You know, do we have the right environment that our people can feel comfortable to voice disagreements? Do we have a forum to really allow folks to productively reconcile differences? You know, manager and staff, is there a productive relationship to really help inspire new ideas generation and find the solutions? Really all of these and translate also in a way to, you know, how do we hold each other accountable with compassion? So we genuinely understand what are the kind of hurdles we have in our organizational environments. So all of these are the kind of, you know, data points in my mind as to why we need to drive to a friendly environment, but encouraging productive environment to generate ideas and fundamentally really stimulate to a productive process and, you know, productive outcome. So all of these are boiling down to the people side of things, which is why it's so critical. You know, you can put, depend on the circumstances, put different weight to the people, the process enabled by system and technology and performance. But I think almost universally people is always the big part of this equation and calculus in terms of problem solving. For sure. Can you talk a little bit more about how the focus on people, the role that plays in enabling EdgeConnects to scale globally? Well, we are in a world of shifting challenges. You know, I guess really just when I think about ideas, ultimately it's about the business innovation and ingenuity that come from people. I think this is something that maybe Steve Jobs popularized with it, right? Technology and good process. It's the bicycle for the mind. When you unclutter the data noises and the distractions in not having a good process, well-defined set of tools, I think that really can release people's ability to generate ideas. I mean, when you look at evolution of corporate business model, how it has sort of succumbed, well, really surpassed the circumstances and hurdles historically, it's always, always about how we clear the clutter and really give people the environment to generate ideas. And this is something that has been manifested by a business strategist, Gary Hamel, somebody's book work I've been following a lot. The survey, there are some stats out there, 75% of the time, our people are actually spending data compilation spreadsheet. So these are the kind of clutter that we wanted to transform ourselves to have better technology, better process, to drive our people to gain insight from ideas and the free environment to exchange and remove the inefficiency and waste of what process we have. So all of these points down to why I need to translate into something that's easily understandable by our workforce to drive to the kind of goals we as organization need to accomplish. So I wanted to ask you about data. Data we know plays a pivotal role in your organization, both internally and externally. How do you leverage finance-driven insights to maximize the value of data in digital transformation efforts? Good question. My daughter is actually taking, she's a college freshman. So she's actually taking accounting 101 as a summer class. I used to tell every new recruit to the department to know accounting is the basic ground principle. It's going to help propel how we think about business issues exponentially. When I think about accounting, really translate into the financial sort of data analytics in a really driving insight from business operations, introduce such a logical way of just sifting through voluminous activities into themes and patterns. I think really no matter which part of operations we are in, cutting through noises and clutter to glean the pattern, it's always a fundamental core to what business solution making is about. In undertaking transformation projects, really not all priorities directly are driven off of financial terms. When I think about operations, for example, how do we evaluate our data center maintenance data to see whether there's actually a pattern that we can use to allow us to focus on the issues more poignantly as opposed to sifting through hundreds of thousands of ticketing. These are the kind of examples as to how we look into to really use the kind of logical thinking and trying to get to data insight a lot more efficiently. Fundamentally, transformation really in my mind starts with value chain, value creation activities. With all of our operating pillars where the outcome and the performance are so critical to how we gauge business success and achieve our business objectives, they require the same kind of data-driven exercise, no matter what angle we approach to it. That's fundamentally how I look at the finance side of the logical thinking analysis would be so important to how we conduct our transformation activities. Got it. Thanks for that. My next question is, it's possible if a company is successful today, that might not guarantee its success tomorrow. How do you identify and implement technologies or strategies that will keep your company, EdgeConnects, ahead of the market? Really, really good question. One of the primary goals in driving changes at scale around our people, process, and tools is to place agility in our company's DNA. When you think about the future, even really looking back in the past five months of the year, you look at DeepSeek came out in January and introduced such a big shock to our business. When you look at the activities and investment decisions, Liberation Day, that was only a month or so ago, just threw such swings into the market. These things like this, they constantly introduce shocks to the market. The business we are in, we definitely feel from macro and micro level, the kind of rippling impact from these kind of market events, the geopolitical power balancing, all of that. We intimately feel their impact to our day-to-day business. When you look at all these noises that may be representative of not just the world, the business of today, but really to the future, it's really, again, the agility that we need to get ourself into, I think, ultimately is one of the most important ingredients to make us successful. I, in a way, when I look at the noises out there in the market, I kind of buy in the theory, I'm not sure you've heard, chaos theory. No matter how much randomness there is in the market, there's always some kind of pattern that would be the winning card. For me, it's about training in our DNA, the ability to flex, to change, and to adapt. There's actually another book, you just reminded me, Anti-Fragile, it really talks about how do we cultivate in an organization the ability to react quickly to shocks in the market. To accomplish all these and to really get back to your question, how do we future-proof ourselves, I think the foundation lays with the tools and technology we need to have in place and that enable our people to be released from dealing with the day-to-day clutter and the noises. The other thing, as I think about tomorrow, we all hear AI day in, day out these days, so that's something that we have actually started building in our organization to start training our organization at scale about the kind of AI tools out there beyond what we use day-to-day, the basic, the convenience, really translate and automate a lot of the manual process we have. All of those are things that we need to constantly stay vigilant, in my mind, to equip our enterprise with the right tools to ultimately get to that kind of future state of agility to react to sudden changes in the market. Right. So, kind of as a follow-up question, touching on the incredible pace of growth and change in the data center industry, much of it driven by this AI moment, I wanted to ask, what strategies are you implementing to ensure that EdgeConnects remains agile yet grounded? You're just hitting all the right notes as to how I'm thinking about this transformation. I think we touched upon a few of those earlier, right? Fundamentally, the people and the process initiatives that lays the foundation to cultivate the culture where we have accountability, whether it's to our process. It's how we interact and collaborate with our colleagues. How do we form the tribal culture within the group so that cross-functional team come together? In terms of the strategy, it's really all the way down from front-end, customer-facing, consistently strengthening the seamless customer experience all the way down to how we operate in our operations, data center maintenance. So there's a set of strategies and tactics that are developed but continually be in the works where we thread the three Ps that we talked about earlier. But fundamentally, really hit on the last note, it's the performance, right? We talk about agility is not just the ability to react quickly, but also having the dynamics of constantly coming back, looking at the combination of people and process that we place, but also be able to tweak our process, be able to course-correct quickly, right? So I think this iterative loop ultimately drives the kind of effectiveness that we need to have in our operations. The other very fundamental and critical element, it's also upstream ourselves to understand and appreciate our customers' decision-making. Some of the market shocks that I cited earlier that happened in the year, beginning of the year period, right? A lot of those, the way to impact us was through how we impact our customer in their decision-making. So again, going back to earlier notion of the ability, the mental, the intellectual organizational capacity to understand the whys and the hows and connect the dots, and the ability to get into anti-fragile state. Those are all the right elements for us to get our enterprise to be in the state of agility. And that ultimately enables the global scale efforts of our business. Yeah, very good. So do you have any thoughts within the organization on ways to foster strong leader-team relationships during periods of significant transformation and change like the data center industry is undergoing right now? Yeah. You know, when I look at, I've been into several industries before transitioning my career into data center. When I reflect my journey, really data center is probably one of the most integrated set of market environment where all the way from customer down to supply chain management. That integration, the success of it depends upon a lot of inspiring and productive relationships. I think we've covered a lot of those basics. We translate down to organizational alignment on effective high-performing relationships. So some of these require constant, you know, one trial and error so that we can course correct where we need to be. But it's also enable effective information flow. You hear the notion data is the new oil. It does happen. When I think about, you know, our business value chain beginning to end, it's the three piece we talked about, but ultimately a key ingredient that throw all of them together. It's about information flow and that ultimately drives the execution that require us to look at, you know, the set of behaviors, the ways so that we can replicate success from very small things across the organization globally at scale. I think in a way, corporate America is also a reflection of our social environment. The good thing about, you know, corporate enterprise, it does allow us to form the culture, which is a set of norm behavior that creates the corporate brand. So building on these kind of the value set that we need for the business, you know, the approach to identify the leaders from ground up from our rank and files and promote their role, their organizational impact are again, those kind of little things, but have such long lasting impact that we need to replicate. So these are, you know, the kind of examples when I think about what are the kind of relationship that are important to business success. It's going to be these and to continue the expensive collaboration across all teams vertically and horizontally. Understood. So communications and negotiation appear to be key aspects of your role. How do you align different parts of the organization to ensure collaboration and effective decision making during transformation? If anything, I think when I when I look at, you know, I've been with the business for about seven years. I think this is a typical, very typical place where because of the high growth, because of the volume, we have not really had a chance to holistically evaluate our processes. And what we had in place that can support successfully before no longer work as recognized within the business. Now, along with that, undoubtedly, there's going to be a lot of folks and a lot of different views where it's consistent with what's required. Ten years ago, when my revenue was very minimal and now is tenfold bigger. Right. So with that kind of growth, we all have to, especially the leaders, all have to reflect, you know, whatever the way of doing business ten years ago would no longer continue to support what needs to be done now and for the future. However, navigating, promoting the changes can be very difficult, compounded by a very diverse set of background of views and working styles. So I think, you know, the empathy, the ability to communicate with a sense of humility, all these are important qualities for our leaders to really to be out there, you know, understand from all the way down to the ground level across different parts of the organization. What are the true hurdles that really impede us from moving forward? What are the behaviors, even small, that need to be scaled and replicated to be the core to drive us to the next level and be self-aware? I think being the functional leader for a 40 million business fundamentally is very different, you know, from my 400 million business. So the constant upscale of ourselves and continuous learning are so critical. And these are the kind of values and these kind of behaviors I really try to drive and promote within the organization. Great points. I wanted to ask, additionally, how do you balance the data-centric, analytical side of transformation with the human-centered aspects of leadership and change management? Going back to people and people and people, I think that's where you're getting at, Matt. So it's assumed there will be and it's actually it is around us. You know, there are all kinds of divergent views. The good thing is you have you know, we have a lot of great, smart people with very clear views about what the vision is, you know, from different angles as to where we built this to be in totality a success in transforming ourselves. But what's pivotal for me is really to align the organizational culture so that it has the intellectual capacity and skills to negotiate differences to come to the common ground. I think we may land in a place where it's not reflective of the best pros and cons or tradeoffs, but really ultimately it's the agility that we talked about earlier that can get us to a quick set of actions to, of course, correct and get us back to the right track. Ultimately, it's the mental flexibility and again, the anti-fragility attributes or behavior that formed the fundamental DNA to allow us to change and adjust ourselves to really support the mode of operations today, tomorrow, in the future. Right. Especially as we talked about earlier, AI is going to fundamentally change our world. That's just the one way as to, you know, the best and manifest what we need to get to to support our business operations. So I can't say enough about the people and people, people, right. We can give our people the best tools in the process. Ultimately, again, ingenuity comes from how our people operate. So that's really what I'm trying to drive with this initiative to make sure that we focus on our people and give them the right technology and that combination drive to the kind of business outcome we need to get to for our shareholders. Absolutely. Well, as we wind down to our last couple of questions here, I wanted to ask you what excites you the most about the future for EdgeConnects as you lead the company through these transformation efforts and how will it help the company scale? That strikes such a nice chord as I reflect on the journey. I feel we're so lucky to be in a place where we're experiencing AI as, you know, it starts to transform the world. You know, reflecting, I'm dating myself, but reflecting the past 20 years, right. You know, we go through the Internet phase, the cloud, the mobile and AI. This seems to be the one fundamentally different than past waves in terms of where it's leading the culture and society into. When I put it in the context of EdgeConnects, we're right in the center of supporting that culture and social evolution. So it's really what can be more exciting than being in the right industry, the right market at the right time. So my biggest joy when I look forward, based on all that exciting market dynamics, is if we can transform this place so that we can really produce or cultivate the environment to remove the clutter, distraction, the noises, unleash our people from being bogged down by the day-to-day inefficient process or data clutter or inefficient processes. Right. So ultimately what comes out of that is the collective intellectual power in analyzing our business, drive the right side of insights, create value. From all of that, it's going to create for our vast workforce a lot more and more of our young generation of workforce. It's going to just create such a great, exciting environment for them to grow in their career path. That is the most exciting thing for me to look forward to, Matt, in executing the changes that we have planned. Really happy people, successful career development for our younger generation. That's just so much exciting to look forward to. Thank you. Very well put. And finally, last question here. What advice, Jenny, would you give to other leaders navigating fast-growing and rapidly evolving industries like the data center industry? Thank you for asking. I'm only four months into this journey. Every day when I come to work, when I think about what our people have accomplished, there's so much time, there's so much to be done, but so little time. So prioritizing based on, you know, I guess every business value chain that really allow us to allocate our resources, prioritize critical items to deliver enterprise mandate, the ability to really move fast, have the calculus to really recalibrate as changes come. Always be clear with the vision. I think all of these are super critical. We just need to constant course correct or course adjust because, again, in this space we're in, there's constant market drivers that change things to a different extent. So all these bring it back to the agility that we need to form. So being in this role, I'm lucky to be asked to take on this journey. It's also a privilege. And that really put me in a place where, you know, I have to remind myself to really be, you know, holding the vision clear, aligned with our leadership, aligned with our staff and together build the pathway to where we want to drive to the business mandate and again to, you know, to our shareholders. But ultimately, what we are supporting, like I said earlier, is the backbone of society in driving and riding the AI wave. I think ultimately having the ability to really network and communicate and again, the sense of humility is something that's fundamental to how we drive this to be successful. Absolutely. Well, thank you so much, Jenny, for joining us here today on the DCF podcast. It's always great to catch up with EdgeConnects and I wish you luck in your ongoing role as Chief Transformation Officer with EdgeConnects. Thanks, Matt. Really appreciate the opportunity. Same here. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-12\n==================================================\n\n### Open Source, AMD GPUs, and the Future of Edge Inference: Vultrs Big AI Bet\n**Episode ID:** 5091\n**Transcribed:** 2025-08-04 23:53:26.508839\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief of Data Center Frontier. And today we're here with Kevin Cochrane. He's Chief Marketing Officer for Vultr, cloud infrastructure provider. Hi, Kevin, thanks for joining us. Hi. Great to be here today. Yeah, great to have you here. Well, I know we're going to be talking about Vultr's deployment of AMD GPUs in your Chicago data center. But maybe first for our audience, you could just tell us a little bit about Vultr as a company and about your own history in the industry. Great. Well, again, thank you so much and so happy to be here today. You know, Vultr is a very interesting company. So the company actually had its roots 25 years ago with two engineers literally in a basement in New Jersey, a group of network engineers that decided to start getting into the hosting business. So a lot of enterprises back then were standing up their first websites and web applications. And you know, this group of engineers were helping them starting to stand up all the physical infrastructure and different data centers around the country in order to move their business to the web. And after 15 years of helping support some of the most ambitious enterprise workloads, this group of engineers decided to build an entirely new cloud platform, taking all of the learnings that they had, scaling and delivering outstanding performance at incredible price to performance ratios for these large enterprise customers. And in 2014, they launched Vultr. Now, Vultr, the platform, was built by developers for developers to support, again, some of the most demanding enterprise workloads. And the platform, with zero sales, zero marketing, zero on any outside funding, simply took off. Took off because it was easy to use, simple, transparent pricing, and just the best price to performance in the industry, literally offering all of your core infrastructure services at prices that are anywhere between 50 and 90 percent below the hyperscalers. Over the course of the past 10 years, the platform has grown exceptionally rapidly. Now actually spanning 32 data center regions across all six continents, reaching 90 percent of the world's population in between 2 and 40 milliseconds. And now today, here at Vultr, we're pioneering a new generation of cloud infrastructure specifically to support AI and our premier partners of NVIDIA and AMD being the only vendor taking top of the line GPUs to market based on customer demand anywhere in the world. So we couldn't be more thrilled specifically to be partnered with AMD. AMD has been a longstanding strategic partner of Vultr, you know, on the core CPU side of the business, you know, our customers shown a clear preference for AMD data center CPUs to support all of their cloud data workloads. And it was an incredible honor to be able to extend our relationship with AMD to also bring to market not just their epic CPUs, but now their instinct GPUs to market as we reinvent cloud infrastructure for this new day and age of AI. Thanks for that, Kevin. It's, you know, as a privately held cloud provider, how does Vultr's approach to AI infrastructure investment differ, you know, from the major hyperscalers? Can you color that in a little for us? Yeah, so it's a really good question, because again, I think one of the things that's most amazing about the Vultr story is how a company like Vultr in a capital intensive industry grew to our scale, all without a single dime of outside capital and without a single person in sales and marketing. And the reason Vultr was able to do this is because of the incredible cost efficiency of the platform. At Vultr, everything we do is highly automated. And the automation enabled this business to grow profitably and to fund CapEx literally through free cash flow. And what's even more remarkable about that is that cost efficiency is something that we pass on to our customers in terms of lower prices to get the same performance and the same scale, the same security and the same compliance as you would find with the hyperscalers. But again, a dramatically lower cost. We here at Vultr believe that price to performance matters more than anything. Developers love efficiency and organizations today that are facing exceptionally high and variable cost of cloud compute deserve better. They deserve to have a lower cloud compute bill for core commoditized cloud services to free up capital to invest in innovation, specifically to invest in AI, AI infrastructure and GPUs. And so what makes Vultr different is because we were private and literally funded through free cash flow, again, cost efficiency is everything in this organization. And we take all of those same principles of cost efficiency to how we're building and scaling GPUs. OK, so there are many companies that are raising gobs of capital and deploying GPU rentals at rock bottom prices simply to get any money to any and any money to pay off the inventory. And the long term economic model behind these GPU rentals does not work. So in the Vultr case, everything we do is built with maximum capital efficiency in mind. So when we're building and we're scaling our GPU business to help enterprises build and scale new AI native applications, we're doing it with a level of automation that enables us to deliver the best price to performance in the industry on the GPU side as well. At the same time as ensuring our long term economic success so that we can continue to invest in new GPU infrastructure, continue to invest in new generations of GPU and to continue to expand the global availability of those GPUs to support developers wherever they live and work. Because we've been doing this for so long, no one can match our cost efficiency and the model that we can apply to GPU so that we can actually be your long term partner in guaranteeing your AI success. Excellent insights. Thanks. So let's take it down to the GPU level now. According to your press release, Vultr's partnership with AMD includes both the MI325X and the MI300X GPUs. What role does each play in your AI cloud strategy and how do they complement each other for different AI workloads? Yeah, so it's a wonderful question. So I think the one of the most important things to focus on is where the GPU market is going. And, you know, the market up until now, we like to say it was kind of the early market. And in the early market, people were just looking to get whatever GPU they could, like the H100, as quickly as possible to start standing up large scale training clusters and centralized facilities. OK, and the majority of these GPUs got hoovered up by a very limited number of entities that were building kind of frontier models. Right. And here at Vultr, right, our viewpoint has long been that we're in a 10 year cycle where enterprises are going to reinvent their entire tech stack, rebuild all of their enterprise applications, and they're going to put AI at the core and they're going to build out all of this on a new generation of cloud infrastructure and services. And, you know, we've seen these 10 year cycles before. It's kind of the same 10 year cycle that, you know, started in the late 90s with, you know, moving, you know, server machines out of your own data center into colo facilities and starting to stand up entire web infrastructure stack. It happened with the move to cloud and now it's happening again with AI. Now, the reason why this matters is because here at Vultr, what we've been focused on is how to enable the greatest cost efficiency and the best price to performance for enterprises that are going to actually build and scale, you know, agentic AI specifically enterprise wide. And what that means is that you need to have the optimal infrastructure, the optimal GPU that can deliver the best possible inference performance and to do it at the greatest power efficiency and the greatest cost efficiency. Because in the early market where you were just spending whatever you could to get as many GPUs as you could into one centralized facility, in this new world, you have no idea where you're going to need to put your GPUs because it's going to be sensitive to whatever the spiking demand patterns are from your employees and customers in different operating regions. So you need to be able to have access to GPUs wherever your customers and, you know, employees live and work and they need to be able to scale up and down quickly and easily. And they need to also have, you know, again, the greatest power efficiency, the greatest cost efficiency, because you're going to be doing this at a scale that's 100x what you're doing for training. Okay, literally, the scale is going to be 100x what you're going to do at training. And so this is where AMD comes into play. Because already today on the CPU side of the business, we have AMD CPUs across all of our 32 worldwide data center regions for scaling your application code. And now we're coupling that with AMD GPUs in those same data center regions. So you can start scaling your inference workloads that are backing those new applications. And again, where the MI 300x and the MI 325x excel is they excel at inference workloads, highly tuned, highly optimized for inference with the largest amount of VRAM. So these large models can actually operate on fewer GPUs. So just as an example, you know, if you take the latest, you know, Lama, where it might take four GPUs from one GPU provider, it would take two GPUs from AMD and their 325x. So that matters because it helps with the scaling and it helps with the cost efficiency when you're trying to support very large inference workloads everywhere around the planet. So this is why we're particularly excited about AMD and where they're heading. Now, the 325x is just simply the latest generation of the Instinct series. So for customers that want to be at the frontier of performance, you're going to want to get on the MI 325x. But for the majority of your inference workloads, you know, you can also get a better price to performance ratio with 300x. So it really depends on your application workload and, you know, what the ROI is you're seeking to achieve on that. And then there's just simply two different price to performance equations that you need to look at to make the right choice between the 300x and the 325x, both extremely compelling chips fit for purpose for any inference workloads worldwide. And just it's an optimization decision, which chip works best for you. Got it. Thanks for connecting those dots for us. Now, the press release makes special mention of the Supermicro AS8126GSTN-MR8U servers that host the AMD GPUs. Any extra details you want to give us about that relationship? I know Supermicro is pretty pervasive with seems like all the GPUs these days, but. That's correct. Yeah. No, we're super proud to be partners with Supermicro. We'll actually be with AMD and Supermicro at an event in San Francisco the first week of MAIA Infra Summit. So we're looking forward to that. The great thing about Supermicro is Supermicro works very, very closely with AMD and very closely with us to make certain that we have the right bill of materials and the right build out of our servers and our racks. Because at the end of the day, this matters. We want to be first to market. And when we get to market and we turn on availability, we want the performance to be the best it possibly could be. And so being able to work so closely with Supermicro to be able to get the earliest access to the most hardened systems that work out of the box and deliver blazing fast performance. Is something that we're going to be more excited about. And we've done this with them with the MI300X. We've done this with them now with the MI325X. And we're looking forward to even future generations of AMD that we can continue the partnership to provide faster access to the best performing, lowest cost infrastructure that people need today to deliver new agentic AI workloads. Yeah, excellent details. Thank you. Well, next, I want to ask you about AMD's ROKM Open, or however it's pronounced, ROKM or ROSM Open Software Ecosystem as compared to CUDA. You know, just looking for some details on how that integrates with Vultures cloud infrastructure, any steps that might be underway to enhance compatibility with those major AI frameworks. Yeah, so we're super excited about ROKM. And the reason is simple. You know, we believe open source, open standards is the key to innovation and the key to a healthy ecosystem. So here at Vulture, we are an open standards based platform. We support all of your standard APIs. We support, you know, the OpenAI API for our inference endpoints. We support the cluster API so we can hook into your multi-cloud deployments. You know, we support all of your core, you know, infrastructures, code standards, you name it. Open source and open standards always win in the long run. And open source and open standards only mean that more developers can get more access and build and build faster than if they wasn't in an open community. So what ROKM is doing is ROKM is building a new open source ecosystem around all of the tooling for GPUs. And already today, you know, we're seeing developers around the world get really, really excited about the granularity of control that they have with ROKM and be excited about all of the innovation coming out of ROKM, like the latest 6.4 release just this week. So what we're doing with AMD is helping to shine a spotlight on ROKM. And we do that by doing hackathons. So currently we're running a hackathon just, you know, this month, three cities around the globe, London and Paris and Berlin, with over 750 developers to basically showcase the power of ROKM for developers to build and scale AI applications. You know, and it's fun because, you know, with things like ROKM, you know, hugging face is zero day support for ROKM. So what does that mean? It means any containerized model that you have on hugging face, you know, the day it's pushed on the hugging face, it automatically just works on AMD GPUs and on ROKM. So, again, that's the power of open source. That's the power of open standards. It encourages all developers to jump in, innovate faster and make things happen. So we plan on doing a lot more showcasing of ROKM and doing a lot more hackathons. And we couldn't be more excited to participate in that open community and help push progress there. Yeah, that's great. And I totally agree about open source and standards. Yeah, so here's a simple reason. Just look at it. Look at any enterprise software stack today. Look at any SaaS application today. Unpack it. Like 90% of it's open source, right? It's basically stringing together a whole bunch of Apache modules. You know, I had the benefit earlier in my career of, you know, working very closely with the Apache Software Foundation. And I remember, you know, in 2005, 2006, starting an open source company in Europe. And, you know, back then people were like, can you trust open source? And, you know, we went to enterprise after enterprise, all the big banks, all the big, you know, public sector institutions and basically evangelized open source and said, you know, in the long run, you know, by having access to an open community and a community led development process, it's only going to mean faster innovation, better, more secure, more hardened, more resilient infrastructure when you're building the application. And at the time it was a little bit heretical, but within a few years, people started adopting open source and started seeing the benefit of that. And really it unlocked a lot of the potential, a lot of the potential that you're seeing in the cloud and the mass adoption of the cloud wouldn't have been possible without the rapid adoption of open source and open standards. And so when you're looking at GPUs, what needs to happen with GPUs and AI is the whole ecosystem needs to similarly get unlocked, right? And the only way to unlock it and accelerate the knowledge of every developer around the world to build AI applications is it's got to be through open source. It's got to be through open standards and an open community led development process, because otherwise the community will never grow. And, you know, we won't see the adoption that we need to see over the next 10 years as we rebuild the foundations of how enterprises, you know, communicate with their employees and communicate with their customers and, you know, and deliver services and value to people all around the world. Well, great points, and especially for data center frontier, but so this has been great to wrap things up. Let me ask you one last question with a $3.5 billion valuation and recent growth financing. What role does AI infrastructure play in Vultures long term expansion strategy? Is it the whole game or is it a half the pie graph or how does that come down? Yeah, so, you know, like I said, we're in the start of a 10 year cycle and that 10 year cycle is going to be a wholesale radical rethink of cloud infrastructure and, you know, what we call AI native applications and AI native applications are all your containerized application code. It's all your containerized model. It's all of your containerized data all being orchestrated across large scale global clusters of GPUs and CPUs, all running as close to the edge as possible. So we believe that, you know, the next 10 years is going to see a massive build out of GPUs. So we're going to see the same scale of build out on the CPU front on the GPU front that we have on the CPU front. So take all of the cloud infrastructure you see deployed, double it, triple it, layering in GPUs. This is the 100x growth goal over when you go from inference to training, right? The world is going to be one big inference engine in the long run. And so it's going to be hard to imagine now. If you had to imagine, you know, 15, 10 years ago where we would be with cloud today, would you have thought it would have reached the scale that it has? No, you would not have. And we're going to be at that same point in 10 years time when we look back and marvel at how did we not see the massive scale of what's going to happen when every city on every planet will have massive clusters of GPUs running inference, basically supporting how people drive to work, how people communicate on the phone, how people answer questions in their daily lives. The world's going to become one big inference engine and the opportunity for GPUs and for Vultr, I think, is relatively unbounded. Well, very well put. So thanks again, Kevin, to you and to Vultr for joining us here today on the podcast. Thank you so much. We'll see you next time on the Data Center Frontier Show podcast. We'll see you next time on the Data Center Frontier Show. Thank you. Bye. Bye.  Bye. Bye. Bye. 
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-10\n==================================================\n\n### DCIM (Data Center Infrastructure Management) and its Role in Data Center Security\n**Episode ID:** 5092\n**Transcribed:** 2025-08-04 23:53:26.508876\n\n**TRANSCRIPT:**\nHello and welcome into another episode of the Datacenter Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Datacenter Frontier. And today we are here again to speak with Craig Campiano. He is the CEO of Modius Incorporated. Today we're here to speak about the role of datacenter infrastructure management or DSIM tools as a key component of a strong datacenter security strategy. So Craig, welcome in again to our podcast. Great to have you here. Thank you, Matt. Good to join you today. Yeah. So let's dive right into the questions, Craig. I wanted to ask you, can you explain why DSIM is such a critical component of a modern datacenter security strategy and how Modius' open data solution stands out in addressing such challenges? Sure. I think, you know, datacenters have evolved over time, especially as it relates to the information access that's now part and parcel of enterprise management of these critical datacenters. So in the past, I think the software systems that supported management of the infrastructure rested primarily on the facility side and in building management systems, and that worked for a number of years. Today, enterprises want more data available in business applications for analytics purposes, for customer SLA purposes. Even the advent of AI and the leveraging of AI into this critical infrastructure all requires that data from critical facility infrastructure needs to be accessed at the IT layer, and it needs to be integrated with the IT asset objects to make a holistic view of critical availability, capacity, and efficiency. Those three dimensions all depend on marrying measurement data with IT workloads or the IT assets themselves. DSIM plays that role. So DSIM is not just an inventory system, and DSIM is not just a monitoring system. DSIM is the combination of those two disciplines into a holistic view. Thank you for that explanation and connecting those dots for us, Craig. So next, I wanted to turn to the topic of zero trust architecture, which we know has become a buzzword in cybersecurity. I wanted to ask you, how does Modius integrate zero trust principles into the open data platform, and why is this integration so important for data center security? Sure. So as I mentioned in the last question, the DSIM today stands at this intersection of OT data, operational technologies, think facility infrastructure, right? Those components, and getting that data out of those protected networks up to an enterprise level to the IT layer. That transformation of data requires additional security practices. The IT world has been used to the concept of zero trust for quite a while. It's now the need to bring those same principles of never trust, always verify, to bring that principle down to this new level of integration and new levels of data. So we have built our platform from inception with this idea that we have verification of user access rights, two-factor authentication principles, encryption of data as it moves over the network from the device network and the devices themselves up to that IT layer, encryption of data at rest, some basic principles that I would say have lived for years within the IT stack, but may be new to those who are implementing this converged data set from the OT data to the IT analytics and management layer. So the OT layer is especially weak when it comes to implementing modern IT security practices. The legacy systems, many have been in place for 10 years, 20 years, some of the devices, the protocols. They were not designed originally with the cybersecurity in mind. So what we have tried to do is take those best practices of zero trust architectures and bring them down to this physical layer and provide some degree of inoculation for those OT devices and systems. Yeah, really good insights there, and especially with data centers becoming increasingly complex. I wanted to touch on this interconnection between IT and OT systems that you just alluded to. How does Modius's open data bridge the gap between the IT and the OT environments? Why is this so critical for operational security in terms of bringing the OT up to where it needs to be for cybersecurity? Well, the data centers are increasingly complicated. The risk and cost is increasing. So people are just paying more attention to the analysis of operational performance, whether that's for preventative maintenance, predicting failures. Capital is expensive. Infrastructure is expensive. People want to optimize the use of that infrastructure. What is the available capacity? Is there stranded capacity? And in great part, people are still conscious of the need for efficiency and sustainability in these facilities that consume so much energy. So the point is that I think the desire to manage more efficiently and more completely is dependent on having rich data sets of normalized data that is available for analysis. The analysis tools, whether it's, you know, I'll say simple charting and reporting things or more complicated BI tools or next generation AI tools, all those things work on, I'll say, the IT stack. They don't work so well or they're not made for really using down at the OT layer. So the data needs to be captured from this disparate equipment with very disparate data sets and brought up to a normalized and structured data set that allows for easy consumption by every stakeholder, not just some, you know, some guy back in the IT department who can run some customized query when somebody asks the question, but can the data be used day in and day out by the operators in the field? And that requires tools that are built for easy access, for user customization, for providing those insights. So the data is there. The data needs to be wrangled into data sets that allow for that analysis. So this OT to IT convergence occurs almost point by point coming out of the equipment and being mapped into a construct that the operator can recognize, whether that's by data hall, by row, by rack, by server, whatever their construct is, we need to be able to supply measured data against those objects and do it in an efficient way. Yeah, great points there all, especially about the accessibility of this type of data. But so looking ahead, what do you see as the biggest challenges, but also opportunities for data center security? And how is Modius positioning itself to lead the industry into the future? Well, I would say cybersecurity is more, is a higher risk today because the value of data centers for protecting our resources, managing our economy, everything that we see around us probably has a basis in operational control within a data center. So bad actors look for ways to disturb our economy, our health, and those disturbances are likely to affect or be implemented in a data center. So all that same infrastructure that we've been talking about is susceptible for bad actors to perform bad acts and we need to be better prepared. So NIST and the federal government has already taken steps to implement this zero trust architecture within its domains. And I believe that over time, either the folks that service those agencies, other public agencies, other entities that are regulated, they will move slowly towards that zero trust architecture, not just within the IT layer, but also within that OT layer where DSIM plays. So again, the zero trust architecture is not a new idea. It's been around for a number of years. We are seeing now that infrastructure that supports these regulated or government entities has to start conforming to zero trust and it will percolate through the stack because DSIM and IT infrastructure are interrelated so much. So they share the same networks, they share the same servers, the same IT infrastructure supports workloads that could look like a DSIM workload or it could look like a workload for an HR system. So if you have the overlap of data and access and infrastructure, you have an opportunity for bad actors to interrupt the performance of the data center. So DSIM, I think, needs to stand up and be supporting a zero trust architecture. It's part of supporting the IT loads in a safe, secure way. So it needs to stand up on its own merit and fulfill the expectations that people have for cybersecurity mandates. And the old way of just trying to build a firewall around the facility infrastructure and leave that alone is out of date because these data sets now cross the boundary between protected networks and the enterprise networks. It's the nature of this data analysis that we've been speaking about. So the opportunity, I think, for Modius and for other DSIM products is to stand up and help organizations migrate their infrastructure from 20-year-old technologies that do not provide a lot of integration support and do it in a cyber secure way and migrate those systems to next generation or what I would almost say current generation IT standards. And I think that across the domains, we encounter everyday environments that are not very well protected. People are struggling today because many of the, and I won't, I hate to use the term DSIM, but some form of infrastructure software in many cases was developed in-house years ago without a cybersecurity mandate in mind. And now they, you know, different enterprises are facing the reality of having to upgrade systems that are very expensive to upgrade because they're older, they're intertwined with their organization, they're integrated to their building management systems or their EPMS, and those systems are frankly kind of messy to integrate to. So the opportunity for the vendor community is to help organizations migrate from older technologies to current standards and do so in cost effective manner and then be able to maintain those systems as the threat vectors change over time and always have a highly resilient and cyber secure framework for moving forward with these critical assets. Absolutely. It's such a vital and timely discussion for the data centers tying in cybersecurity with systems like Open Data. So we thank you for bringing it to us, Craig. Really appreciate having you here today. Always great to catch up with Motius. Thank you. I would make one final note. We do have a good white paper on our website that really goes into the details of Zero Trust Architecture and how DSIM should support this initiative. And that white paper is available on the website and can be downloaded if somebody wants more information. Yes, absolutely. Folks should definitely go and check out the Motius white paper on DSIM and cybersecurity. So thank you for bringing that to our attention. Okay, well, thanks again, Craig. Like I said, always great to catch up with Motius here. Thank you, Matt. Bye bye. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-06-03\n==================================================\n\n### Navigating the Future of Data Center Project Management\n**Episode ID:** 5094\n**Transcribed:** 2025-08-04 23:53:26.508951\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Datacenter Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Datacenter Frontier. And today we're here with Sean Farney. Sean is Vice President of Datacenter Strategy and Innovation with JLL. Hey, Sean, welcome into the podcast. Great to talk to you again and see you again. Matt, always great to be back with the DCF, the place on the internet, about the internet and data centers. Great to be here on a Friday afternoon. We're going to have fun talking about all this cool stuff. Yes, I can't wait. Of course, I should let our audience know that Sean is also a member of the Editorial Advisory Board for our event, the Datacenter Frontier Trends Summit. And of course, Sean, I just saw you last week in Nashville at JLL's Datacenter Forum, which was super awesome. But today we bring you in here to talk about navigating the future of datacenter project management. I know that JLL has quite a datacenter project management practice. And the first question I really wanted to ask you goes right to the heart of datacenters these days. It is, how is JLL addressing the growing need for liquid cooling retrofits in datacenters at this moment? Well, that's what's allowing JLL to bring sexy back to project management, because when you talk about project management, you might think, well, that's kind of boring. But it's not now because of the amazing new tech in datacenters. I think anyone that's been doing this decades like I have realizes that we've seen more change in the last two years than perhaps the last 20, for real. I call this creative destruction, which was a term used by 20th century economist Joseph Schumpeter to explain how rapid innovation destroys the constructs of the past and ushers in new tech, sometimes at an alarming rate. And that's what's made project management exciting again, and especially the way JLL does it, because we've got things like liquid cooling and high power density that AI has brought very rapidly that need some attention. And we actually were prescient as a company a couple of years ago, looking at how to grow and adjust to the changing market. And last year acquired SCAE, what I call a technical services company that does all of the datacenter, electrical, mechanical, retrofitting, upgrades, you know, batteries, UPS, power distribution, WIPS, power to the cabinet, all the way back, you know, to the high and medium voltage transition. So we can soup to nuts, help our clients with their datacenter needs inside of the datacenter. And because of this rapid change, we're finding the market really needs help managing projects around what some people call retrofitting, and we call futurefitting, you know, equipping and upgrading your datacenter to handle the future vis-a-vis dense power and liquid cooling for AI. So it's very complex. It's very new. Some say this AI thing is still a research project. Not a lot of folks with hands on experience. The benefit that JLL brings to the table is we, you know, we operate hundreds, if not thousands of datacenters for our clients all over the world across more than three gigawatts of mission critical load. So we're doing this stuff. And this includes portfolios of distributed liquid cooling environments. So we have ops teams deployed today every day managing liquid cooling environments. So we, you know, the best way to do something or to learn something is to do it. We do this stuff every day, which translates into making us better at managing the projects which implement this new stuff. So we're well equipped to do this. And we're having a whole lot of fun doing new stuff in old datacenters to make them relevant again. Yeah, yeah. The feeling that I get from you folks is that the demand for these AI retrofits is really palpable, to say the least, these days and insistent. You know, are they really banging on the door for this kind of guidance in your experience here? It's really interesting. My new metaphor for this AI stuff and deploying it is it's like planning a trip to Disney a year in advance with the whole family. You know, you got to do it. You haven't bought your airplane tickets yet or booked your hotel, but it's exciting. You know, it's going to be really exhausting and it's going to cost a lot of money. But it's coming and you're going to do it. We're sometimes learning on the fly. Even as we see over the last year, depending on the environment and the subsegment in the datacenter space, we're seeing this rapid move through different types of liquid cooling technologies, for example, you know, immersion, directed chip, rear door heat exchanger, or all of the above in single environments as we operationally and strategically determine what's going to be the technology of the future. We're still figuring out. So we don't exactly know in three years where we're going to be. We need to start implementing now. We need to make sure we put in extensible and agile infrastructure so that we, when we have to change in the future, change technology type, we have the fundamental backend cooled water and power and network in some cases to make those changes. So it's fun. It's it's very scrappy, I like to say, and the demand is very high because the enterprises who have been largely on the sidelines or not at the forefront, not on the field, for sure, vis-a-vis AI, those have been the hyperscalers. It's mostly been their space. It's mostly been their space as they've built very large learning models in their kind of centralized facilities. Enterprises have been dabbling in this. They've been they've been renting it. They've been using it as a service. There's there's a number of GPU as a service providers out there, but mostly this is hyperscale focused in the learning phase as we move into the inference phase or the insights phase, as I like to call it. This is where AI will be monetized for everyone but the cloud service providers. So every company knows AI will have to be in their value prop, are still thinking about how they actually turn AI into money. But because they're going to have to do it, there is now the strategic need to plan in the long term capital budget that aligns with product development and strategic growth and go to market for any large company, making sure that AI capabilities are built into your digital infrastructure, meaning you may have to deploy this on prem in that corporate data center that you've been migrating out of into the cloud for the last 10 years, where there's now a bunch of stranded power. There's still a pretty viable chiller loop, which can be leveraged for liquid cooling. So it's this really interesting confluence of exogenous technology, kind of cultural events going on that's now tightly woven into the broader economy, which is bringing our clients and the whole industry into this phase where, wow, we better start planning for this stuff strategically so our company can grow over the next five years. Yeah, well, on that note of the strategy, let's zoom out just for a minute. And, you know, we're talking about this retrofitting trend with liquid cooling. Can you explain the larger concept of a term that we've heard from you and JLL a lot, because it's so relevant in this AI moment? Can you explain the concept of adaptive reuse in data centers and the benefits and also the, I'm not sure if I want to say the urgency or just the imminence of this trend? Yeah, of course. My favorite, I might be the industry champion for adaptive reuse, one of my favorite topics, and I've written a couple of chapters in JSA's Greener Data books on this topic. And it's very real estate-y. Adaptive reuse is simply reusing a built asset for another purpose. And interestingly enough, this is how the internet and all the original data centers got started when telecom facilities were deployed and were turned into network access points and metropolitan access points. And out of that grew the first data centers, and the rest is history. So storied facilities like 350 Cermak in Chicago, where I've cut my teeth, one Wilshire, the QTS facility in Atlanta, you know, 111 8th in New York, a whole host of these legendary facilities were adaptive reuse. They were all former, you know, paper mills and printing press manufacturing facilities, a whole wide range of other things, which were then turned into data centers. So the data center industry is good at this. We know how to do it. It's how we got our start. For the last 10, maybe 15 years, we've been on a green field, a really strong green field build push to build brand new facilities. But we're getting to the point where it's pretty constrained as far as finding large pieces of dirt in the power to fuel data centers. I used to say it was 100 acres and 100 megawatts. Now it's 1,000 acres and a gigawatt. We're running out of those low hanging fruit locations. We're having to look harder for those locations. We're finding places like Fort Wayne, Indiana and DeKalb, Illinois, and now even Louisiana and Mississippi, certainly not NFL cities or the core data center markets. So that fact, the fact that hyperscalers are looking at all those sites and competing for all of them. Also, we know from JLL research that the US co-location vacancy hit an all-time low of 2.6% end of year last year, and that's trending down. Rents are up a blended 11% across all categories year over year. It's not a great environment if you're a buyer. So perhaps it is time to take a hard look at the adaptive reuse idea, because we've got a lot of real estate, which perhaps has a better higher use, and there are some opportunities because there's some really great modular implementations of data center gear, mechanical, electrical, and critical load, and all of the above in a box, which would allow you to leverage a shopping mall, for example, or there's a little north of me here in Wisconsin. In Wisconsin Rapids, there's an old paper mill powered by hydro. That's a great candidate. So there are buildings like this all over the world that could be repurposed. There's a company, Techfusions, which uses adaptive reuse as its basis of design. It's also hugely more sustainable. You are not consuming the structural steel and concrete to build a new facility or all of the supply chain related carbon impacts from creating all this stuff and building and so on and so forth, and it's faster to revenue. So there's a lot of really great boxes to check with adaptive reuse, and we help our clients given our real estate breadth and depth. We help our clients find those facilities. Yep. I wanted to kind of pivot here with you into the data center project management piece because you mentioned the SCAE acquisition earlier by JLL. But just as sort of like a lightning round question, what are the key challenges along with cooling that you're seeing data center clients face, and how is JLL's project management approach addressing them? Yeah, we can help our clients because we can deploy a team of seasoned construction and project management folks, and they just may not have that depth on their bench or they're deployed elsewhere. And as mentioned earlier, our folks do this project to project, site to site, client to client all over the world, and have experience managing these retrofits, these future fits, bringing in liquid cooling, rejiggering power distribution to cabinets for dense AI loads. So it's just having the experience, the depth of the bench in the geography. Our reach globally is very, very strong. So we can help clients, particularly who have baskets of portfolios of real estate, lots of locations. We can replicate the labor and the process and the standards across multiple sites. And we've done this for some pretty large players in the space and just accumulate this experience along with it, the run books and process and procedures, and can just help and be deployed quickly and really increase the velocity at which these projects are completed. For sure. How about sustainability these days? What role do you see that playing in the modern data center project management sphere? Yeah, it's been really interesting. I kind of joke that in 2021, 2022, all we talked about was sustainability. And you went to a data center conference, and that was all across the agenda in every single panel. And then along came AI and sustainability got a little more quiet as we got pushed to deliver value, especially in mid-2023-ish, where Wall Street got very interested in AI and its revenue potential to see the pressure go from being sustainable to kind of shift to primarily deploying AI and being ahead of that curve. We're now seeing kind of an even or balanced integrated approach. And there's actually some examples of these retrofits in construction and project management work, which are upgrading facilities to be more efficient, to handle load differently, reusing stranded power, leveraging and reusing chiller plant capacity, and certainly making the overall efficiency, whether you use PUE or other metrics, just make these facilities more up-to-date, more modernized, and by that nature, more efficient. So it's almost like an explicit focus on deploying the latest, greatest technology, but implicitly, you get the benefits of sustainability. So most of the things that we touch, we may be increasing power consumption for a client of a data center, but we may also be increasing the efficiency at which this power is consumed. Right. Yeah, it's interesting to think about those types of equations. I wanted to ask you next about emerging trends in data center technology, how they're shaping project management strategies in particular. I guess you're probably going to tell us about power and modular, but also anything else that you've been tracking there at JLL. Yeah, there's a couple trends, high level. So time to revenue is really becoming important. As I alluded to in the last answer, efficiency and sustainability used to be number one. And then we've already talked about power density and liquid cooling. Almost on top of all of that is time to revenue. Like how quick can you get this? And that's where we excel because we have the experience and the depth and the breadth and the large teams that can help get these projects going and drive them to completion quickly. And again, at the hyperscale level, and I used to work at a hyperscaler, we always have had a very visible product-driven deadlines tied to revenue and product release. That's not abnormal. But I think in the last year and a half, we've seen such a coupling of the street, Wall Street, to what's going on in the industry. And that's brand new. This is really, it's shocking to hear about data centers on the news, see it in the headlines and hear talk of data centers as strategic assets as part of industrial policy. That just blows me away. Having done this so long and having really no one know what we're doing, right? Until the last couple of years. So this injection of urgency vis-a-vis time to revenue, because of the importance now of data centers, that's an interesting factor. So the agility and the ability to work quickly to drive to product completion is more and more a really high-level trend above all else. Now, behind that or going down a level, it's absolutely power density, liquid cooling. I have to support AI. I haven't done this. Hey, JLL, give me one of your teams that has done this. Help me manage this project, get it into fruition. Oh, and you guys can operate this stuff too? Well, then help me there too. So it's this, really, we're moving so fast. 10 years ago, we might have been able to slow roll a new technology or a new build or an expansion of the data hall to allow our teams to train and come up to speed on a technology. That's no longer the case. And that's where you have to bring in outside help, which comes pre-packaged with this experience. And again, that's where we really add a lot of value for our clients. So the speed, the emerging trend, it's funny. It's not, there's a lot of new stuff. I talk quite a bit about that, the creative destruction. But what's also new and surprising is this urgency around driving to revenue so I can meet my earnings expectations and what I promised the street and the critical load that I've shown to my shareholders and I'm going to deliver over the next five years. That's an interesting new variable here on top of all the cool new tech. Yeah, that is, boy, that is just a great insight. And one, by the way, that I think I perceived in some of the talks at last week's JLL Forum in Nashville with the focus on investment. Hey, just going back to how JLL works in global markets. So I wanted to ask you, how does the company help clients navigate complex and different regulatory environments for data centers? And I've been wanting to ask you about zoning and permitting. Somebody else in the data center industry pointed out to me recently that that can be as much of a bottleneck at times as the power conundrum. So what are your thoughts there? It can. And this is where we clearly differentiate and add value. That's bread and butter stuff for JLL. I've been around for more than 200 years doing this stuff. And on the commercial real estate side of things, the numbers are astounding. It's millions and millions and millions of square feet of commercial real estate that we've helped manage, commission, develop across the full spectrum of real estate services. So we bring all that experience dealing with the local authorities having jurisdiction, all the municipal regulations, codes, zoning. We're really good at entitling land and facilities and enabling projects. So we just have teams, we're over 110,000 employees globally now. We have a cast of thousands, quite literally, helping our clients do this type of thing on a day-to-day basis. And it starts pre-funnel with our capital markets folks who help finance deals, debt and equity, and put together developments. And we started that early on in the process. And we inject our folks to help with all of the pre-con, all of the submittals, all of the different permitting. And it goes all the way through construction. We commission sites. So by being in every single part of that ecosystem along the way, including the project management and construction management, we do this all over the, across the globe for our clients in some really, really large environments. We just take all that experience. Again, we take that experience, we populate our teams with it, and we can stamp it out as a template approach. And that's what our folks do. They focus on that all day, every day. And that just allows us to get really good at it and get very efficient at it. Absolutely. I can tell. And I know that you have a great team over there at JLL. Well, Sean, thanks again for another great talk. It's always great to catch up with you. Thanks, Matt. Anytime. Great talking about fun data center stuff. And I always look forward to it and look forward to the next one. Definitely. We'll leave it there then. And we'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-29\n==================================================\n\n### Powering the Future with Fuel Cells: A Deep Dive into On-Site Power Solutions for Data Centers\n**Episode ID:** 5095\n**Transcribed:** 2025-08-04 23:53:26.508988\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Datacenter Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief of Datacenter Frontier. Well, with the datacenter industry's explosive growth, straining power infrastructure, operators are exploring cleaner, faster, and more reliable on-site power options. And so today we're joined by Bala Naidu, he's VP of Energy Transition Solutions with Bloom Energy, and we're going to be discussing how solid oxide fuel cells are helping datacenters to meet these power challenges head on. So welcome in, Bala. Great to have you here. Hi, Matt. Great to be here. Yeah, we always love to catch up with Bloom Energy. So maybe we can begin just by level setting a bit, taking a wide picture view. What, Bala, are the key considerations for on-site power of datacenters? Great question, Matt. On-site power is a growing segment of the power industry today. People expect about more than 30% of capacity that is going to be brought online will be on-site. And when people are choosing on-site power solutions, one of the key considerations is power density, whether you can fit all that power in a given block of land. That is one of them. And then other things like air emissions also matter because generally you want to select technology that have lower emissions so that you can put it on-site. And things like noise and other factors matter as well. And then finally, there is also power quality considerations because typically when you're not connected to the grid, depending on the variations that you expect, power quality considerations come in. And then, of course, finally, the economics of it all. Those are the key things that we have encountered when we talk to our customers. Understood. And now to sort of narrow the focus a little, can you tell us what the progress has been with solid oxide fuel cell technology? Solid oxide fuel cells have been around for a while. But the key part of the technology that Bloom brings to the market is that these fuel cells operate on natural gas. And unlike most of the fuel cells that operate on hydrogen, solid oxide fuel cells operate on natural gas. And they can also operate on other fuels like hydrogen and biogas. And we are in the sixth generation of our fuel cell technology. We have made dramatic improvements to the fuel cell technology over the last 15 to 20 years where we have increased the power density. We have also improved the efficiency and the lifecycle and the reliability. And off late, the last three, four years or so, we have been focusing on load following applications where we have enhanced the technology to be able to follow load very quickly. And all of this done in a reliable manner while delivering power at an efficiency of close to 55 to 60 percent. So those are the improvements that we have seen over the last two decades. I did not realize that Bloom Energy was in the sixth iteration of that technology. That's fascinating. So I wanted to ask you this. Why should data centers consider fuel cells versus reciprocating engines or turbines for natural gas? Yeah. If you look back to the key considerations, right, power density is one factor where we have come a long way. Today's fuel cells are able to deliver 50 to 100 megawatts in a single acre. That is one of them. And then the second part of it is air emissions are negligible with fuel cells. It's very easy to get permitting, air permitting. And the third part of it is also the fact that our modules, we do it in modules. A smallest module is 325 kilowatts. And the largest one, we can build them up to hundreds of megawatts. And when people are constructing data centers, not all the load comes in at the same time. So you can actually build it out in a modular fashion in line with your demand growth. And then you can keep adding several of these as you get more and more demand. And then you're able to do all of this using 30% lower natural gas fuel than any other technology as RISP or gas turbines. So you're getting more power for the same amount of gas, 30% more power, or for the same power you're getting using 30% less gas. So all of these make it a very viable economic consideration for data center owners. Yeah. Thanks for connecting those dots for us. I wanted to ask you about deployment. What does the deployment process of Bloom's solid oxide fuel cells look like in the context of a new data center campus? How quickly can these systems be up and running? We can deploy these fuel cells very quickly in a matter of two months. We can get up and running. It depends a lot on the block size and what kind of construction needs to happen on site, but we can deploy them very quickly. And in contrast to other technologies, our supply chain is not as constrained. If you want to buy a turbine today, you have to wait a couple of years, RISP the same thing, and some of the high voltage transformers, there is a long lead time. Bloom fuel cells, we produced AC power, but our native power is direct current. And in some cases that can be utilized directly in data centers as well. Because of those reasons, our deployment cycle is very, very quick. So we can have power up and running in a data hall in the two to six month period, depending on the size of the installation. That's amazing. I wanted to ask you about integration. How do fuel cells integrate with grid power or renewable energy strategies? And can fuel cells support hybrid or microgrid configurations? Absolutely. We have been operating with microgrids for the last 10 plus years. We have about 170 microgrids operating already. And most of our historical installations have been in retail locations like Home Depot, Walmart, those type of locations, behind the meter type of applications. But today we have close to 300 megawatts in data center applications. Some of them are grid connected and some of them are islanded. Depending on the particular use case, people do not want to wait for the grid power to come in. They want to put in data centers quickly. And in those cases, the preferred method of connection is just islanded operation. And our microgrid energy server fits very well with that. We are able to load follow effectively. And at a certain point in time, when the grid power comes in, you're able to connect it directly to the grid as well. And that way you can utilize the combination of the grid and the fuel cells to serve your needs. Understood. I wanted to ask you, what are the long-term cost implications of using solid oxide fuel cells compared to other on-site generation technologies in the way of CAPEX, OPEX, and maintenance? Great question. I think if you look at fuel cell technology, we are the fastest to get to a gigawatt compared to other technologies. And we are going to be getting to the next gigawatt over the next 18 months or so. So our costs have come down dramatically since we started producing these fuel cells a while ago. And as I mentioned earlier, we are in the sixth generation. And as customers deploy our technology and as we make improvements to our fuel cell technology, we are always backward compatible. So like 10 years from now, we are able to retrofit into existing installations with the latest and greatest fuel cell technology. Recipes and gas turbines haven't changed a lot over the last five, 10 years. So although you may get some minor improvements, fuel cells have a lot more technological improvements that we can bring to our customers. So you mentioned about cost. I think the key thing is today we are very competitive with recipes and gas turbines for onsite power generation. We can produce power for less than 10 cents, depending on the gas price. And that is very compelling for customers who don't have to wait three, four years for this other options to arrive. And over time, as we make improvements, we are able to bring in the latest and the greatest and save our customers life cycle costs. Our efficiency is very, very high. Starting out, we produce electricity at a 60% efficiency. Recipes and turbines are usually in the 40s, in the low 40s. So you get a big improvement in your fuel costs and also the CO2 emissions. We are 30% lower in CO2 emissions just because we are more efficient. So I think economically, today we are compelling our efficiency and the emissions benefit provide a compelling option to data center providers to incorporate our technology into onsite power solutions. Yeah, those are some compelling data points. So how does Bloom Energy address fuel supply concerns, particularly with regard to natural gas availability or hydrogen readiness, as well as long-term decarbonization goals? That's a great question as well. I think natural gas is plenty in the United States, but the issue depends on the site, where the site is located and where you have the last mile or two miles of distribution pipelines. So it's very site-specific, but generally natural gas is very widely available. And our fuel cells can operate, of course, on pipeline natural gas. We can also operate on hydrogen when it is available, blends of natural gas and hydrogen as well. So we are future-proofed in terms of fuel availability. The other factor that distinguishes fuel cells is that we require very low-pressure gas, something like 15 PSI atmospheric type of pressure natural gas. And compared to other technologies that you have to pressurize gas, you need 300-400 PSI gas at least for the other technologies. Because of that, gas availability has generally not been an issue. But there are some sites where you have to build the last mile or the last few miles of distribution pipelines where it can be a consideration. But we generally work with our customers to find the best site allocation as well as find the gas providers that can provide gas, even if it's for a temporary period of time until the pipeline has been built. So there are different options that we work with our customers on enabling that capability. Thank you for that. Great insights into the fuel cell technology. I have another question here about scalability. How scalable are Bloom's solutions in meeting hyperscaler versus co-location needs? And what differences exist in deployment strategy between those models? Yeah, so we have close to 1.4 gigawatts deployed around the world today. We have different sites ranging from a few kilowatts to several megawatts. So I think for us, scaling up is very straightforward. The smallest block is 325 kilowatts. And you can deploy multiple versions of that depending on the demand. And you can put them in 25 megawatts an acre, 50 megawatts an acre, or 100 megawatts an acre depending on the land availability. And today, our production capacity is about a gigawatt a year. And we are scaling up our capacity to two to three gigawatts in the next year. And we have plans to go even beyond that. So absolutely. So as you look at hyperscaler installations, depending on the block size of power, we are able to scale up rapidly to meet the demand. As I mentioned before, we don't have constraints in our supply chain. We are not dependent on precious metals or complicated alloys. And we have a very robust supply chain that is available, and this is made in America. So we are able to support a lot of the hyperscaler demands producing it here locally. Understood. So my last question is about lessons learned. Can you share any lessons learned from recent data center deployments using Bloom's solid oxide fuel cells in the way of surprises or innovations or even unexpected benefits? Absolutely. I think there are a few examples that I can think of. I think optimizing the site layout to deliver the most dense power is one important factor. And in some situations, we get into customer sites where we think a certain amount of space is available and then kind of work with the customers on installing it. And we find that we may have to double stack them to get the right power density. All of this is fully capable. So that is one of them. And then in general, there are other aspects like depending on what kind of load following the customer has in some islanded situations, when you start operating, they have a certain amount of load, but then the load doesn't materialize. So our fuel cell systems, the good thing is we deliver our flexible fuel cells that are capable of load following, and we can go from 30% load to 100% load. So we have that capability, but sometimes early on in the process, how much of the load profile varies is not fully known. And so we work with our customers through that. We tailor our products to be able to meet the varying load profiles. And then those are the couple of examples that I can think of. In general, we have enough redundancy in our fuel system, in our overall design. We also have excess capacity that we install on site in case the customer has a little bit more demand. Like if somebody wants 50 megawatts, we usually generally put in 10% more. So that even when you have a forced outage, you are still able to deliver the full capacity. So those are the kind of resilient things that we do to meet, to account for unexpected things that come up in any project. Thank you. Well, it's certainly been a fascinating discussion. Always take any opportunity to catch up with Bloom Energy and learn more about the company's solid oxide fuel cells. So thanks, Bala, for joining us here today. Thanks, Matt. I followed your podcast over the last few weeks as well, and I generally enjoyed all the presenters that you've had. Good luck to you. Well, thanks very much for that. And thanks to all of you for listening. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-27\n==================================================\n\n### Solving the Power Problem for Data Centers\n**Episode ID:** 5096\n**Transcribed:** 2025-08-04 23:53:26.509025\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier, and today we're here with Zachary Zawila. He is Vice President of Business Development for Distributed Power Solutions, DPS. DPS is a global leader in providing scalable and reliable power solutions for data centers, oil and gas, utilities, and industrial customers. Hi, Zachary. Welcome into the podcast. Thanks for joining us. Thank you for having me. Yeah. So, let's talk about this. Does Distributed Power Solutions, can you tell us the experience that your company has providing power for data centers? Yeah, sure. If you don't mind, I'll give a quick snapshot of the evolution of DPS, how we got here, and what we're doing today for data centers. Absolutely. So, DPS is a power solutions provider, bridge power solutions provider, and we've been in business for about seven or eight years now. Essentially, we got started in the oil fields. There were some heavy electrification pushes on the oil fields where a lot of their equipment was being electrified, and they'd have to rely on the grid. Prior to the data centers, they were kind of experiencing the same issue where unable to get utility power or maybe in too much of a remote area to receive that utility power. So, we would build microgrids throughout their properties and let them distribute that electricity out. And then, our second evolution, essentially, DPS started getting involved with utility work, a lot on the borders in Mexico, some within the US, but the reason that was an evolution is we started taking on some significant more scope in terms of substation buildouts, playing around with high voltages, baseloading the utility grid, all of that type of stuff that's a little more complicated than just a microgrid in West Texas, which actually Position does very well to support the data center network once their problems came to light here. And really, in July of 2022, I think when Dominion was the first one to deliver the bad news to Northern Virginia, which is when we really got engaged into the data center industry. So, July 2022, Dominion gave the bad news to a significant amount of their customers that they're probably going to be delayed on their interconnections by a good three to five years. Since then, that's got a bit worse, right? But we got a ton of inquiries from that industry over that timeframe and got engaged with a significant amount in that second half of 2022. But specifically, one of those customers was a bit far ahead in their feasibility, I'd say for one of these self-generation projects. And also, their internal mentality, I think, pushed them to want to be a pioneer, right? They want to be the first to do it. So, we engaged with this partner in October of 2022 and essentially had a contract signed right before Christmas to provide 100 megawatts of prime power to their data center in island mode in an N plus one configuration. So, there would be no utility connection. The power provided to that data center to deliver five nines of reliability would be a combination between DPS's power plant and their backup diesels. And obviously, their UPS's and whatnot inside. But those were just the only power sources, the only generating power sources. Signed that contract in December of 2022, and we commissioned the project in March of 2024. Or sorry, no, December 2023 is when we signed the contract in March of 2024. No, yeah, it was a 14-month procurement time essentially. So, and yeah, we've been running up there for a little over a year now. It's been a great experience for our customer. Our availability over the last year was over three nines, which is pretty, I think that's more than better than they've experienced from any utility that they've had in the country. So, we're pretty proud of that. Yeah. Yeah. I mean, it, you know, impressive story. So, just to level set just a bit, from the perspective of DPS, what is the preferred technology for powering a data center? You know, that's, it could be, there's a few different technologies. So, DPS, we carry both reciprocating engines and gas turbines in our fleet, which is great because we can be flexible to the application at hand. You know, I will say for providing longer-term prime power, a turbine does have its benefits. Reason being their maintenance intervals are significantly more elongated than a reciprocating engine, not only on the major maintenances, but you also don't have the preventative maintenances. So, you know, reciprocating engine every 2,000 hours or so, you're going to need to change the oil and every 4,000 hours, probably change the spark plugs. You don't have any of that with a turbine, right? And turbines, they're kind of made to run and not really shut off. Now, with that being said, you know, the reciprocating engines do bring their benefit where, you know, if you have a scaling load or a smaller load, they're smaller units, redundancy is cheaper and they're typically more fuel efficient from an open cycle standpoint. Understood. So, can you tell us a little bit about whether DPS has experience or the nature of DPS's experience with mission-critical facilities? Yes, this facility here was really our first what the industry would label as mission-critical. We provided power to some significant operations, right, utility peaking operations, which is pretty critical, oil and gas guys, they're not necessarily in the mission-critical category, but they definitely treat their operations as mission-critical if you're down. So, that's so kind of, you know, playing around in those two, those, you know, those industries prior to experience in the data center got us prepped, but I don't really think anything can prep you for the data center industry. It's a, you know, it's definitely some high-end clientele in a good way. And they, you know, they're, the way they look at things, the way they design things and their, their, their embracement of reliability is something that no one is much greater than any other industry out there. So, you know, for DPS stepping into this first mission-critical project, it was absolutely a learning experience. But because of that learning experience, because of our success in that learning experience, we're a significantly stronger company today. Got it. So, what makes the DPS power solution different from others in the space? Yeah. So, what makes the DPS power solution a little bit different is, you know, I think a lot of it goes, there's, there's a few things. First being, you know, the, the way we design our, our power plants and the way we approach that is, you know, we, like I said, we came from a very temporary power aspect from when we got started in the Permians. Our jobs were anywhere from three to six months. You know, now we're looking at five, 10, 15-year projects. But, but coming from that, that background, all of our equipment and the way we design it had to be in a modular aspect or prefabbed aspect. So, whenever we show up to do the job, we're able to mobilize in a very short manner and with minimal works completed at site. So, that's what, you know, that, that, the truncated mobilization is really, or in construction is really what separates DPS and, you know, because, you know, Bill, there's hundreds of power plants in the country, right? But I really think the speed and reliability that DPS does is absolutely one of our competitive advantages. Another one being is, you know, the OEMs we choose to go with, you know, DPS is itself within the Caterpillar network. So, we, you know, we can be a bit biased on that. So, we obviously use Caterpillar recipes, and then we go with Solar Turbines, who is a Caterpillar company. And there, there's, there were a lot, there, there's service and support from both, you know, within both technologies is the best in the industry. You know, you obviously drive around the highways and you can see the support from Caterpillar. As you see, there's signs around everywhere, but solar, you don't necessarily see it, but it's there. There's a ton of Solar Turbines installed in mission-critical facilities that they put support from a field service perspective. So, that's the third being that. And then fourth being DPS's people. That's something you can't recreate. We got a very, very strong talent pool here at DPS all the way from the top down. I'll say specifically our engineering department and our O&M department, they're definitely some of the best in the industry. Our engineering department, everybody has had experience either in bridge power, temporary power, utility power, and mission-critical power. And then on the other side, you got our operations and maintenance team, many people that are power providers outsource their operations and maintenance teams. DPS, a few years ago, we noticed some of those talent pools getting a little, a little light in terms of headcount and talent. So, we decided it was best that we stand up our own team here and give the guys a reason to stay. And it's been a lot of success since then. Right. So, let's get down to brass tacks. How long does it take to provide power after contract signature? Yeah. So, that can depend, right? DPS has two business models. We got a depot model and a build-to-suit model. Within our depot model, that's where DPS, we carry the fleet as is, right? So, we have about 400 megawatts in our fleet, evenly split between reciprocating engines and turbines, and that can support projects anywhere from 5 to 100 megawatts. You know, if we have the equipment in our fleet and it's available, we can install, you know, call it 5 to 20 megawatts in a week to two weeks. We can install, you know, 50 to 100 megawatts and call it 30 to 60 days. There's a lot of other things to consider there, but in the grand scheme of it, that's about our typical process, and we've done it many times as we participate in a truncated project every year for our summer peaking in Mexico. On the other side of that, your build-to-suit, we can do it pretty quick. The issue there being the lead times that a lot of the OEMs are now experiencing due to, you know, the demand everyone's seeing with power. So, you know, I'll say, you know, call it a 300-megawatt power plant. We were probably 18 to 20 months in Q4 of last year. That 18 to 20 months is now, you know, 26 to 30 months today. Gotcha. So maybe you could just color in a little for us your view on how a behind-the-meter power solution compares generally to the utility. Yeah, I think there's some advantages of it, right? Specifically, you're controlling your own destiny. That's probably the biggest one of it. And, you know, that's been some of the greatest feedback from our customer in Virginia is whenever, if they're experiencing a problem on their side or we might be experiencing a problem on our side, it's communicated right away. You know, they let us know that, you know, typically if they were experiencing a utility outage, they would go ahead and call the local utility. If they were even able to get a hold of anybody, it would be a very vague response around timeline and reason. However, us, it's typically proactive. Outage is typically avoided and it's constant communication. So you got that aspect to it. And I think the other aspect that's, you know, that's to be considered too is timeline, right? I think you can beat a lot of the utility timelines with the timeline I just mentioned. And that's, you know, that's obviously a heavy consideration into speed to market, speed to revenue. And then the third one, I think really is, you know, something that wasn't necessarily considered when you're looking at a very short term application, but when you're looking at, you know, five plus, especially 10 plus years, I think you're behind the meter solution will be cheaper than your utility solution. And if it's not today, probably in a three to five year time range, it will be with where the tariffs are heading. Yeah. Now with DPS, are there emissions solutions available? Absolutely. Absolutely. So that's another reason we went with the solar and Caterpillar products. Both of their products are out the factory, have the best in-class emissions, especially solar on the turbine side, they're very low coming out the factory, which makes them pretty compliant in most jurisdictions for, you know, it's typically, you know, at least 364 days or less. If depending on the regulations and the size of the project and the area we're at in the country, there might need to be some addition, additional emissions control added. And DPS has solutions for that, for both the recipients and turbines, specifically our project in Virginia. We have SCRs on all the, on all the, all eight turbines up there and we're injecting ammonia to reduce those emissions. And we're dialing them down all the way down to 2.0 parts per million NOx, which is actually better than the local utility. Huh. Wow. Good data point there. So what are some of the challenges faced for data centers when planning a self-generated power solution? Yeah, there's, well, you know, we'll start with the one we're just kind of getting off of, it's emissions, right? You know, are you in an attainment zone or are you in a non-attainment zone? Do you want to trigger federal regulations or do you want to stay under federal regulations? That's, that's definitely an issue, especially as you're seeing some of these data center campuses grow and their power demand, right? You know, I think two years ago or so it was, you know, 50 to 100 megawatts was a typical ask from a data center. Last year that was, you know, that 100 megawatt number was pretty constant. Second half of last year was 300. Now everybody wants a gigawatt. So permitting a gigawatt is definitely a pretty, pretty substantial ask, but, you know, you can, you can get pretty crafty in that 100 to 300 megawatt range, depending where you're at. So there's some red tape associated with it, but there is possibilities. And then, you know, but one that's became the light a little, a little more than that, which, you know, you might not necessarily think would be a problem, but it's the availability of gas. So a lot of these data centers, they, you know, when they were performing their site selections, they initially would go where the fiber is. Then a lot of the heavy fiber locations, you know, we're experiencing, you know, issues with power supply. So then the data centers were going to find the power. And then after they couldn't, you know, the power was ran out, now they were going to find the gas. So that's kind of where their site selection lays now. But even if you find the gas and you buy a massive pipeline, there might not be availability on that pipeline or getting that lateral over to your site from that main pipeline, you might run into some issues depending on the easements that you'll need to, need to come across to in order to, to make that lateral. So that's, those two things right there, I think are probably the biggest challenges and that we're, that we're seeing customers face. Understood. So you mentioned Caterpillar, but are there any other DPS partners who we should know about? Caterpillar and Solar Turbines are our biggest partners for sure. That's, that's our, that's our, those are our main guys. Yeah. And, and finally, you know, could you describe differences between a short and a long term powering solutions that, that might be deployed by, by DPS? For sure. For sure. And I, I'll, I'll take the opportunity to get my line here. I always say, you know, and that's, and that's the one thing, that's why I love our engineering team here at DPS is you got, you got to know when to design a Ferrari and you got to know, know when to design a Ford Taurus. Right. You know, if you're going in there for, for five plus years at a mission critical application, you probably want the Ferrari. Right. And it makes sense. And in that term allows you to pay for it. If you're going in for a three month or 12 month application, it probably doesn't make sense to go that full blown on the design or reliability that you might put into a five year project. So that's when you, you know, you're able to get some efficiencies for, for that shorter term project, or, you know, maybe the availability doesn't need to be at the level that, that of, of the mission critical level. Right. So there's some things you can do in that aspect with, with, with each of those. And then, you know, obviously with a shorter term, your, your situations for, and regulations around admissions become a bit more alleviated. For sure. Well, I want to thank you so much for joining us here, Zachary. Any closing thoughts or a final point that you'd like to make here for our audience on behalf of Distributed Power Solutions? Yeah, just, you know, Distributed Power Solutions, right? Like I touched on a lot throughout here. We think that we have the most experienced powering data centers in the country, in the U.S. right now, especially on the hyperscaler form. And we're looking forward to doing more. You know, there's a lot of people out there that are saying that they think they figured this out. They might have a solution, but DPS is an actively working solution out there. And we're looking forward to, to deploying more and helping people solve their problems. Very good. Well, thanks again, Zachary. It's been a pleasure having you here on the podcast. And we'll see you next time on the Data Center Frontier Show podcast. Thank you.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-20\n==================================================\n\n### Tony Grayson Breaks Down NorthstarCompass Quantum Deal for AI Data Centers\n**Episode ID:** 5097\n**Transcribed:** 2025-08-04 23:53:26.509062\n\n**TRANSCRIPT:**\nOkay. Hello, and welcome to the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief of Data Center Frontier, and joined here today with my colleague, David Chernikoff, Senior Editor of DCF. And we're here today with Tony Grayson. His company has recently been acquired by Northstar Technology's group that was the Kompass Quantum, known for deploying rapidly deployable modular edge infrastructure. And this was a recent announcement positioning the company to scale its defense and AI-ready platforms globally. So hi, Tony. Welcome into the podcast. Great to have you here. No, great to be here, Matt, and thanks for having me again. Yeah, anytime we can have a Data Center Frontier Trends Summit alumni, past and future. I'm sure it's great. So Tony, let's really peel this apart with Kompass Quantum, because I think people are probably might be a little familiar with the company, but let's just level set it here for what's going on with the acquisition and the company. Yeah. So we were, you know, at Kompass, I was developing a modular brand. So I think kind of where Kompass is building the gigawatt scale campuses, I was building the smaller campuses using, you know, building blocks of modules by, you know, kind of like a stick build. We were doing well on enterprise and we have, you know, some Fortune 50 customers, but then we started getting defense customers. And Kompass is Canadian owned, and that goes against some of the rules that the U.S. government has. If we want to continue doing defense, which, you know, Chris Crosby was a huge supporter of and very fortunate that, you know, he wanted to sell us, he wouldn't hinder us from growing the company, but also service in U.S. defense needs just because we have a lot of contracts that we're going to have to deliver for the U.S. government, as long with defense globally. But, you know, partnering with North Star gives me, I mean, North Star is a partner of Owens Corning. So they're the manufacturers of our composite and they hold all the IP and Owens Corning does all the testing. So it was a natural fit, considering all of our modules, what makes us special is, you know, is the ability to manufacture these as opposed to using steel and, or, and or concrete to do it. And so it was a great partnership. Now it gives me a lot more freedom using North Star as different groups, because they have an engineering group, they have a manufacturing group, they have a construction group. So everything is now in-house instead of trying to go outside to other, you know, using other consultants to build stuff. Yeah. How, how does the acquisition change the company's strategic priorities in terms of the breakdown between, you know, defense versus AI driven edge deployments? I think it gives us a lot more freedom right now because, you know, we are getting, we've been asked to do a little bit larger stuff too. So let's say five, you know, NVIDIA is building these five megawatt increments and you've seen Wade Vinson talk about it, you know, latest data center world, and you see Jensen talking about five megawatts is the kind of line where you cross between the L2, L3 network where prices go up because of all the transceivers you have to use. So, you know, we're kind of built, we have this design that we combined to build in this five megawatt increments for these, for these AI factories and, and drop that stuff in parking lots and do it as an operating lease or as a sale. So, you know, it kind of, where Compass was solely focused on the bigger stuff, it gives me a lot more freedom to do more of this smaller stuff and possibly sell stuff and not just have, not just do operating leases because we, we're working with a lot of channel partners and some of these channel partners do want the sales. And so it just gives us more of that freedom to do that kind of stuff. But, you know, we have, we have customers now defense globally. So think European, APAC, US, we have now customers across the world now to both enterprise. And these are, you know, kind of a telecoms, big enterprise companies, you know, it's kind of really taken off, you know, sometimes it's better to be lucky than good. And I feel like the timing of this couldn't have been better for where, you know, the industry is at right now. Yeah. Could you talk a little, I want to get into some details about, about the platform and stuff, but could you talk a little bit about the team? Because it looks like a dream team of, you know, former service members, of course, you know, people who follow you on LinkedIn know that you were a submarine commander and in the US Navy and I was looking at the, you know, the company's officers now since the North Star acquisition and it just looks like a unique, a uniquely qualified bunch. So I just wanted to ask you about that. Yeah. On the defense side, I would dare, I would, you know, challenge you to find a better group of advisors. We basically have every outgoing six. So the people in charge of the IT and comms for each of the services for the Air Force, for the Marine Corps, for the Army, and for the Navy, we have all them as advisors and some will also be coming over more as a full time. So, you know, that kind of capability where, you know, they, they have a cool down period and restrictions on what they can do, but they can do global stuff and they can advise us on, you know, stuff that the, the US government needs. So, you know, I feel like we're making huge progress on the, the enterprise side, but the defense side is where we need to catch up a lot because defense globally needs this distributed compute. So, you know, they're ahead of where enterprise needs to be with inference platforms and all of that kind of stuff. So they're helping me really do more in that area just based on their, you know, 30 plus years as experience and they all left us three stars. So you can't really beat that. And then of course, we're also doing a lot of stuff with the Navy on, you know, kind of their aircraft as well. And that's why we have the old air boss coming out, Admiral Weitzel. He helps us on, you know, all this stuff that's because all these planes generate so much data. And so, you know, we'd seen, we need some advice on how best to approach that data to be able to internalize it and analyze it and all that kind of stuff for the military. Sounds like a tremendous team. So I want to ask you one question here. And then David, after this question, let's get you in here for questions for Tony. But so the North Star release, I emphasize, I guess what are known as FRP composites. And I'm linking this to the top of the conversation you mentioned, steel and concrete. How did these FRP materials enhance edge deployments compared to traditional steel or concrete enclosures? Yeah. First thing, there's no tariffs involved in any of this stuff. And there will never be any tariffs because we don't use steel or anything like that. So it's all locally sourced and rather easy to get from Owens Corning. And it gives us the capability to use material science and design to what the application is. So if you need a two hour fire rating, we can do a two hour fire rating. If you need, you know, our minimum spec right now is a 250 mile an hour wind load. So it makes it a category five hurricane and, you know, F5 tornado, we're actually certified to be an emergency shelter. If you need more than that, for whatever reason, like some of the military wants, you know, 762 and 50 cal protection, we can do all that kind of stuff with the composite. So it makes it very easy for us to tailor a solution for what you need. But it's also 30% lighter than aluminum. I don't know if you've ever seen the picture of me holding the 15 foot I-beam up. I'm a sub guy. I'm not, you know, I'm not Army. I'm not Army tough. I definitely couldn't hold that up. So it just gives you a lot more capability to get this out, deploy it more cheaper. But the best thing about it is once you deploy it, you don't have to do anything to it. It doesn't have rust. It doesn't require maintenance, doesn't require any of that stuff. So that's why, you know, all these ILA huts that are starting to fall apart and people are starting to realize they're deploying this stuff into, you know, basically in locations where, you know, steel does not do well, let's just say, that we can deploy our stuff and not have to worry about it. So, you know, we have that capability and in Europe, they like us because we are super low carbon. So like our typical shelter on a 55 by 12 by 12, we're about 90 times less carbon than a steel module, the same size, but we're also 100% recyclable too. So we kind of hit everything. And the only problem that we had was initial capex cost was probably about 10% more. But if you take in tariffs, we're about the same price now, but also, you know, our tail end is a lot cheaper. So your TCO, even though your upfront capex may be more, your tail end is a lot cheaper. And you start seeing that just in your first shipments because it doesn't weigh anything. Got it. David, you have a question for Tony? Oh, yeah, the, in your initial announcement, the CEO of Northstars pointed out that using the composites was going to increase the energy efficiency and resiliency of your solutions. Exactly. I mean, is that we're talking about, well, when we, before we set up, is there an ongoing benefit to this or is it strictly in the maintenance and things of that nature? No, this is, so if you can imagine when you, what we don't have with the composite of this is this idea of temperature, you know, thermal hysteresis, meaning if you have a steel module out there, even a concrete module during the day, if it has direct sunlight, you are going to heat up that module, the shell of it. And the energy that you have for cooling your inside is not going to have to cool the computer. It's going to have to cool the shell. Ours doesn't heat up. So the composite does not heat up at all during the day. It maintains thermally neutral. So we actually put your reflective coat on it. So it actually reflects a lot of that energy out. So just think of it like a big Yeti. And so, you know, the whole premise of AI factories is every electron that doesn't go to your making tokens is, you know, you're losing money. Now we take away of you having to cool that entire structure, the outside structure. So you, you know, all those electrons can be used to make it more tokens. So it's, I mean, it kind of comes back to PeeWee a little bit, but you know, you can't get more efficient than a large Yeti. Because it's essentially when you put the parts together, they're bolted and then glued. So you don't have an air exchange. It's like a big unibody. Again, the other benefit, too, is it's also almost impossible to get water into it, too. Wow. OK, so there's no heat sink value there? No, it doesn't heat up during the day, which is also great for thermal, you know, for defense, too, because it doesn't look like anything. It's still cold when you're outside. So it's like it's an R44 is what it comes out to. And that, but that external composite does not pick up any heat. But it also, if it does have like an hour or two hour fire rate, you can just scrape it off the burnt stuff and reuse it. I mean, it will never, it's impossible to catch fire. Cool. This is obviously lighter than traditional building materials. What does that mean in terms of on-site construction or delivery of units? Yeah, so imagine where a lot of the problems with construction is you have to get these massive cranes in there. So these cranes are super expensive. We can use the cheapest crane out there or we can design it so everything is manportable. So all you need is a forklift to install the pieces together. So it really takes care of the labor piece and allows you to use a lot, both in urban areas, but also in areas out in the middle of nowhere where it might be hard to get a crane out there. You can install these very easily. Are you certifying installers in specific techniques such as the gluing and the bolting of these together? Yeah, I mean, we have our own, it's called Commodore Construction, is our part of the Northstar family. But we are also training people in that as well. And, you know, I hate to say, I can say this because I'm at Navy, it's sailor proof. It's like, the best way to describe it, it's like a big IKEA set, put notch in A and then notch B and glue it. It's essentially what it is. So you can use a lot less skilled labor. In fact, you know, we have a big project going up in Atlantic City where they're looking at building a new form of the one track. And we're doing a lot of training for the unions up there so they can learn how to work with this material and install it. I mean, and the cool thing about it is everything's digital. So if you can imagine digital design, the CNC machine, the robots put walls together so we can make an entire wall section in 20 minutes. And then a four person team could install 20 wall sections in a day. So you can crank out modules and or buildings very, very quickly with this method. Just to be perfectly honest, I don't know why more people aren't using it. It really I just don't get I don't get it anymore. Why people really gear towards wood, concrete and steel now, now that I've been able to work with this stuff. Well, the obvious question was, sorry, Matt, I'm sorry, the obvious question comes building codes, so things of that nature, are you working with municipalities to get this approved? Yeah, I mean, so what we have, you know, basically what we had, I mean, North Star has been at this for 10 years, so they basically had to go through all these certifications that are out there for regular building codes. And so we have to work with the AHJs when we first go in and basically say, this is what you have, this is what we have for the requirements of it to translate it. But once you go through it one time, they get it. But it's always that first time we have to sit down and explain it to them. But, you know, these are there are actual manuals out there that have the specifications. So it's not like we're making this stuff up anymore. It's it's it's getting more out there in the mainstream. Thank you. So North Star acquired the company, but it sounds like Compass remains a strategic partner. Are there any specific synergies, joint initiatives or ways those businesses will be integrated under this new structure? I mean, Compass does maintain a share in the company. And, you know, for me, you know, I we can you know, we can use shared projects together, but also, you know, we have some really good supply chain that we set up underneath Compass that we don't want to lose. And so, you know, working still with Compass and bringing them in still in the fold means that we can still work with them on, you know, using our supply chain. And there might be times where Compass needs us for something and there might be times where we need Compass for something. So we want to maintain that kind of partnership because we all have shared culture, shared values and a desire for, you know, kind of what we're doing. So we want to make sure that we're providing the best product for the customer. And I want to go back to something David said, you know, the other benefit of kind of these installations we do, we try to shy away from concrete and we use these things called helical piles. And you can do that because we're using a composite since they're so light. So if you could imagine, we don't have to land grade. I don't have to land grade and put a concrete pad down. I can just come in and use these helical piles, screw them in and install a module in four hours. The benefit of that is no civil permitting, no land runoff. And so this whole permitting cycle for our units is actually a lot quicker than your traditional units where you have to get the permitting, the runoff, all that stuff when you're clearing land to try to level it and laying concrete down. So the other benefit of using this is we are tied to these interesting piers that we can use to install. And that could be for a big building. It could be for a module. It doesn't matter. That is that is such an interesting wrinkle. You're calling this a data center as a service model. Apparently. So how are you seeing demand evolve for that, particularly for the edge? I was having a conversation earlier today with a cloud infrastructure provider who was saying that, you know, in 15 years, the entire planet is going to be one big inference operation. I agree. And, you know, all this is about right now, inference is running on big data centers because it can. It's all text based. Write my paper for me, write my email for me, that kind of stuff. You know, future platforms that are coming out might be latency restricted, meaning thisogenic-degenic comms that you have has a time cycle that it has to meet, you know, whether it's quality control with CPUs, which is 100 microseconds or preventing pedestrian deaths with is five milliseconds. And those are actual platforms out right now. So you'll have those kind of platforms. But there's also the bandwidth problems that, you know, fiber does not, you know, it's it puts a lot of light down, you know, down the fiber. But if you have a thousand people on a phone, on a radio access network and they're recording video, all trying to get to AI, that can't go back to Ashburn. I mean, that's why you have Snowball, all these other things that transport so many petabytes of data. You just can't do it. So you will have to push these platforms closer and closer to where the customers at because, you know, they brought up this idea of data gravity and we've been kind of able to ignore it a little bit. But I don't think we're going to be able to do that with inference. You're going to have to start moving those platforms closer. I mean, that's what defense is doing right now. Whether they by necessity, they have to move their insurance platforms closer because they can't depend on reach back of the cloud because of jamming and everything else. It's the same principle. And so we're going to have to do the same thing. So I do think most of this stuff for inference platforms will be this distributed compute. And you will have these hundred, 500 kilowatt data centers everywhere. And then you'll have some kind of metro aggregation point that might be 10 to 20 megawatts. And you'll have some regional hyperscale data center that's for storage or for some of this other stuff that could be used. So I think it's it's it's you're going to grow out a lot more from these hyperscale data centers into these metro areas. And it's going to be, you know, it's almost like we're shifting from a real estate focus where it was all about cheap real estate and cheap power to a network centric focus where you're trying to put compute closer you can to the customer, but also close to the peer and on ramp points. And so because that's what everything's based on. All these platforms is this network that you have to do. And that's why you're starting to see when people start building an ILA now, they're being smart. They're putting compute right next to that ILA because they know that that ILA is appearing on ramp. So it's you're basically combining back on front hall, which I know is a is a thing that networks don't like to do. But it is an easy way to get into the inference platform, providing that kind of compute to that local area. That's that's a great observation, really, about it going back to the networking. I wanted to ask you, though, what specifically would you say sets Quantum's modular architecture apart from other containerized or skid based edge solutions on the market? I mean, I think it's what makes us a little bit different. We can't do this as an OpEx, as a service. And that's not something Schneider or Vertif do. And that's not something they want to do. We work with some of the best partners out there. You know, Schneider and Vertif, you know, we consider them partners. We don't consider them competitors. But also it really it's our ability to mass manufacture these modules at an incredible place with a high quality, with a composite, because it's so much easier to work with where we don't depend on welders and all that kind of stuff. We can crank these things out. So if you need 10,000 modules, that doesn't scare me. I don't have to worry about all I got to do is build more manufacturing with robots. I don't have to worry about finding 10,000 welders. And so the ability to really get these out using the composite and also tailor that composite to what people's needs are, I think, is what sets us apart. You know, it's everyone can use the same mechanical electrical. Our IP really is in the shell deployment capability, which we talked about, the purine and then the opex capability. Can we zero in on cooling for a minute? Is it sort of, you know, user's choice? I assume there's liquid cooling happening at some level in the enclosure. Oh, yeah. I mean, it really is what the customer wants. And we're seeing everything from standard compute, which is DX with Econ, up to direct-to-chip Blackwell designs. So, you know, we have that, we have root or heat exchanger for hoppers, so we can kind of do it all. And we have the same reference design that NVIDIA uses, you know, working with them right now. And because for us, you know, you can use HPE or you can use WBT or you can use whoever, we can provide the structure and all we need is a hitching point. Like we can take away all those concerns of where do you put that, those racks that you're buying from NVIDIA or, you know, one of these distributors. We can house those racks, put it anywhere. We just need, give me a lat and long and a hitching point. By hitching point, you know, your power and your fiber, you want it to, and we can go anywhere. So we enable everyone to deploy their stuff. So that's why some of these companies that are really using us right now, it's because they've been having some problems selling their kit because the data centers can't handle it. And now they can because I can just deploy anywhere. Amazing. I want to ask you one more question. And then David, I want to let you in here for another question for Tony before we wrap up here. But Tony, you mentioned that Owens Corning is a materials partner. Does that, I can't hear Corning without thinking about fiber optic interconnects and cabling, but is that not what the story is here? It's a little bit. I mean, we're essentially using some of the same, you know, some of the same stuff, but it is two different sections of that company that we're using with. But the good thing is that we're all, they're all one big, happy partner. So we have not been getting into that game yet, but we could easily, you know, get into that game too of, you know, working with them for that, you know, the supply and the fiber. I mean, what I don't want to do is, you know, separate the business away into what we're really good at. But I do think there's possibly some capability of bringing cheaper costs in, you know, because we're using us, we can get, we have direct access to Owens Corning and say, I have to go through a third party. We can probably bring cheaper pricing to some of the customers who want to put the fiber in on, you know, as an OSP. Yeah. David, you have another question for Tony based on what we've been talking about here? Well, you mentioned Schneider Invertebrate as partners. Are you partnering with anybody like HPA or Dell who already is selling equipped modular data centers? Yes, but I can't say who just yet. So, yeah, we are working with several of them because they, you know, like I said, we enabled them to be successful where now they can, we can deploy together. They already know we can handle what they're doing and deploy it for them. We're working with four of them right now. And that announcements will be coming out here shortly too. And from the 20,000 foot perspective, well, actually, or the 3,000 foot deep perspective as a first subcommander, is there a significant reason for these companies that are selling equipped data center, modular data centers to move completely away from their existing biopod and put a box in it to your technology? I mean, I think it requires a mental shift. You know, we're so used to having these big data centers and building the pads out and expanding those pads. But now, you know, I can crank a module out pretty darn quick, you know, it might take you, you know, ideally I'd love to be able to, you know, you want something to get it to that quarter, you know, right now it's not that, you know, we're probably two quarters down, but you order your servers and you order your data center at the same time. So you do not, you know, we're trying to protect the risk from you overbuilding right now. And it's just a different way of thinking. Think of it as more of a just-in-time data center delivery, along with your just-in-time capacity, instead of trying to guess two years out of how much capacity you need, because that's how long it takes to build a data center. So we're trying to take that risk away using the benefits of, you know, mass customization in the manufacturing approach from the composite. Excellent. Well, last question is kind of a searching type of question. It sounds like Quantum's as-a-service model handles everything from permitting to break-fix, but what part of that life cycle would you say customers tend to underestimate? And how does your company seek to simplify that? I mean, I think it's the permitting. You know, it's this, everyone is just, we're so, it takes six months or eight months or nine months to permit. Everyone kind of accepts that. There's ways around that. And, you know, using modules, you can exploit those rules. I mean, heck, you can even put modules, keep them on Moboys and never permit the thing. So, you know, we actually have a design that's 1.8 megawatts and they could easily stand up a cloud. We build a data center in the background and we never permit it. You just move it to the next section. So there's a lot more flexibility that this kind of stuff gives you around that permitting quicker deployment times, which is the best thing people want. You know, in the end, people are going to talk. They want low cost and quick time to market. In the end, time to market is always going to win. And that's what we're really keyed towards, even though we are cheap costs. But, you know, we do have a lot of tricks up our sleeves for that time to market and permitting aspect to it. Well, thanks so much, Tony, for joining us here today on the podcast. It's always great to talk to you. And this has been a great sort of exemplary discussion of stuff that's happening in the data center industry now with AI deployment at the edge and this train of inquiry. So we'll look forward to covering developments with Northstar and Quantum going forward here. Thanks again. No, thank you. We'll see you next time on the Data Center Frontier Show podcast. Transcribed by https://otter.ai
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-15\n==================================================\n\n### Meeting Increasing Cooling Demands in the Data Center Market with LG\n**Episode ID:** 5098\n**Transcribed:** 2025-08-04 23:53:26.509100\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier, and today we're here with Doug Bauer. He's Senior Director of Applied Sales at LG Air Conditioning Technologies, and today we're going to be discussing how increased capacity demands in data centers affect cooling needs and how LG's recently unveiled end-to-end data center cooling solution can help meet these needs while optimizing efficiency and minimizing downtime. So welcome in, Doug. Great to have you here. And it was great also to meet you at Data Center World last month and to get an eye full of the full LG offering for data centers. When LG attended Data Center World and announced their end-to-end cooling solution for data centers, can you tell us a bit more about what LG is doing in the industry in terms of the data center sector and about that latest announcement? Sure. Sure. And basically, we have launched a lot of new products and end-to-end solution of products on the thermal side for data centers, and we're really entering the U.S. market for the first time. We've been in the cooling business for decades. Our chiller business, for example, started in the late 60s, but we haven't really brought that to the U.S. before. So the end-to-end cooling solution that we mentioned in our announcement has to do with taking heat from a chip and taking it all the way to atmosphere, all the products that are involved in doing that and providing cooling and or heating when you need it, but mostly cooling for data centers, for the chip, for the room, for the offices, white space, gray space, what have you. We have the products that can help you do those things. So that includes a cold plate, it includes a CDU, computer room air handlers and chillers and the likes, all of those products. Great. It was certainly great to see all of that hardware on display at Data Center World. So the data center market is experiencing tremendous growth, as we all know, from sort of a 30,000 foot perspective. How would you say this demand will impact data center cooling needs? So it impacts it in a couple of ways. I mean, the initial way is just with availability of product. We saw that a couple of years ago where data centers weren't able to get some product. Technology has a lot of new capacity for the market. So that's going to help there. You also see the need for flexibility and innovation for that product, because with that growth, there's going to be more investment. People are going to come out with new technologies. People are going to want to push the envelope in what a data center is. And so you see the amount of heat concentration required to reject increasing the needs for better and better efficiencies using less and less water. All of those things are going to need a company that has a strong capability in those sciences and a strong ability to invest in those quickly for that ever-changing environment. We think LG has those things to bring to the United States. Absolutely. Well, you know, obviously there's a lot of different components in a data center that have varying cooling needs. So can you walk us through now how LG's end-to-end cooling solution can address these unique needs? Sure. And there's a few different kind of what I'll call cooling loops that exist. There's the, in the white space, there's the going from the chip itself to a CDU, right? And then there's the CDU exchanging that heat out to a chiller that's outside and rejecting that heat to atmosphere. So we have all the products related to that. Then the computer room itself, the data center white space room itself, where people might be working on servers, they're usually have air handler, like a craw or something along those lines that helps cool that room, which we also support. And then in the gray space, there may be offices or other things that need cooling or heating, depending on the time of the year, you know, regular HVAC and LG is involved in that as well. So really all the thermal management pieces of the puzzle LG has. Great. Thanks for connecting those dots for us. So energy efficiency and sustainability, we know are critical when it comes to data center operations. How do LG solutions help to drive efficiencies? Sure. Well, I mean, efficiency can be put together in a lot of ways. One, as you look at each individual component doing what it needs to do, we have a lot of history in managing, you know, thermodynamics and understanding how to move heat around efficiently. That's what our company is based on with air conditioning, for example. And so we do things like CFD analysis, computer modeling to help optimize efficiency of each component of our product. Then we're also very vertically integrated LG is right. The components in our chiller, for example, you know, the compressors are ours, the fans that may exist are ours, the inverter technology on how to make it variable speed is ours. And so you not only have expertise in each of those things, but you know how they can work together because you're not relying on somebody else's technology to apply to your product you're using your own. And then so those are the main things with efficiency and making them work together helps. But then also with sustainability, you know, in the past you would have technologies that really would be air within the white space going to an evaporative cooling outside and now it's kind of the tables have turned and you have water cooling inside the white space, taking things from the chip and then going to an air based rejection of heat outside because people don't want to evaporate water. And so, you know, all basically all of our technologies can use non evaporative cooling as a means to reject that heat. And so there's there's a sustainability piece both from the electrical side and the efficiency, but then also the water usage. So the PUE and the WEE, we can help them meet both those. Very good. I want to ask you now about downtime, downtime due to maintenance or an operational issue can be catastrophic for data center customers. So how do LG's solutions help to minimize downtime? Yeah, there's a few different pieces where that would come into play. First off is just, you know, even though I said we're new to the United States, we're not a new company at how to make these products and how to how to do them in a way that has high quality and minimized downtime. So really, the beginning is just our history and know how with making these devices like chillers. But then you also within the design of the product or the design of the system, you incorporate redundancy that as we work with our our customers, we incorporate a level of redundancy to minimize any risk of downtime. Our CDUs, for example, are fully redundant within the in row CDUs that we have. And then, you know, if there is a unforeseen issue, right, whether it's the product's fault or something outside the product that is causing it, which is usually the case, you want to make sure that the product is designed to minimize any time needed for maintenance, whether it's preventative or because of an event. And so, again, we work with our customers to incorporate that into how we design things so that there's redundancy and it can run while being maintained or being worked on or that it could have quick disconnects or ways that would minimize the amount of time. So, you know, it's the history, the design aspects, and then also the maintenance and speed of maintenance that we that we work on. Got it. So we've been talking about increasing cooling demands due to the increased overall demand on data centers. I wanted to ask you, what else do you think will impact data center cooling needs in the next, say, five to 10 years? Is LG planning for anything specific in this way? And if so, how? So as, like I mentioned before, how things have switched from maybe an air-cooled technology, if you go back in time to the first computers, like in the 50s, they were all water-cooled because the amount of heat was massive. They were highly inefficient. Then, you know, we developed technology such that the heat was reduced and then you could cool things with air. But over the last decade or two, data centers have gotten to the point where you can't do that anymore. You need to use water. And then eventually it's going to get to the point where the concentration of heat will not allow you to use water either. You need to figure out another way. And you know, at Data Center World, there were multiple companies that were mentioning the use of refrigerants or multi-phase fluids to increase the amount of heat rejection that's available in a smaller and smaller area. And so LG is pretty uniquely well-suited for this because we're experts with the refrigerant cycle in that we have an HVAC business and we deal with that. Particularly in our VRF business, it's called, where we manage different refrigerants and bring that evaporative cooling right to the location where the cooling needs to happen. And so we see that happening with data centers at the chip level. And so, you know, that's where I think things are going to go over time is that you're going to have to continually improve technologies to have a greater and greater amount of cooling in a smaller and smaller space. And LG, again, like I was saying, has the technologies to develop things likely faster than other people who haven't been doing that for decades. Yeah, absolutely. You know, I certainly saw at Data Center World the range of technologies that you're offering there from the, you know, the CDU and the CRA to the cold plate and the oil-free centrifugal chiller. Were there any other takeaways from Data Center World that you think are important for the HVAC industry and or facility specifiers to understand? Yeah, I mean, like I was saying, with data centers specifically, LG is kind of coming in with a lot of, we seem like a new player, but at the same time, we're not new. We've been doing this for decades. And so, like we had mentioned with the growth and how you might need to make sure you have a strong supply chain, you might need to make sure you have a partner who has the right set of technologies who can both be capable, but also then have the money be large enough to have the money to invest in things that are changing. But then be a company that can be flexible and create a partnership with you. Given that we're new, we are very flexible right now. So, we're looking for partners to help drive what is state-of-the-art in this business. And we have the know-how, the technology, and the investment capability to be able to work with whoever that partner is or group of partners are to create something in this ever-quickly-changing landscape that's to come. So, we think we're a good partner that people should look into as they look at creating new designs for their data centers in the future. Absolutely. Well, we'll leave it there for now, but it's been a great talk. Thanks so much to you, Doug, and to LG for joining us today. Yeah, thank you for having me. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-13\n==================================================\n\n### Digital Hub on Fortaleza: The role of Tecto and V.tal\n**Episode ID:** 5099\n**Transcribed:** 2025-08-04 23:53:26.509137\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Data Center Frontier Show podcast. I'm Matt Vincent, Editor-in-Chief with Data Center Frontier. And today we're so happy to welcome in Andre Busnardo. He is Chief Commercial Director, or Commercial Director, I'm sorry, with Tecto Data Centers of Brazil. Welcome in, Andre. Thank you very much, Matt. Thank you for having me. Yeah. So happy to have you here. So today we're going to be talking about the area of Fortaleza in Brazil as a strategic digital hub in LATAM, and integration of data center and submarine cable infrastructure, and Tecto's investment and long-term commitment to regional development. So my first question for you, Andre, is what makes Fortaleza such a strategic location for Tecto's data center operations? Yeah, we have a long story in Fortaleza, and we are very excited with what we have achieved there, with what we have been able to do to serve our customers, and also looking to the future. So Fortaleza is a big telecom connectivity hub, not only for Brazil, but also for South America, integrating the continent to the rest of the world. We have 17 subsea cables landing there. There are some projects to expand, to have new cables there. When we started our business, actually our investors started in another company that today gave origin to Tecto, right? When GlobeNet Times, we had the cable there. And we started building the cable landing station space to house local customers, international customers there. And this had a very good demand. And when more recently, when the big techs, the data explosion happened, customers started to look not only for capacity in the cable, but also for space to house, to process their data there. We were very fast to identify that and not only expand the cable landing station, but build a more world-class data center with more density, with world-class reliability. That data center is called Big Lobster. It was launched in February 2023, so a little bit more than two years. And it has been very well received. We have almost sold out this data center. There's a mix of customers there with local customers, telecom ISPs, and also international customers. So with that demand and the customers that we already have there, it made a lot of sense for us to expand fast. So we are now delivering in October this year, a new data center, like three times bigger than Big Lobster. It's called Mega Lobster, not for coincidence. And this one will continue to be like a natural expansion of the first one. It's like three kilometers apart in the same neighborhood in Prado Futuro, very close to the landing station. And obviously it's integrated with the subsea cable, but also with the terrestrial network from Vital, which is from the same investors as Tecto. Obviously it is carrier neutral. But also Mega Lobster will be able to house more dense applications. It's a new data center, so we are building it with a flexible density and will be a natural expansion for the processing that we have today in Big Lobster. But also we'll be capable to house more dense applications such as AI 5G applications, machine learning maybe. So very excited with what we have ahead of us in Fortaleza. Yeah, it sounds exciting. Can you tell us anything more about Tecto's and Vital's current presence in the region? Maybe in terms of the energy mix and the vision behind investing in the three data centers there? I mean, maybe picking up on what you just mentioned, the AI factory push. Yes. So a couple of things here. One, I told the story in the previous answer, but there's some more details there. I think something that was very important to our success was leveraging the connectivity. So obviously we had the subsea cable. Now we have the Vital terrestrial network. But we were very, very happy to build an ecosystem there. So one of the things we did from the beginning was to attract the IX, the Fortaleza IX. So we are a central piece, as we say in Brazil, in Fortaleza. And then one thing starts to attract the other, right? You have an ecosystem, customers want to be there. We have customers that are like OTTs, content. And so everything collaborates to the growth. And the other thing that we have invested a lot, our investor, BTG, has a huge expertise in energy. It controls some companies. If you put them all together, they are the second or third largest energy company in Brazil. And obviously this matches a lot with the data center business. This is not only for our expansion in Fortaleza, obviously it's for the whole footprint that maybe we can talk a little bit later. But it also makes our data center and our expansion, our new data center in Fortaleza, be able to sell energy at very competitive prices. We do auto-production, which has some incentives. And green energy, right? We are doing auto-production with solar. So this is like the scenario that international customers want, like competitive prices and renewable energy. So we are able to mix that, to blend that into our offering in Fortaleza, and we'll continue to do that. Got it. Is there anything more to say about how Tecto and Vital's joint presence in Fortaleza really sounds like made the region into a digital hub for the world? Yes. Fortaleza, just some numbers. Fortaleza, two years ago, turned into the second largest IEX traffic in Brazil. It's only second to Sao Paulo. So it's bigger than Rio, for example, that has a much bigger population and revenues. With that, we were able to, as I said, to blend this with the connectivity stuff. What we are actually doing right now is, apart from expanding Fortaleza, the thing we are looking for in terms of expanding the synergy with the Vital Terrestrial Network is what we have in Big Lobster. We are like using this as a template for our expansion in other edge locations in Brazil. So we are looking for locations across the coast, from the north to the south of Brazil. So we see a lot of opportunities to blend terrestrial and subsequent connectivity with data centers. So you'll see some announcements in the near future there. And also some places that we look that are not in the coast, but the Vital Terrestrial connectivity can help us a lot, such as Brasilia, for example. This is also a place that we look with some interest. In short, we want to replicate the experience we had, and we are having Fortaleza to other places in Brazil. And this is something that makes a lot of sense on top of Vital Terrestrial and subsea connectivity. Yeah. So the new data center, the Mega Lobster, and really all the data centers, any other details you can give us in terms of cooling or anything interesting that's going on there in terms of the mechanical and electrical side of the infrastructure? Yeah. So we have currently two projects in development at different stages of development. So Mega Lobster is almost ready to be delivered in less than six months. And we have a huge project in Sao Paulo region, in Santana de Parnaba, that is a 200 megawatts facility. So this one is totally aimed at cloud and AI machine learning developments. And what I can say about Mega Lobster in terms of differentiator is in Fortaleza, maybe Big Lobster was already the more dense data center there. And what we are doing in Mega Lobster is trying to make it, as we say internally here, future proof. So we are delivering the first phase right now in October with a bigger density than Big Lobster. But for future, for the second and third phase that will complete the total 20 megawatts energy that we have there, we are planning a flexible density design. So we have already seen some demands for customers with higher density. We didn't see yet like this crazy, I don't know, 100, 150 per kilowatt for Fortaleza at the moment. But we have seen like 25, 40 kW per rack. So Mega Lobster is able to house this kind of deployments. And obviously, just making a quick note in Santander de Parnaiba, then it's a data center that we have like the liquid cooling and everything that we understand that will be necessary for very dense machine learning developments. Yeah. So next, I wanted to ask you about the types of customers and workloads that you're seeing in Fortaleza and how demand is evolving, you know, especially it's probably going to lead us into an AI discussion, but just also overall, you know, enterprise and cloud demands. I'm curious about it. We saw demands evolving through time. So as I said, when we started with the subsea cable, the landing station, we had like low density and small demands, mostly to process part of the content that goes through the subsea cable. But then we started as time went by and as the ecosystem that I mentioned started to develop, we started seeing more dense and bigger demands. So we have a customer, for example, that in Big Lobster, we have like one megawatt data house. So we have a single customer that occupies almost two data house, so mostly half of the data center. This made a little bit of the spark for us to say we need to expand fast and we need to expand in a flexible way in order to be able to house demands more bigger and denser. But in short, if I would say like the types of customers we have there, it's a very good mix. So we have the content providers, they attract ISPs and everything. We have the central peaks there. We have some big techs, most of the big techs are our customers there with cages, very, very good size there. So I think it's a very good plan and it's helping us to grow because one customer attracts the other and so on. So obviously, to keep that, we have a very good service level delivered and we have been very successful in attending all the needs of our customers. So that's a little bit of what I said previously that we want to replicate. It's not only the technology, but also the operation model that we have there. This has been very key for the continued growth we are having there. Understood. So I have to ask you, how do you see the expansion of edge data centers in Brazil and Latin America going these days due to the growth of AI and low latency applications? When I say that we want to replicate the model, it's exactly because of that. So if we look at Brazil, it's a very large country, it's a continental country. And with these applications that you just mentioned, like AI and 5G, it's very hard for us to imagine that this will go forward in terms of infrastructure without having a world-class data center reliable and scalable at different parts of the country. And today, some regions in Brazil, I won't say they don't have data centers, but they are low density, they are mostly telecom data centers that can house part of this demand, but we think there's a huge opportunity there. So the plan of the subsea capacity, the terrestrial connectivity, and also data centers that can house maybe, I don't know, as I said, the example, 25 to 40 KW per rack applications in the near future will be necessary. So this is a little bit of what we are looking in parallel with the hyper strategy that I mentioned, but in the edge, I think we have, there is less competition there from other providers. We have the network from our partners to leverage, and we want to build fast in this part. So the example, the classic example is if you are in the north of Brazil, I don't know, in Manaus or Belm, and you have an application that needs a high volume of traffic and processing, it can't go to So Paulo or Campinas to process and go back because the latency requirement will not match with that. So part of the processing will need to be done more close, and that's what we are aiming for. Absolutely. Well, we're just about coming to the end of our time here, but any closing thoughts for us here on what Tecto is doing in Fortaleza and in Brazil? As I mentioned briefly, just elaborating a little bit more, what we believe is exactly in the growth of the technology and covering Brazil in a strategic way in terms of infrastructure, both connectivity and data center. So we want to be the digital platform. In the same time that we are investing in the more traditional regions in terms of data center deployments like So Paulo, Campinas, Rio, starting with this project in Santana de Parnaba to house cloud and AI machine learning developments, we understand that part of this deployment will need to happen at several regions in Brazil, and we'll complement that with that strategy. So we want to be able to serve both local and especially international customers, the big techs, with capacity where they need. So if they need, I don't know, 50 megawatts in So Paulo and five megawatts in like three or other edge regions, including Fortaleza, obviously, that's what we are preparing to be ready to be the easy button to these guys where they need capacity. We'll be able to deliver with quality and in a short time. So this is the vision, what we are telling our customers, and they seem to be liking it and make sense for them. So that's what we are already doing and preparing to do more in the future. Absolutely. Well, thanks for joining us here today, Andrei. It was great talking to you. Thank you very much, Matt. It was a pleasure. Bye-bye. Bye now. We'll see you next time on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n\n## 2025-05-06\n==================================================\n\n### Are we coming up short? Navigating the Global Power Deficit\n**Episode ID:** 5100\n**Transcribed:** 2025-08-04 23:53:26.509174\n\n**TRANSCRIPT:**\nHello, and welcome to another episode of the Datacenter Frontier Show Podcast. I'm Matt Vinson, Editor-in-Chief of Datacenter Frontier, and we're so happy to be here today with Kathleen Margolis. She is Senior Client Account Director with Black & Veatch's Datacenter team. And Nick Bustamante, who is Chief Technology Officer for Horsescale Datacenters. So welcome both of you into the podcast here. Thank you. Thanks, Matt. And, well, maybe I'll just ask both of you to, before we tee up today's topic and the questions that I have for you, if you wanted to, Kathleen, maybe you could start and tell us a bit about Black & Veatch and your work there in the datacenter unit. And then, Nick, if you wanted to tell us about your work with Horsescale, I know you have a lot of hyperscale clients. Thank you, Matt. Black & Veatch is an employee-owned EPC with 12,000 employees. We've been in business for 120 years and we deliver sustainable infrastructure solutions from anywhere in the world. We've been in the mission-critical datacenter space since the late 1960s. In 2017, we commercialized it. Today, you'll find us at the intersection of power and data. We work across the entire datacenter ecosystem, from powering sites to designing for direct-to-chip typologies. Yes, indeed. And, Nick, would you like to tell us a little bit about Horsescale Datacenters? Yeah, happy to, Matt. So Horsescale is an approximately four-year-old company now and specializes in hyperscale datacenter development, primarily in the United States and in Europe, although we do look at other markets. And we have been quite successful in the last few years. We have a growing team of experts, much like myself, who've been in the hyperscale datacenter world and also lived in what I'll call the supply side of the world, not just the demand side. So we work for contractors, work for developers in a lot of different geos. We consider, of course, the hyperscale market, like I said, datacenters are typically going to be 50 megawatts and larger. We have developments anywhere from around 50 megawatts to upwards of 1,000 megawatts in our portfolio. Okay, thank you very much, Nick, for describing your company. So let's get right into the topic of today's podcast. I like the title of this podcast, Are We Coming Up Short? Navigating the Global Datacenter Power Deficit. And of course, we're here today to talk about that deficit in power and its impact on the datacenter industry. Where the demand for power often exceeds available supply. So first question I wanted to put to both of you here is, what do we really mean by a global power deficit for datacenters? Yeah, I'm happy to take that right away. And Kathleen, if you want to join in, I think, look, it's no surprise to those in the industry and many outside looking in that datacenters consume an ever increasing amount of power and have been over the last couple of decades and lately, a lot of the discussion leads with that. There's a couple of reasons why. Number one, that's a gating factor, the availability of power ultimately to the market itself. And so we spend a lot of time trying to figure out how to de-conflict that, how to get power to our developments as an industry as quickly as possible. And secondly, the utilities, publicly regulated utilities and otherwise, have really struggled to keep up with that demand. And the rest of the world aren't exactly in other verticals, slowing down their needs. We see industrial, we see life sciences, we see other verticals that demand more and more power, the rise of electric vehicles, for example. And we see a lot of other needs there in tandem with the datacenter. We see a lot of what I'll call thermal assets, legacy generating assets and thermal assets being shut down for various reasons. So together, those things are combining to create this power deficit, which is more pronounced in certain markets and that's also leading us to new markets. So in a nutshell, that's what I believe it is. Thanks, Nick. I like how you connect the dots there. Kathleen, did you want to weigh in on what we're really talking about when we're talking about a global datacenter power deficit? I would like to. Thank you. And I agree with Nick. He's very good at connecting the dots. It's one of the reasons we enjoy working with him. One of the things that I found very interesting when Nick actually brought this topic forward is as I canvassed our engineers, everybody says yes, that we're in the middle of a global deficit, power deficit. And so I started doing some headline searching. I started looking at where communities are and found it really fascinating that we're all very aware of it. It's not isolated to just the datacenter sector, but communities, individuals, hospital specialty organizations are very sensitive to it. I live in Texas and there's an ERCOT grid and market conditions dashboard that anyone can plug into. And my neighbors will talk about, oh, we're going to lose power next week because of X. So the deficit is becoming topical and we always look for someone to blame. And what I find so interesting is that the rise of datacenter demand has really called this forward as an issue, but it's not just datacenters. Just as Nick connected all those dots, it's the electrification of the grid. It's the AI introduced in industry manufacturing and distribution, just to name a couple. And what I find really kind of interesting and something that we all have to take ownership of, and it's one of the reasons I love working with my company, is that there is a gap between customer expectation, we always think it's there, right? And so there's great demand, and the actual availability. And what's happening from our perspective, because we work on both sides, on industry and utility, is that utility's grid is fragile, but not for cause. It's kind of an evolution fragility, right? If you look at the grid, it's 100 years old, but most of our facilities are 50 to 70 years old and really outdated based on where we are today. There's a mix of reliable and unreliable or vulnerable sources, lack of planning, coordination between the users, the tech companies, for example. And that lack of coordination is creating a real struggle. So it feels like we're in this terrible deficit, but when you work with people like Forescale or Affinias, or when you're working at a Black & Veatch, all you're looking for are solutions and that's what you see. So there's great, although we're in a deficit, we know that it's just part of the evolution of what we're going to build next. Yeah. Well, this discussion that we've just had here is a great way to set the table. And Kathleen, your points get me to my next question, really, talking about navigating power availability to data centers. I just wanted to see if we could get both of you to unpack the different options that are available. I think we all might know, but the ones that you're really seeing the focus on, if we could get you to both elaborate on that. Yeah, I'm happy to start on that. I think that the market, whether that's what I'll call a tier one, major metro market or we're going to something less populous or we're going to a rural market or an undeveloped market will drive a lot of that discussion. We see the transmission operators and the large utilities all talking about different aspects of their operation. So they're all using multiple sources of fuel, renewables, nuclear, gas, and all in different mixtures, some more than others, depending on their utility. Those all start to look interesting for different reasons to those utilities, whether they're leaning into sustainability aspects or they're leaning into grid reliability and aspects, whether they're looking to address base load needs, depending on the regulatory environment that they are held by. And so I think the single message I would leave people with about it is that there is no panacea, there is no single kind of silver bullet that will solve this problem. I think all the things I just mentioned that utilities are already turning to today are absolutely necessary for us to solve this crisis. I think we also have to get very much more efficient with the way that we operate those grids when we consider that in some markets, two thirds of the energy that's generated is lost just in distribution. That's a horrible metric. And so some of us in this industry are looking at it like, well, why don't we avoid that distribution? Why don't we help solve some of those problems that the utilities have that I just kind of alluded to with their fuel sources? And why don't we create some of our own generation in tandem with our data center so that the data center isn't, to some utilities, the data center, especially AI data centers and very large campuses can be a true liability and create something that's more of an asset so that at least we're offsetting our consumption and probably in some cases generating more than we're going to use and be able to interact in a beneficial way for everyone. And then we think that's a very virtuous way to conduct business. And then we look at different fuels, right? I mentioned all of the things the utility will look at, we will look at as well. And so if those entail renewables, if it entails gas or nuclear, we are being somewhat agnostic about that while we go and then in-depth look in each of those areas and decide, well, maybe this is going to comprise a bigger part of the solution in a certain market. Nuclear of course gets discussed a lot, and we can touch on that in a minute, but gas is also a very viable and currently very attractive way in a lot of markets to meet the need. As are certain renewables. Thank you. Yeah. Kathleen, from Black & Veatch's perspective, in this realm of self-generation and building custom power solutions that Nick has been sort of alluding to here, can you talk a little bit about the demand and what you've seen in terms of how it's sort of been unfurling? Yeah, I'm not as eloquent as Nick, but I have to say I'm right behind him on, and it's one of the reasons that virtuous organizations, as we reference oftentimes, like a core scale or others that are taking that responsibility are exactly what's needed right now. And it's exactly the space that we want to work in. What I mean by all of that is you can't put it all on the utility. You have to step into it. There has to be a corroboration, a collaboration. And I think that's what we're finding as a consultant to firms like Nick's. We're finding that we have to come forward as an ombudsman. We are that solution that helps clients investigate their options. We get well beyond, and Nick articulated it beautifully, but I'll just reiterate, it's wind, water, solar, biomass, you know, the hydrogen thing, green hydrogen, geothermal heat, battery, nuclear. All of that is on the continuum of consideration. There isn't a site we go on to today for a hyperscale level client tech company, for a chip manufacturer, or for a developer that isn't asking us to investigate. And so what we find ourselves very often doing, as Nick alluded to, is working with the utility, helping them to understand what asset we're willing to bring onto the grid and what might support their program. But I have to say, and I'm just going to use this as a sidebar here, everything I've been doing outside of my direct client contact is working on, you know, whether I go to a conference or I go to, oh, I took a client centric class at Rice recently, and the utilities are there. They're going to the data conferences. They're going to the, they're represented in this new customer centric model of education where they're really trying to upscale or refocus the way they work as a collaborator. And I think if there's any message I leave, that that's the hopeful part of fixing, evolving the grid to its next generation, to really getting it to a place where users like a hyperscaler, an AI system, or anyone from industry or community will have reliability in the system. The specifics are a little hard to reference by region. As you all know, we have three owners of the grid, East, West, and Texas, and each one has a different policy program. You know, unfortunately, when the grids were established, they were really established on the demand side. You know, any policies really kind of regulate, I think Nick referenced it as a constraint, but regulate the demand. And the reality is we now need more policy around capacity. And so that's what these utilities struggle with. But I think organizations that Nick works with, for example, the big techs, are willing to take more on. They're willing to become part of that power solution. They're willing to invest. And where we're seeing really interesting investment is in the green hydrogen and nuclear. Yeah, such interesting points that you make, but you kind of land on sustainability. And I did mean to ask you, as a leader in providing sustainable infrastructure, how does Black & Veatch integrate the sustainability model into overall data center design? Well, there are engineers much smarter than me that can explain that, but I will tell you that we do have a distinct point of view on the issue. And Phil Fisher is one of my colleagues I work with, and he's always good with quick quick and quick quip. I'll get that. And he says that the most sustainable watts we deliver are those that we do not use. And Nick made a really good point, you know, that so much power is lost in the transmission of power that it creates such waste. And in a sustainable model, you can't afford to create that kind of waste. And so what I would say that organizations like Black & Veatch, and not only Black & Veatch, I mean, we have some really good coopetition in this business that think very much the same way. We have to consider our real focus is on the intelligent infrastructure development, not just on the data center design, right? What's going to power that data center that makes it make sense from a sustainable model? And so we start kind of like thinking, as we referenced, you know, how power is transmitted from high to low, how are we going to deliver the low voltage, the facility, is 2N really necessary, is it wasting power? There's a lot of conversation that goes on with our teams talking about, well, if you don't use it, you lose it. So what are we going to create that's more efficient? Then we go into what's really in demand right now is kind of the cooling systems. You know, we start to think about what systems are going to work for that particular topology as we build heat rejection into a system, for example, how do we consider the reuse of heat? How does the site support that distribution of a reused heat? Then we get into, I guess the chilled water backup is a big issue, efficiency in water use. We do a lot of studies for our clients on recycled water or non-revenue water capture and how that could be revenue producing. So you have to go really deep into all of the utility that makes that data center possible. And I really hope I answered some of your question there. Absolutely. Nick, did you want to give us some data center operator perspective on the sustainable infrastructure conversation from a core scales perspective? Yeah, absolutely. I mean, I think if we're taking sustainability seriously, we have to consider, as I mentioned earlier, and Kathleen reiterated, you know, some of those distribution losses, right? And, you know, serious operators, serious hyperscalers do look at those losses. And it's tough to solve what I think is a really large generational problem or maybe multi-generational problem with power because people want to be able to plug in, you know, even though it's a much larger industrial scale, they still want to be able to plug in at some level and say, well, everything above that, you know, medium voltage or high voltage, right, is someone else's problem. And let them figure out the transmission distribution issues. Let them figure out how to operate and all of that. I just want something that, you know, I can essentially, you know, subscribe to as a service. And when the, you know, loads were smaller in data centers and in a lot of cases still, that's okay. I think when we go to a utility and ask them to make significant investments, not only is the utility going to respond to that and say, okay, I need you to collateralize that, you know, up to 100% in some cases. They're also going to come back and say, well, we're going to use our conventional way of addressing all of that. And if you were looking at the system holistically, which is a system of systems, you're going to realize that, you know, in your generation and in your transmission, there are a ton of inefficiencies and losses that really, you as the now the owner and, you know, payer for all of that in a proprietary way are completely being burdened by. And also that's going to limit you from scaling. And there are also potentially, you know, significant emissions and carbon effect from all of that. And so you have to look at that system of systems and pick the places that you feel, you know, will give you the most return. There's a ton of low-hanging fruit, to use that term, in the utility world, especially when we build a large, very large load data center that's going to consume a lot of power with conventional type of delivery. And then, you know, stepping that into the data center itself and onto the property that is the data center, you know, we really focus on a lot of the issues that Kathleen mentioned. There, you know, really are a million different places to look. That means that, you know, every detail is important, right? You can no longer ignore a fan. You can no longer ignore the blades on that fan. You can no longer ignore the bearings on that fan. You have to look at things like that I just mentioned at an atomic level and make sure that you're optimizing it as much as you can. And everybody's optimization is different too, right? We have some tenants who want things fast, other things sometimes, you know, more economically focused, but the drivers are, you know, still the same, right? Still power, speed. I'm sorry, speed, cost, and quality of delivery. And so we look at everything through those lenses broadly. But my point in standing, detail is very important and we like to consider that, you know, no detail goes left unaddressed. The cooling systems in particular, Kathleen mentioned, and, you know, there's obviously a drive towards addressing higher density racks and artificial intelligence coming into the field more and more every day. That area is ripe for optimization, right? Where we're using liquid, you know, frequently water and some mixture, but a lot of efficiencies can be had just by moving to that system. But then there are a lot of other areas within the system that still have to be optimized. Pumps, controllers, things like that, valves that themselves use a lot of energy and can also be, you know, points that hang you up in your design. So, again, I think it's a very target rich environment. You know, we look at economical ways of addressing our needs today and finding more scalable ways that are also sustainable is what we're all about. I think the industry is great because it's also, you know, fully involved in that discussion. I don't think, you know, in the past we would see, you know, where it was more of an enterprise driven world. Folks have said, you know, reliability is number one. Well, I think we've gotten past that and now people consider sustainability a major topic and that's great to see. Whether it's in a hyperscale or an enterprise setting, that's the number one thing that I see, you know, across the world right now. And it's fantastic, frankly. And we feel like, you know, we are fortunate to be a newer company because it also means that the assets that we build have already been, you know, sustainable since day one. And I'll leave it at that for now. Yeah, I, you know, thank you for those insights. You know, we could almost dwell in that depth for a little while. But I want to ask you now, this is kind of a longer question, but so per a recent white paper from Affinias Capital, Affinias Capital, of course, happens to be the vertically integrated affiliate investment partner for core scale data centers. But per this recent white paper from Affinias, it was noted that the race to meet data center demand is not just a story about rising power usage, but it's about tech strategy, regulatory pivots and energy reinvention as sort of a trifecta effect. So can you elaborate? Can you both elaborate on the convergence of these three elements and how this shapes your approach to data centered development? Yeah, I think first off, you know, you have to acknowledge those things. I think that folks who aren't looking at those variables with open eyes are really doing themselves a disservice. And we've seen this path that it's unfolding quickly in front of us develop because we've seen folks, whether through deliberate action or just through inaction, force some municipalities in certain states and certain countries even to look at data centers and say, no, I don't want that in my backyard. And that's fair when you consider that history has given them cause to think that. I think we as developers at Corsica, we like to enter into a market. We'd like to sit down with municipality and say, look, we want to be here. We want you to want to be us to be here as well. And so we want to feel like whatever we're developing there is somewhat equitable, which means it has to look the right way. It has to sound and emit things or not the right way. It has to have minimal demands on the public infrastructure and do that the right way. And now you're seeing a regulatory agency in a lot of cases say, you all talk about that. We're going to hold you to that. You're going to have to do that to deliver a project or even to get a permit to deliver a project in my backyard. And some concrete examples of that are, for example, in parts of Northern Virginia now, we see proffers conveying with property where the entitlement and the proffers that convey with it say, you cannot use open loop cooling systems. You cannot use open loop water cooling systems. You cannot use water. We see that all over the Western United States. And it's completely fair given the constraints on the public water infrastructure in the Western United States. So I think folks need to be understanding those things. And I think that that's unfolding faster now than it ever has as the data centers have scaled up and as folks have generated metrics, whether it's been data center operators or regulatory agencies or lobbyists supposing the DC industry have now gathered a quiver of these really powerful metrics and use that for different purposes. Thank you, Kathleen. Did you want to comment here on this convergence of developments related to power usage and data center development? This is Nick's way of thinking. It's his vocabulary that's beautifully articulated this almost internative things, right? It's the convergence of all. And so we're responsible for everything. And it's this ideology that sort of sets a platform for organizations like CoreScale that make them good corporate citizens in the world. And I really respect that. I look at engineering firms and architectural firms working in the space as good stewards of our environment. And so to understand this convergence really makes us, again, reflect stronger on the things we're responsible for and how we are conscientious, more intentional about the kind of design that we do. And I think that's the call to action is to become more intentional. And I know some of our colleagues across the ecosystem in data specifically are very sensitive to this. And it takes leadership like Nick's to make that happen. And so that's all I wanted to point out. And I'll just make one more comment, Matt and Kathleen. You know, I firmly believe and we do at CoreScale that chaos creates opportunity. And some markets are more chaotic than others. I think there's a lot of opportunity out there for us. I'm going to quote you on that one, Nick. Agreed. Well, it's just been such a far-reaching and illuminating discussion. I kind of wanted to close on an umbrella sort of question for each of you to dive into here. Would you say, are there regions of the world where access to power is not a challenge for data center development? You want to take that, Kathleen? Yeah. Well, I think based on the conversation we're in right now, I would have to say that across the globe, this kind of intentionality, being aware, knowing what you're asking for is critical everywhere. There are places where regulations, where the policies around capacity really guide you well, they're pretty constraining. And great creativity comes out of constraints. So I'm not arguing it. I'm just saying that there are places that are more difficult, but easier to navigate as a result, which is really weird, right? So we work across the globe in this space. And work that we're doing in Europe, we're seeing as highly constrained, but it forces you to be more creative and you kind of know what you're doing. I mean, you know what you're going to have to do to make it work. In APAC, it's different. You don't have the constraints that give you kind of a roadmap. And so you kind of have to figure it out as you go, but they're much more open to what you're bringing to the market. So you get to set some standards yourself and good actors will set good standards. LATAM is in such demand of data as part of its critical infrastructure that they're just figuring it out. And it's something that Nick had referenced early, and I want to call to attention as a relatively new enterprise in the data space. Well, not so new. Data is kind of new. But over the last six, seven years that core scales come on, they've had the fortune of leapfrogging over all the old typology and moving into new pretty quickly. And LATAM kind of operates that way. And so we're finding that the constraints there aren't what you might find in the states. But the lack of resources make it very difficult. So each region has its own. Everybody has a need. And so once you figure out what those constraints and policies allow you to do, if you go in as a good actor, you can do well. If you go in as a bad actor, you're going to get the same results you get in the states. So Nick, yeah. Yes, sir. I think Kathleen wisely pointed out a unique aspect of this, which is that in markets that are more highly regulated and where energy is in short supply, it definitely makes things more straightforward. Because when it happens, when we get to places to address your question, where energy is more unbounded, it's more of an unbounded problem. And we are able to do something of larger scale with respect to power. We frankly turn to the easy list of things that we can check off and say, OK, look, yeah, we've got gigawatts of power here, guys. Let's go do all the things we think right now today make sense for us. Well, typically, as you study those things, you start to look at it. And whoever owns that energy, if we're talking about something, say, in the United States, they're going to realize not only is there value there for them, there's value for their customers elsewhere. And they start to realize that maybe that multiple gigawatts that you thought you had needs to be constrained. And we see that conversation, we see that evolution occur faster today than we ever have. And it's not just for sustainability reasons. Investment is tougher than it ever has been. Capital markets are also very mixed, especially for big infrastructure investors. And it's a tough, tough world. So just because a lot of energy is available doesn't mean that one will have access to it in an easy and quickly consumable way. So in some of those places where it's more restricted, it's equally as important to be and in some cases, easier. Right. Well, Kathleen and Nick, I want to thank you for joining us here today on the Data Center Frontier Show podcast for a really great discussion. And thanks to Black & Veatch and CoreScale Data Centers as well. It's been a pleasure having you. Thank you, Matt. Thank you, Matt. Thanks for your leadership on this. Thank you. OK, well, we'll see you next time then on the Data Center Frontier Show podcast.
\n\n--------------------------------------------------------------------------------\n\n
